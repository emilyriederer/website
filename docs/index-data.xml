<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Emily Riederer</title>
<link>https://emilyriederer.com/#category=data</link>
<atom:link href="https://emilyriederer.com/index-data.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.4.554</generator>
<lastBuildDate>Thu, 18 Jul 2024 05:00:00 GMT</lastBuildDate>
<item>
  <title>Crosspost: Data discovery doesn’t belong in ad hoc queries</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/data-discovery-ad-hoc/</link>
  <description><![CDATA[ 





<p>Credible documentation is the best tool for working with data. Short of that, labor (and computational) intensive validation may be required. Recently, I had the opportunity to expand on these ideas in a <a href="https://www.selectstar.com/resources/data-discovery-doesnt-belong-in-ad-hoc-queries">cross-post with Select Star</a>. I explore how a “good” data analyst can interrogate a dataset with expensive queries and, more importantly, how best-in-class data products eliminate the need for this.</p>
<p>My post is reproduced below.</p>
<hr>
<p>In the current environment of decreasing headcount and rising cloud costs, the benefits of data management are more objective and tangible than ever. Done well, data management can reduce the cognitive and computational costs of working with enterprise-scale data.</p>
<p>Analysts often jump into new-to-them tables to answer business questions. Without a robust data platform, this constant novelty leads analysts down one of two paths. Either they boldly gamble that they have found intuitive and relevant data or they painstakingly hypothesize and validate assumptions for each new table. The latter approach leads to more trustworthy outcomes, but it comes at the cost of human capital and computational power.</p>
<p>Consider an analyst at an e-commerce company asking the question “How many invoices did we generate for fulfilled orders to Ohio in June?” while navigating unfamiliar tables. In this post, we explore prototypical queries analysts might have to run to validate a new-to-them table. Many of these are “expensive” queries requiring full table scans. Next, we’ll examine how a data discovery platform can obviate this effort.</p>
<p>The impact of this inefficiency may range from a minor papercut to a major cost sink depending on the sizes of your analyst community, historical enterprise data, and warehouse.</p>
<section id="preventable-data-discovery-queries" class="level2">
<h2 class="anchored" data-anchor-id="preventable-data-discovery-queries">6 Preventable Data Discovery Queries</h2>
<section id="what-columns-are-in-the-table" class="level3">
<h3 class="anchored" data-anchor-id="what-columns-are-in-the-table">1. What columns are in the table?</h3>
<p>Without a good data catalog, analysts will first need to check what fields exist in a table. While there may be lower cost ways to do this like looking at a pre-rendered preview (ala BigQuery), using a DESCRIBE statement (ala Spark), or limiting their query to the first few rows, some analysts may default to requesting all the data.</p>
<pre><code>select *
from invoices;</code></pre>
</section>
<section id="is-the-table-still-live-and-updating" class="level3">
<h3 class="anchored" data-anchor-id="is-the-table-still-live-and-updating">2. Is the table still live and updating?</h3>
<p>After establishing that a table has potentially useful information, analysts should next wonder if the data is still live and updating. First they might check a date field to see if the table seems “fresh”.</p>
<pre><code>select max(order_date) 
from invoices;</code></pre>
<p>But, of course, tables often have multiple date fields. For example, an e-commerce invoice table might have fields for both the date an order was placed and the date the record was last modified. So, analysts may guess-and-check a few of these fields to determine table freshness.</p>
<pre><code>select max(updated_date) 
from invoices;</code></pre>
<p>After identifying the correct field, there’s still a question of refresh cadence. Are records added hourly? Daily? Monthly? Lacking system-level metrics and metadata on the upstream table freshness, analysts are still left in the dark. So, once again, they can check empirically by looking at the frequency of the date field.</p>
<pre><code>select max(updated_date), count(1) as n
from invoices
group by 1;</code></pre>
</section>
<section id="what-is-the-grain-of-the-table" class="level3">
<h3 class="anchored" data-anchor-id="what-is-the-grain-of-the-table">3. What is the grain of the table?</h3>
<p>Now that the table is confirmed to be usable, the question becomes how to use it. Specifically, to credibly query and join the table, analysts next must determine its grain. Often, they start with a guess informed by the business context and data modeling conventions, such as assuming an invoice table is unique by order_id.</p>
<pre><code>select count(1) as n, count(distinct order_id)
from invoices;</code></pre>
<p>‍However, if they learn that order_id has a different cardinality then the number of records, they must ask why. So, once again, they scan the full table to find examples of records with shared order_id values.</p>
<pre><code>select *
from invoices
qualify count(1) over (partition by order_id) &gt; 1
order by order_id
limit 10;</code></pre>
<p>Eyeballing the results of this query, the analysts might notice that the same order_id value can coincide with different ship_id values, as a separate invoice is generated for each part of an order when a subset of items is shipped. With this new hypothesis, the analyst iterates on the validation of the grain.</p>
<pre><code>select count(1) as n, count(distinct order_id, ship_id)
from invoices;</code></pre>
</section>
<section id="what-values-can-categorical-variables-take" class="level3">
<h3 class="anchored" data-anchor-id="what-values-can-categorical-variables-take">4. What values can categorical variables take?</h3>
<p>The prior questions all involved table structure. Only now can an analyst finally begin to investigate the table’s content. A first step might be to understand the valid values for categorical variables. For example, if our analyst wanted to ensure only completed orders were queried, they might inspect the potential values of the order_status_id field to determine which values to include in a filter.</p>
<pre><code>select distinct order_status_id
from invoices;</code></pre>
<p>They’ll likely repeat this process for many categorical variables of interest. Since our analyst is interested in shipments specifically to Ohio, they might also inspect the cardinality of the ship_state field to ensure they correctly format the identifier.</p>
<pre><code>select distinct ship_state
from invoices;</code></pre>
</section>
<section id="do-numeric-columns-have-nulls-or-sentinel-values-to-encode-nulls" class="level3">
<h3 class="anchored" data-anchor-id="do-numeric-columns-have-nulls-or-sentinel-values-to-encode-nulls">5. Do numeric columns have nulls or ‘sentinel’ values to encode nulls?</h3>
<p>Similarly, analysts may wish to audit other variables for null handling or sentinel values by inspecting column-level statistics.</p>
<pre><code>select distinct ship_state
from invoices;</code></pre>
</section>
<section id="is-the-data-stored-with-partitioning-or-clustering-keys" class="level3">
<h3 class="anchored" data-anchor-id="is-the-data-stored-with-partitioning-or-clustering-keys">6. Is the data stored with partitioning or clustering keys?</h3>
<p>Inefficient queries aren’t only a symptom of ad hoc data validation. More complex and reused logic may also be written wastefully when table metadata like partitioning and clustering keys is not available to analysts. For example, an analyst might be able to construct a reasonable query filtering either on a shipment date or an order date, but if only one of these is a partitioning or clustering key, different queries could have substantial performance differences.</p>
</section>
</section>
<section id="understanding-your-data-without-relying-on-queries" class="level2">
<h2 class="anchored" data-anchor-id="understanding-your-data-without-relying-on-queries">Understanding Your Data Without Relying on Queries</h2>
<p>Analysts absolutely should ask themselves these types of questions when working with new data. However, it should not be analysts’ job to individually answer these questions by running SQL queries. Instead, best-in-class data documentation can provide critical information through a data catalog like Select Star.</p>
<section id="what-columns-are-in-the-table-and-do-we-need-a-table" class="level3">
<h3 class="anchored" data-anchor-id="what-columns-are-in-the-table-and-do-we-need-a-table">1. What columns are in the table? And do we need a table?</h3>
<p>Comprehensive search across all of an organization’s assets can help users quickly identify the right resources based on table names, field names, or data descriptions. Even better, search can incorporate observed tribal knowledge of table popularity and common querying patterns to prioritize the most relevant results. Moreover, when search also includes downstream data products like pre-built reports and dashboards, analysts might sometimes find an answer to their question exists off the shelf.</p>
</section>
<section id="is-the-table-still-live-and-updating-and-are-its-own-sources-current" class="level3">
<h3 class="anchored" data-anchor-id="is-the-table-still-live-and-updating-and-are-its-own-sources-current">2. Is the table still live and updating? And are its own sources current?</h3>
<p>Data is not a static artifact so metadata should not be either. After analysts identify a candidate table, they should have access to real-time operational information like table usage, table size, refresh date, and upstream dependencies to help confirm whether the table is a reliable resource.</p>
<p>Ideally, analysts can interrogate not just the freshness of a final table but also its dependencies by exploring the table’s data lineage.</p>
</section>
<section id="what-is-the-grain-of-the-table-and-how-does-it-relate-to-others" class="level3">
<h3 class="anchored" data-anchor-id="what-is-the-grain-of-the-table-and-how-does-it-relate-to-others">3. What is the grain of the table? And how does it relate to others?</h3>
<p>Table grain should be clearly documented at the table level and emphasized in the data dictionary via references to primary and foreign keys. Beyond basic documentation, entity-relationship (ER) diagrams will help analysts gain a richer mental model of grains of how they can use these primary-foreign key relationships to link tables to craft information with the desired grain and fields. Alternatively, they can glean this information from the wisdom of the crowds if they have access to how others have queried and joined the data previously.</p>
</section>
<section id="what-values-can-categorical-variables-take-do-numeric-columns-have-nulls-or-sentinel-values-to-encode-nulls" class="level3">
<h3 class="anchored" data-anchor-id="what-values-can-categorical-variables-take-do-numeric-columns-have-nulls-or-sentinel-values-to-encode-nulls">4. What values can categorical variables take? Do numeric columns have nulls or ‘sentinel’ values to encode nulls?</h3>
<p>Information about proper expectations and handling of categorical and null values may be published as field definitions, pointed to lookup tables, implied in data tests, or illustrated in past queries. To drive consistency and offload redundant work from data producers, such field definitions can be propagated from upstream tables.</p>
</section>
<section id="is-the-data-stored-with-partitioning-or-clustering-keys-1" class="level3">
<h3 class="anchored" data-anchor-id="is-the-data-stored-with-partitioning-or-clustering-keys-1">‍5. Is the data stored with partitioning or clustering keys?</h3>
<p>Analysts cannot write efficient code if they don’t know where the efficiency gains lie. Table-level documentation should clearly highlight the use of clustering or partitioning files so analysts can use the most impactful variables in filters and joins. Here, consistency of documentation is paramount; analysts may not always be incented to care about query efficiency, so if this information is hard to find or rarely available, they can be easily dissuaded from looking.</p>
<p>Beyond a poor user experience, poor data discoverability creates inefficiency and added cost. Even if you don’t have large scale historical data or broad data user communities today, slow queries and tedious work still detract from data team productivity while introducing context-switching and chaos. By focusing on improving data discoverability, you can streamline workflows and enhance the overall efficiency of your data operations.</p>


</section>
</section>

 ]]></description>
  <category>data</category>
  <category>workflow</category>
  <category>elt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/data-discovery-ad-hoc/</guid>
  <pubDate>Thu, 18 Jul 2024 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/data-discovery-ad-hoc/featured.png" medium="image" type="image/png" height="76" width="144"/>
</item>
<item>
  <title>Crosspost: Why You Need Data Documentation in 2024</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/docs-personas/</link>
  <description><![CDATA[ 





<p>We’ve all worked with poorly documented dataset, and we all know it isn’t pretty. However, it’s surprisingly easy for teams to continue to fall into “documentation debt” and deprioritize this foundational work in favor of flashy new projects. These tradeoff discussions may become even more painful in 2024 as teams are continually asked to do more with less.</p>
<p>Recently, I had the opportunity to articulate some of the underappreciated benefits of data documentation in a <a href="https://www.selectstar.com/blog/why-you-need-data-documentation-in-2024">cross-post with Select Star</a>. This builds on my prior post showing that <a href="../..\post/docs-closer-than-you-think/">documentation can be strategically created throughout the data development process</a>. To make the case for taking those “raw” documentation resources to a polished final form, I return to the jobs-to-be-done framework that I’ve previously employed to talk about <a href="../..\post/team-of-packages/">the value of innersource packages</a>. In this perspective, documentation is like hiring an extra resource (or more!) to your team.</p>
<p>Some of the jobs discussed are:</p>
<ul>
<li>Developer Advocacy and Product Evangelism for users
<ul>
<li>Users think data doesn’t exist if they can’t find it, they think data is broken if they misinterpret it</li>
<li>Documentation is both a “user interface” to make data usage easy and a bulwark against confusion and frustration</li>
</ul></li>
<li>Producct and Project Management for developers
<ul>
<li>Data intent can “drift” over time</li>
<li>As teams evolve and collaborate, this risks initial intent getting lost and poluted (after all, what really is a “customer”?)</li>
<li>Documentation serves as a contract and coach for one or more teams to force clarity and consistency of intent</li>
</ul></li>
<li>Chief of Staff oversight for data leaders
<ul>
<li>Leaders face increasing demands in data governance: navigating changing privacy regulations, fighting decaying data quality, and discerning their next strategic investments</li>
<li>Documentation is their command center to understand what data assets exists and where to better spot risks and opportunities</li>
</ul></li>
</ul>
<p>If you or your team works on data documentation, I’d love to hear what other “jobs” you have found that data documentation performs in your organization.</p>



 ]]></description>
  <category>data</category>
  <category>workflow</category>
  <category>elt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/docs-personas/</guid>
  <pubDate>Mon, 15 Jan 2024 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/docs-personas/featured.PNG" medium="image"/>
</item>
<item>
  <title>Crosspost: Why you’re closer to data documentation than you think</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/docs-closer-than-you-think/</link>
  <description><![CDATA[ 





<p>Documentation can be a make-or-break for the success of a data initiative, but it’s too often considered an optional nice-to-have. I’m a big believer that writing is thinking. Similarly, documenting is planning, executing, and validating.</p>
<p>Previously, I’ve explored how <a href="https://emilyriederer.netlify.app/post/latent-lasting-documentation/">we can create latent and lasting documentation</a> of data products and how <a href="https://emilyriederer.netlify.app/post/column-name-contracts/">column names can be self documenting</a>.</p>
<p>Recently, I had the opportunity to expand on these ideas in a <a href="https://www.selectstar.com/blog/why-youre-closer-to-data-documentation-than-you-think">cross-post with Select Star</a>. I argue that teams can produce high-quality and maintainable documentation with low overhead with a form of “documentation-driven development”. That is, smartly structuring and re-using artifacts from the development process into long-term documentation. For example:</p>
<ul>
<li>At the planning stage:
<ul>
<li>Structuring requirements docs in the form of data dictionaries</li>
<li>Creating early alignment on higher-order concepts like entity definitions (and <em>writing them down</em>)</li>
<li>Mentally beta testing data usability with an entity-relationship diagram</li>
</ul></li>
<li>At the development stage:
<ul>
<li>Ensuring relevant parts of internal “development documentation” (e.g.&nbsp;dbt column definitions, docstrings) are published to a format and location accessible to users</li>
<li>With different information but similar motivation to ER diagrams, sharing the full orchestration DAG to help users trace column-level lineage and internalize how each field maps to a real-world data generating process</li>
<li>Sharing data tests being executed (the “user contract”) and their results</li>
</ul></li>
<li>Throughout the lifecycle:
<ul>
<li>Answering questions “in public” (e.g.&nbsp;Slack versus email) to create a searchable collection of insights</li>
<li>Producing table usage statistics to help large, decentralized orgs capture the “wisdom of the crowds”</li>
</ul></li>
</ul>
<p>If you or your team works on data documentation, I’d love to hear what other patterns you’ve found to collect useful documentation assets during a data development process.</p>



 ]]></description>
  <category>data</category>
  <category>workflow</category>
  <category>elt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/docs-closer-than-you-think/</guid>
  <pubDate>Fri, 05 Jan 2024 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/docs-closer-than-you-think/featured.PNG" medium="image"/>
</item>
<item>
  <title>Data Downtime Horror Stories Panel</title>
  <link>https://emilyriederer.com/talk/data-downtime/</link>
  <description><![CDATA[ 




<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>In October, I joined a Halloween-themed panel along with Chad Sanderson and Joe Reis to discuss our horror stories of data quality gone wrong and how to build successful data quality strategies in large organizations. Key takeaways are summarized on <a href="https://www.montecarlodata.com/blog-scary-data-quality-stories-7-tips-for-preventing-your-own-data-downtime-nightmare/">Monte Carlo’s blog</a>.</p>


</section>

 ]]></description>
  <category>data</category>
  <guid>https://emilyriederer.com/talk/data-downtime/</guid>
  <pubDate>Mon, 23 Oct 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/data-downtime/featured.PNG" medium="image"/>
</item>
<item>
  <title>Scaling Personalized Volunteer Emails</title>
  <link>https://emilyriederer.com/talk/midterm-email/</link>
  <description><![CDATA[ 



<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" aria-controls="tabset-1-4" aria-selected="false">Video</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides.pdf">Slides</a> </span><br>
<span><i class="bi bi-play"></i> <a href="https://youtu.be/5UGUcgxTWTM">Video</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>In this four-minute lightning talk, I explain how Two Million Texans used components of our existing data stack to provide personalized success metrics and action recommendations to over 5,000 volunteers in the lead up to the 2022 midterm elections. I briefly describe our pipeline and how we frontloaded key computational steps in BigQuery to circumvent limitations of downstream tools.</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div id="slides" style="width:100%; aspect-ratio:16/11;">
<embed src="slides.pdf#zoom=Fit" width="100%" height="100%">
</div>
</div>
<div id="tabset-1-4" class="tab-pane" aria-labelledby="tabset-1-4-tab">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/5UGUcgxTWTM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>



 ]]></description>
  <category>data</category>
  <category>elt</category>
  <guid>https://emilyriederer.com/talk/midterm-email/</guid>
  <pubDate>Wed, 21 Jun 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/midterm-email/featured.png" medium="image" type="image/png" height="80" width="144"/>
</item>
<item>
  <title>Industry information management for causal inference</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/causal-data/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/causal-data/featured.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Data strategy motivated by causal methods</figcaption>
</figure>
</div>
<p><em>This post summarizes the final third of my talk at Data Science Salon NYC in June 2023. Please see the <a href="../../talk/causal-design-patterns">talk details</a> for more content.</em></p>
<p>Techniques of observational causal inference are becoming increasingly popular in industry as a complement to experimentation. Causal methods offer the promise of accelerating measurement agendas and facilitating the estimation of previously un-measurable targets by allowing analysts to extract causal insights from “found” data (e.g.&nbsp;observational data collected without specific intent). However, if executed without careful attention to their assumptions and limitations, they can lead to spurious conclusions.</p>
<p>Both experimental and observational methods attempt to address the <strong>fundamental problem of causal inference</strong>: that is, the fact that for a given treatment of interest, we can never “see” the <em>individual-level outcome</em> both for the case when an individual received a treatment and a counterfactual scenario in which <em>for the same individual in the exact same context</em> that treatment was withheld. Some literature casts this as a “missing data” problem.<sup>1</sup> Counterfactual data is uncollectable; however, this fundamental missingness can be partially mitigated by diligent collection of <em>other</em> types of quantitative and qualitative information to control for confounding<sup>2</sup> and interrogate assumptions.</p>
<p>In this post, I argue that industry has unique advantages when using causal techniques over the social science disciplines that originated many foundational methods due to industry’s (theoretically) superior ability to observe and capture relevant supplemental data and context. Examining the implicit assumptions in common <a href="../../post/causal-design-patterns">causal design patterns</a> motivates the types of proactive enterprise information management – including data, metadata, and knowledge management – that will help preserve the raw inputs that future data scientists will need to effectively deploy causal techniques on historical data and answer questions that our organizations cannot even anticipate today. By casting an intentionally wide net on what information we observationally collect, we increase the likelihood that the future “found” data will have what those analysts need to succeed.</p>
<section id="why-industry-needs-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="why-industry-needs-causal-inference">Why industry needs causal inference</h2>
<p><img src="https://emilyriederer.com/post/causal-data/why-not-experiment.png" class="img-fluid"></p>
<p>Industry data science tends to highly value the role of A/B testing and experimentation. However, there are many situations where experimentation is not an optimal approach to learning. Experiments can be infeasible if we worry about the ethics or reputational risk of offering disparate customer treatments; they may be impractical in situations that are hard to randomize or avoid spillover effects; they can be costly to run and configure either in direct or opportunity costs; and, finally, they can just be <em>slow</em> if we wish to measure complex and long-term impacts on customer behaviors (e.g.&nbsp;retention, lifetime value).</p>
</section>
<section id="what-causal-methods-require" class="level2">
<h2 class="anchored" data-anchor-id="what-causal-methods-require">What causal methods require</h2>
<p><img src="https://emilyriederer.com/post/causal-data/patterns-and-variation.png" class="img-fluid"></p>
<p>These limitations are one of the reasons why observational causal inference is gaining increasing popularity in industry. Methods of observational causal inference allows us to estimate treatment effects without randomized controlled experimentation by using existing historical data. At the highest level, these methods work by replacing <em>randomization</em> with strategies to exploit other forms of <em>semi-random variation</em> in historical exposures of a population to a treatment. Since this semi-random <em>variation</em> could be susceptible to confounding, observational methods supplement variation with <em>additional data</em> to control for other observable sources of bias in our estimates and <em>contextual assumptions</em> about the data generating process.</p>
<p>My previous post on <a href="../../post/causal-design-patterns">causal design patterns</a> outlines a number of foundational causal methods, but I’ll briefly recap to emphasize the different ways that sources of variation, data, and context are used:</p>
<ul>
<li><strong>Stratification and Inverse Propensity Score Weighting</strong>:
<ul>
<li>Exploits “similar” populations of treated and untreated individuals</li>
<li>Assumes we can observe and control for common causes of the treatment and the outcome</li>
</ul></li>
<li><strong>Regression Discontinuity</strong>:
<ul>
<li>Exploits a sharp, semi-arbitrary cut-off between treated and untreated individuals</li>
<li>Assumes that the outcome is continuous with respect to the assigment variable and the assignment mechanism is unknown to individuals (to avoid self-selection)</li>
</ul></li>
<li><strong>Difference in Differences</strong>:
<ul>
<li>Exploits variation between <em>behavior over time</em> of treated and untreated <em>groups</em></li>
<li>Assumes that the treatment assignment is unrelated to expected future outcomes and that the treatment is well-isolated to the treatment group</li>
</ul></li>
</ul>
<p>Notably, the assumptions mentioned above are largely untestable statistically (e.g.&nbsp;not like testing for normality or multicolinearity) but rely on knowledge of past strategies and policies that guided differential treatment in historical data.<sup>3</sup></p>
</section>
<section id="industrys-unique-advantages-deploying-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="industrys-unique-advantages-deploying-causal-inference">Industry’s unique advantages deploying causal inference</h2>
<p><img src="https://emilyriederer.com/post/causal-data/industry-advantages.png" class="img-fluid"></p>
<p>Many causal methods originated in fields like epidemiology, economics, political science, and other social sciences. In such fields, direct experimentation is often impossible and even first-hand data collection is less common. Often, researchers may have to rely on pre-existing data sources like censuses, surveys, and administrative data (e.g.&nbsp;electronic health records).</p>
<p>Despite the lineage of these methods, industry has many advantages over traditional research fields in using them because each company controls the entire “universe” in which its customers exist. This should in theory provide a distinct advantage when collecting each of the three “ingredients” that causal methods use to replace randomization:</p>
<ul>
<li><strong>Variation</strong>: We control customer engagement strategies through methods like customer segmentation or models. Subsequent customer treatments are completely known to us but inherently have some arbitrary, judgmental component to exploit</li>
<li><strong>Data</strong>: We tend to be able to collect more measurements of our customers both as a snapshot (more variety in fields) and longitudinally (more observations over time) that can be brought into our analyses to control for confounders<sup>4</sup>, reduce other sources of variation in our estimate, and have additional ‘out of time’ data left over to conduct forms of validation like placebo tests</li>
<li><strong>Context</strong>: We tend to know how past strategies were set-up, how they looked to individuals involved, and <em>why</em> those decisions were made. This can be critical in reasoning whether our assumptions hold</li>
</ul>
<p>However, to convert this theoretical benefit to a practical one requires information management.</p>
</section>
<section id="data-management-for-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="data-management-for-causal-inference">Data management for causal inference</h2>
<p><img src="https://emilyriederer.com/post/causal-data/featured.png" class="img-fluid"></p>
<p>While all causal methods will be enhanced with better enterprise information management, it’s easiest to see the motivation by thinking back to specific examples. Causal inference can benefit from better data, metadata, and knowledge management. These are illustrated by propensity score weighting, regression discontinuity, and diff-in-diff respectively.</p>
<p><strong>Integrated Data Management</strong></p>
<p>Earlier, we posited that one advantage that industry has over academia for causal inference is access to richer historical data sources as a higher level of resolution (more measures per individual at more time points). A rich set of customer measures is critical for stratification and propensity score weighting where we attempt to control for selection on observables by balancing populations along dimensions that might be common causes of treatment assignment and outcome. (And, we may also wish to control for other unrelated sources of variation that effect only the outcome to develop more precise estimates.)</p>
<p>However, this is only true if customer data is proactively <em>collected, cleaned, and harmonized</em> across sources in the true spirit of a customer 360 view. Enterprises may collect data about customers from many different operational systems – for example, demographic information provided at registration, digital data on their logins and web activity, campaign data on attempted customer touchpoints and engagement, behavioral or fulfillment data on purchases / subscription renewals / etc. Any of these sources could be useful “observables” that help close confounding pathways in our analyses.</p>
<p>To make this data useful and accessible for analysis, it must be <em>proactively integrated</em> into a common source like a data warehouse, <em>well-documented</em> to help future users understand the nuances of each system, <em>harmonized</em> so fields have standard definitions (e.g.&nbsp;common definitions of an “account” and a “customer”), and <em>unified</em> by using techniques like entity resolution to ensure all sources share common identifiers so that they can be merged for analysis.</p>
<p><strong>Metadata Management</strong></p>
<p>Beyond those “typical” sources of customer data, our past customer strategies create data beyond the data directly generated by our customers. Metadata about past campaigns such as precise business logic on the different treatments offered (e.g.&nbsp;if sending customers a discount, what algorithmically determined the amount?), the campaign targeting and segmentation (e.g.&nbsp;What historical behaviors were used to segments customers? Was treatment determined by a predictive model?), and launch timing can all be critical to clearly identifying those sources of variation that we wish to exploit. For example, we might know that we once ran an re-engagement campaign to attempt the nudge interaction from customers who didn’t log-in to a website for some amount of time, but knowing whether that campaign was targeting customers &gt;30 days inactive or &gt;45 days inactive impacts our ability to analyze it with a regression discontinuity.</p>
<p>This means that we need to <em>treat metadata as first-class data</em> and ensure that it is extracted from operational source systems (or intent docs, config files, etc.), structured in a machine-readable format, and preserved in analytical data stores along with our customer data.</p>
<p>The importance of “metadata as data” extends beyond business-as-usual organization strategies. We can also fuel future causal inference with better metadata management of past formal experiments and execution errors.</p>
<p>As discussed above, formal experiments may represent a substantial <em>investment</em> in company resources so the data collected from them should be regarded as an <em>asset</em>. Beyond their utility for one-time reads and decisions, experiment designs and results should be carefully catalogued along with the assigned treatment group and the randomization criteria (such as fields characterizing <a href="https://www.census.gov/programs-surveys/acs/technical-documentation/user-notes/2022-07.html">sampling weights</a> as provided in US Census data). This can support future <em>observational</em> analysis of past experiments, including generalizing and transporting results to different populations.</p>
<p>Furthermore, even <em>mistakes</em> in executing past strategies may become “natural experiments” to help businesses understand scenarios that they might never have prioritized for testing. So, machine-readable incident logs and impacted populations can be useful as well.</p>
<p><strong>Knowledge Management</strong></p>
<p>Of course, not <em>all</em> information can be condensed into a nice, machine-readable spreadsheet. Methods like difference-in-differences illustrate how conceptual context can also help us battle-test assumptions like whether the decision-to-treat could have spilled over into the control population or been influenced by an anticipated change in the future outcome. This is the one area where industry may sometimes <em>lag</em> social sciences in information since some population-level treatments like a state law or local ordinance often have documented histories through the legislative process, news coverage, and historical knowledge about their implementation.</p>
<p>Industry can catch up on knowledge management by documenting and preserving in a centralized knowledge repository key information about strategic decisions undertaken, the motivating factors, and the anticipated customer experience. Such documents are inevitably created when working on new projects through memos ad decks intended to communicate the business case, intent, and expected customer experience. However, proactively figuring out how to <em>organize and index</em> this information through a classification system and <em>democratize access</em> through centralized knowledge repositories is critical to giving future users entree to this tribal knowledge. Projects like Airbnb’s <a href="https://github.com/airbnb/knowledge-repo">Knowledge Repository</a> suggest what such a system might look like in practice.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>For example, see https://arxiv.org/abs/1710.10251↩︎</p></li>
<li id="fn2"><p>If you’ve heard of ‘selection on observables’ in causal literature, richer data means observables!↩︎</p></li>
<li id="fn3"><p>There are some exceptions to this like placebo tests, bunching checks, etc.↩︎</p></li>
<li id="fn4"><p>Notable, the availability of more data absolutely does <em>not</em> mean that we should simply “dump in” all the data we have. Controlling for certain variables like colliders is counterproductive.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>causal</category>
  <category>data</category>
  <guid>https://emilyriederer.com/post/causal-data/</guid>
  <pubDate>Tue, 30 May 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/causal-data/featured.png" medium="image" type="image/png" height="60" width="144"/>
</item>
<item>
  <title>DataFold Data Quality Meet Up</title>
  <link>https://emilyriederer.com/talk/meetup-datafold/</link>
  <description><![CDATA[ 



<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" aria-controls="tabset-1-4" aria-selected="false">Video</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides.pdf">Slides</a> </span><br>
<span><i class="bi bi-play"></i> <a href="https://www.youtube.com/watch?v=uAe74zdLHbM">Video</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>This is the full recording from Datafold’s 9th Data Quality Meetup on Thursday, May 11th, 2023, which was focused on ‘Running dbt at scale’.</p>
<p>Following our usual structure, each of our speakers present a lightning talk and then we transition into a panel discussion moderated by Gleb Mezhanskiy - who pulls in the audiences’ questions.</p>
<p>We had 6 guest speakers &amp; panelists: 1. Emily Riederer @ Capital One - “Operationalizing Column Name Contracts” 2. Felix Kreitschmann and Jorrit Posor @ FINN Auto - “Supercharging Analytics Engineers: How to save time and prevent technical debt by automating CI checks” 3. Alexandra Gronemeyer @ Airbyte - “adopting and running dbt within a small data team at Airbyte” 4. Jason Jones @ Virgin Media O2 - “Zero to 200: scaling analytics engineering within an enterprise” 5. Sung Won Chung @ dbt Labs - “Experiences implementing dbt at scale”</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div id="slides" style="width:100%; aspect-ratio:16/11;">
<embed src="slides.pdf#zoom=Fit" width="100%" height="100%">
</div>
</div>
<div id="tabset-1-4" class="tab-pane" aria-labelledby="tabset-1-4-tab">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/uAe74zdLHbM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>



 ]]></description>
  <category>elt</category>
  <category>data</category>
  <guid>https://emilyriederer.com/talk/meetup-datafold/</guid>
  <pubDate>Fri, 12 May 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/meetup-datafold/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Crosspost: The Art of Abstraction in ETL</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/abstraction-airbyte/</link>
  <description><![CDATA[ 





<p><img src="https://emilyriederer.com/post/abstraction-airbyte/featured.PNG" class="img-fluid"></p>
<p>I previously shared the first in my three-part series of guest posts on Airbyte’s developer blog about ETL. The first focused on errors in data extraction. The next two focused on the countless, small decisions one makes when loading data, and finally the DataOps burden to keep things up-and-running.</p>
<p>This post serves only to serve as a quick reference to those posts:</p>
<ul>
<li><a href="https://airbyte.com/blog/dodging-data-extraction-errors">Dodging extraction errors</a></li>
<li><a href="https://airbyte.com/blog/loading-data-in-etl">Making sound loading decisions</a></li>
<li><a href="https://airbyte.com/blog/etl-good-practices">Keeping the good things going</a></li>
</ul>



 ]]></description>
  <category>data</category>
  <category>workflow</category>
  <category>elt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/abstraction-airbyte/</guid>
  <pubDate>Wed, 03 May 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/abstraction-airbyte/featured.PNG" medium="image"/>
</item>
<item>
  <title>The Art of Abstraction in ETL: Dodging Data Extraction Errors</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/abstraction-extraction/</link>
  <description><![CDATA[ 





<p><img src="https://emilyriederer.com/post/abstraction-extraction/featured.png" class="img-fluid"></p>
<p>Whenever I think about data developer tooling, I always like to take the perspectives of:</p>
<ol type="1">
<li>Understanding what higher-level abstractions that it provides that help eliminate rote work or reduce mental overhead for data teams. In the spirit of <a href="../..\post/team-of-packages/">my post on the jobs-to-be-done of innersource analysis tools</a>, this can be framed as what ‘jobs’ that tool can be hired to do (and with what level of responsibility and autonomy)</li>
<li>Interrogating the likely failure modes in the data stack based on the mechanics of the system, in the spirit of my <a href="../..\post/grouping-data-quality/">call for hypothesis-driven data quality testing</a></li>
</ol>
<p>These two themes motivated my recent guest post for Airbyte’s developer blog on <a href="https://airbyte.com/blog/dodging-data-extraction-errors">The Art of Abstraction in ETL: Dodging Data Extraction Errors</a>. In this post, I argue:</p>
<blockquote class="blockquote">
<p>Cooking a meal versus grocery shopping. Interior decorating versus loading the moving van. Transformation versus Extract-Load. It’s human nature to get excited by flashy outcomes and, consequently, the most proximate processes that evidently created them.</p>
</blockquote>
<blockquote class="blockquote">
<p>This pattern repeats in the data world. Conferences, blog posts, corporate roadmaps, and even budgets focus on data transformation and the allure of “business insights” that might follow. The steps to extract and load data are sometimes discounted as a trivial exercise of scripting and scheduling a few API calls.</p>
</blockquote>
<blockquote class="blockquote">
<p>However, the elegance of Extract-Load is not just the outcome but the execution – the art of things not going wrong. Just as interior decorating cannot salvage a painting damaged in transit or a carefully planned menu cannot be prepared if half of the ingredients are out-of-stock, the Extract-Load steps of data processing have countless pitfalls which can sideline data teams from their ambitious agendas and aspirations.</p>
</blockquote>
<p>I then go on to explore common challenges in successfully extracting data from an API and the abstractions that can aid in this process.</p>
<p>Please check out the full post on Airbyte’s site! I hope it resonates.</p>



 ]]></description>
  <category>data</category>
  <category>workflow</category>
  <category>elt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/abstraction-extraction/</guid>
  <pubDate>Wed, 22 Mar 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/abstraction-extraction/featured.PNG" medium="image"/>
</item>
<item>
  <title>Crosspost: Power up your data quality with grouped checks</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/grouping-data-quality-crosspost/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/grouping-data-quality-crosspost/featured.jpg" class="img-fluid figure-img"></p>
<figcaption>Photo credit to <a href="https://unsplash.com/@greysonjoralemon">Greyson Joralemon</a> on Unsplash</figcaption>
</figure>
</div>
<p>I’ve written previously about the <a href="../..\post/grouping-data-quality/">unreasonably effectiveness of grouping in data quality checks</a> and <a href="../..\post/grouping-data-quality-update/">implementing such checks in dbt</a>. To publicize my <a href="https://github.com/dbt-labs/dbt-utils/pull/633">merged pull request</a> for this feature in <code>dbt-utils</code> package, I summarized my thinking on the topic on dbt’s Developer Blog.</p>
<p>Check out the post <a href="https://docs.getdbt.com/blog/grouping-data-tests#grouped-checks">here</a>.</p>



 ]]></description>
  <category>data</category>
  <category>dbt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/grouping-data-quality-crosspost/</guid>
  <pubDate>Tue, 17 Jan 2023 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/grouping-data-quality-crosspost/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The Data (error) Generating Process</title>
  <link>https://emilyriederer.com/talk/data-error-gen/</link>
  <description><![CDATA[ 



<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" aria-controls="tabset-1-4" aria-selected="false">Video</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides.pdf">Slides</a> </span><br>
<span><i class="bi bi-play"></i> <a href="https://www.youtube.com/watch?v=LkfP4dEv11Q">Video</a> </span><br>
<span><i class="bi bi-pencil"></i> <a href="../../post/grouping-data-quality/">Post - Why Group Data Tests?</a> </span><br>
<span><i class="bi bi-pencil"></i> <a href="../../post/grouping-data-quality-update/">Post - Grouped Data Tests in dbt-utils</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>Statisticians often approach probabilistic modeling by first understanding the conceptual data generating process. However, when validating messy real-world data, the technical aspects of the data generating process is largely ignored.</p>
<p>In this talk, I will argue the case for developing more semantically meaningful and well-curated data tests by incorporating both conceptual and technical aspects of “how the data gets made”.</p>
<p>To illustrate these concepts, we will explore the NYC subway rides open dataset to see how the simple act of reasoning about real-world events their collection through ETL processes can help craft far more sensitive and expressive data quality checks. I will also illustrate how to implement such checks based on new features which I recently contributed to the open-source <code>dbt-utils</code> package.</p>
<p>Audience members should leave this talk with a clear framework in mind for ideating better tests for their own pipelines.</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div id="slides" style="width:100%; aspect-ratio:16/11;">
<embed src="slides.pdf#zoom=Fit" width="100%" height="100%">
</div>
</div>
<div id="tabset-1-4" class="tab-pane" aria-labelledby="tabset-1-4-tab">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/LkfP4dEv11Q" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>



 ]]></description>
  <category>data</category>
  <guid>https://emilyriederer.com/talk/data-error-gen/</guid>
  <pubDate>Sat, 12 Nov 2022 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/data-error-gen/featured.png" medium="image" type="image/png" height="54" width="144"/>
</item>
<item>
  <title>Goin’ to Carolina in my mind (or on my hard drive)</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/duckdb-carolina/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/duckdb-carolina/featured.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Photo Credit to <a href="https://unsplash.com/@element5digital">Element5 Digital</a> on Unsplash</figcaption>
</figure>
</div>
<p>There comes a time in every analyst’s life when data becomes too big for their laptop’s RAM. While open-source tools like R, python, and SQL have made “team of one” data analysts ever more powerful, analysts abilities to derive value from their skillsets are highly interdependent with the tools at their disposal.</p>
<p>For R and python, the size of datasets becomes a limiting factor to local processing; for a SQL-focused analyst, the existence of a database is prerequisite, as the gap between “democratized” SQL <em>querying</em> skills and data engineering and database management skills is not insignificant. The ever-increasing number of managed cloud services (from data warehouses, containers, hosted IDEs and notebooks) offer a trendy and effective solution. However, budget constraints, technical know-how, security concerns, or tight-timelines can all be headwinds to adoption.</p>
<p>So what’s an analyst to do when they have the knowledge and tools but not the infrastructure to tackle their problem?</p>
<p><a href="https://DuckDB.org/"><code>DuckDB</code></a> is quickly gaining popularity as a solution to some of these problems. DuckDB is a no-dependency, serverless database management system that can help parse massive amounts of data out-of-memory via familiar SQL, python, and R APIs. Key features include:</p>
<ul>
<li><strong>Easy set-up</strong>: Easily installed as an executable or embedded within R or python packages</li>
<li><strong>Columnar storage</strong>: For efficient retrieval and vectorized computation in analytics settings</li>
<li><strong>No installation or infrastructure required</strong>: Runs seamlessly on a local machine launched from an executable</li>
<li><strong>No loading required</strong>: Can read external CSV and Parquet files <em>and</em> can smartly exploit Hive-partitioned Parquet datasets in optimization</li>
<li><strong>Expressive SQL</strong>: Provides semantic sugar for analytical SQL uses with clauses like <code>except</code> and <code>group by all</code> (see blog <a href="https://DuckDB.org/2022/05/04/friendlier-sql.html">here</a>)</li>
</ul>
<p>This combination of features can empower analysts to use what they have and what they know to ease into the processing of much larger datasets.</p>
<p>In this post, I’ll walk through a scrappy, minimum-viable setup for analysts using <code>DuckDB</code>, motivated by the <a href="https://www.ncsbe.gov/results-data">North Carolina State Board of Election</a>’s rich voter data. Those interested can follow along in <a href="https://github.com/emilyriederer/nc-votes-DuckDB">this repo</a> and put it to the test by launching a free 8GB RAM GitHub Codespaces.</p>
<p>This is very much <em>not</em> a demonstration of best practices of anything. It’s also not a technical benchmarking of the speed and capabilities of <code>DuckDB</code> versus alternatives. (That ground is well-trod. If interested, see <a href="https://DuckDB.org/2021/05/14/sql-on-pandas.html">a head-to-head to pandas</a> or <a href="https://benchmark.clickhouse.com/">a matrix of comparisons across database alternatives</a>.) If anything, it is perhaps a “user experience benchmark”, or a description of a minimum-viable set-up to help analysts use what they know to do what they need to do.</p>
<section id="motivation-north-carolina-election-data" class="level2">
<h2 class="anchored" data-anchor-id="motivation-north-carolina-election-data">Motivation: North Carolina election data</h2>
<p>North Carolina (which began accepting ballots in early September for the upcoming November midterm elections) offers a rich collection of voter data, including daily-updating information on the current election, full voter registration data, and ten years of voting history.</p>
<ul>
<li>NC 2022 midterm early vote data from <a href="https://www.ncsbe.gov/results-data">NCSBE</a> (~6K records as-of 9/23 and growing fast!)</li>
<li>NC voter registration file from <a href="https://www.ncsbe.gov/results-data">NCSBE</a> (~9M records / 3.7G unzipped, will be static for this cycle once registration closes in October)</li>
<li>NC 10-year voter history file from <a href="https://www.ncsbe.gov/results-data">NCSBE</a> (~22M records / 5G unzipped, static)</li>
</ul>
<p>All of these files are released as zipped full-population (as opposed to delta) CSV files.</p>
<p>One can imagine that this data is of great interest to campaign staff, political scientists, pollsters, and run-of-the-mill political junkies and prognosticators. However, the file sizes of registration and history data, which is critical for predicting turnout and detecting divergent trends, could be prohibitive.</p>
<p>Beyond these files, analysis using this data could surely be enriched by additional third-party sources such as:</p>
<ul>
<li>Current Population Survey 2022 November voting supplement from <a href="https://www.census.gov/data/datasets/time-series/demo/cps/cps-supp_cps-repwgt/cps-voting.html">US Census Bureau</a></li>
<li>County-level past election results from <a href="https://dataverse.harvard.edu/file.xhtml?fileId=6104822&amp;version=10.0">MIT Election Lab via Harvard Dataverse</a></li>
<li>Countless other data sources either from the US Census, public or internal campaign polls, organization-specific mobilizaton efforts, etc.</li>
</ul>
<p>Your mileage may vary based on your system RAM, but many run-of-the-mill consumer laptops might struggle to let R or python load all of this data into memory. Or, a SQL-focused analyst might yearn for a database to handle all these complex joins.</p>
<p>So how can <code>DuckDB</code> assist?</p>
</section>
<section id="duckdb-highlights" class="level2">
<h2 class="anchored" data-anchor-id="duckdb-highlights">DuckDB highlights</h2>
<p>To explain, we’ll first level-set with a brief demo of some of the most relevant features of <code>DuckDB</code>.</p>
<p>Suppose we have flat files of data, like a <code>sample.csv</code> (just many orders of magnitude larger!)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-2">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>:[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>:[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'c'</span>:[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>]})</span>
<span id="cb1-3">df.head()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   a  b  c
0  1  4  7
1  2  5  8
2  3  6  9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">df.to_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sample.csv'</span>, index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
<p><code>DuckDB</code> can directly infer it’s schema and read it in a SQL-like interface by using functions like <code>read_csv_auto()</code> in the <code>FROM</code> clause.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> duckdb</span>
<span id="cb4-2">con <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> duckdb.<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">connect</span>()</span>
<span id="cb4-3">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> con.execute(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"select * from read_csv_auto('sample.csv')"</span>).fetchdf()</span>
<span id="cb4-4">df.head()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   a  b  c
0  1  4  7
1  2  5  8
2  3  6  9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">con.close()</span></code></pre></div>
</div>
<p>While very useful, this is of course bulky to type. We may also set-up a persistent DuckDB database as a <code>.duckdb</code> file as save tables with CTAS statements, as with any normal relational database. Below, we create the <code>sample-db.duckdb</code> database and add one table and one view with our data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">con <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> duckdb.<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">connect</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sample-db.duckdb'</span>)</span>
<span id="cb7-2">ctas <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"create or replace table sample as (select * from read_csv_auto('sample.csv'));"</span></span>
<span id="cb7-3">con.execute(ctas)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;duckdb.DuckDBPyConnection object at 0x0000000030026F70&gt;</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">cvas <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"create or replace view sample_vw as (select * from read_csv_auto('sample.csv'));"</span> </span>
<span id="cb9-2">con.execute(cvas)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;duckdb.DuckDBPyConnection object at 0x0000000030026F70&gt;</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">con.close()</span></code></pre></div>
</div>
<p>Now, suppose the data in <code>sample.csv</code> changes (now with 4 rows versus 3).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>:[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>:[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>], <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'c'</span>:[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>]})</span>
<span id="cb12-2">df.to_csv(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sample.csv'</span>, index <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>)</span></code></pre></div>
</div>
<p>Our table stored the data directly within the database (“disconnected” from the file) so it remains the same as before whereas our view changed.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">con <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> duckdb.<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">connect</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sample-db.duckdb'</span>)</span>
<span id="cb13-2">df1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> con.execute(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"select count(1) from sample"</span>).fetchdf()</span>
<span id="cb13-3">df2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> con.execute(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"select count(1) from sample_vw"</span>).fetchdf()</span>
<span id="cb13-4">con.close()</span>
<span id="cb13-5">df1.head()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   count(1)
0         3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">df2.head()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   count(1)
0         4</code></pre>
</div>
</div>
<p>(Here, I focus just on the features we will use; not strictly the coolest or most important. I highly encourage taking a spin through the <a href="https://duckdb.org/docs/guides/python/sql_on_pandas">docs</a> for countless features not discussed – like directly querying from or fetching to pandas and Arrow formats, an alternative relational API, etc.)</p>
</section>
<section id="data-management-pattern" class="level2">
<h2 class="anchored" data-anchor-id="data-management-pattern">Data management pattern</h2>
<p>With these features in mind, we return to the problem at hand. How can an analyst mimic the experience of having the infrastructure needed to do their work?</p>
<p>One approach could look something like the following. As a one-time exercise someone would:</p>
<ol type="1">
<li>Download all relevant files</li>
<li>(Optionally) Convert large, static files to Parquet versus CSV. DuckDB handles both well, but Parquet has some benefits that we’ll discuss in the next section</li>
<li>Create a DuckDB database with references to the files as <code>view</code>s</li>
</ol>
<p>Then, any analyst wanting to interact with the data could:</p>
<ol type="1">
<li>Interact with DuckDB as with any database connection</li>
<li>Whenever needed, re-download the files to the same name/directory to “refresh” the “database”</li>
</ol>
<p>The <a href="https://github.com/emilyriederer/nc-votes-duckdb">nc-votes-duckdb</a> GitHub repo shows this flow in practice. If you want to follow along, you can click <code>Code &gt; Create codespaces on master</code> and follow the more detailed instructions in the <code>README.md</code> or at the bottom of this post.</p>
<section id="one-time-set-up" class="level3">
<h3 class="anchored" data-anchor-id="one-time-set-up">One-time set-up</h3>
<p>The scripts for the first set of steps are in the <code>etl</code> subdirectory. The e-step (extract) isn’t all that interesting – just some basic python scripts for downloading files from the internet, unzipping, and moving them around. These land the raw data in the <code>data/raw</code> subdirectory.</p>
<p>Data transformation mostly involves converting large CSVs to Parquet format (and dropping personally-identifying fields from the data on principle). As mentioned above, this step is optional but has some benefits. First, if one person is “configuring” a database for many analysts, Parquet compression makes files smaller for storage and sharing. Second, at query-time Parquet is:</p>
<ul>
<li>More reliably structured with a well-defined schema</li>
<li>Faster to retrieve due to columnar storage</li>
<li>Able to be pruned by a savvy database optimizer (when appropriately partitioned by columns relevant to common query patterns)</li>
</ul>
<p><a href="https://duckdb.org/docs/guides/import/parquet_export">Conversion from CSV to Parquet</a> itself can be done with DuckDB. However, as of writing, I don’t believe that writing to a Hive-partitioned dataset is possible, so for this step, I used <code>pyarrow</code>, the python interface to <a href="https://arrow.apache.org/">Apache Arrow</a> (another promising, memory-conserving data processing framework.)</p>
<p>This snippet from <a href="https://raw.githubusercontent.com/emilyriederer/nc-votes-duckdb/master/etl/transform-register.py">etl/transform-register.py</a> demonstrates streaming a CSV by chunk and writing it out to county-level partitions:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># convert to hive-partitioned parquet</span></span>
<span id="cb17-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> os.path.exists(path_temp):</span>
<span id="cb17-3">    shutil.rmtree(path_temp)</span>
<span id="cb17-4"></span>
<span id="cb17-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> csv.open_csv(path_raw, </span>
<span id="cb17-6">                  convert_options<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> opts_convr_reg, </span>
<span id="cb17-7">                  parse_options <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> opts_parse,</span>
<span id="cb17-8">                  read_options <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> opts_read_reg) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> reader:</span>
<span id="cb17-9"></span>
<span id="cb17-10">    <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> next_chunk <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> reader:</span>
<span id="cb17-11">        <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> next_chunk <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb17-12">            <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">break</span></span>
<span id="cb17-13">        tbl <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pa.Table.from_batches([next_chunk])</span>
<span id="cb17-14">        pq.write_to_dataset(</span>
<span id="cb17-15">                tbl,</span>
<span id="cb17-16">                root_path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> path_temp,</span>
<span id="cb17-17">                use_dictionary <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> cols_reg_dict,</span>
<span id="cb17-18">                partition_cols<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'county_id'</span>]</span>
<span id="cb17-19">        )</span></code></pre></div>
</div>
<p>(Notably: counties are rather imbalanced in size and not the most important geography in many election contexts. This is for example purpose only, but partitions should always be picked based on how you expect to use the data. )</p>
<p>Once all the data in transformed, we can “load” our DuckDB database with relative-path references to our data. Again, this step can be done through any DuckDB API or the command line. Below, I use python in the <a href="https://github.com/emilyriederer/nc-votes-duckdb/blob/master/etl/load-db.py">etl/load-db.py</a> to create the <code>nc.duckdb</code> database and create references to the different datasets.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> duckdb</span>
<span id="cb18-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> os</span>
<span id="cb18-3"></span>
<span id="cb18-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># clean-up if already exists</span></span>
<span id="cb18-5"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> os.path.exists(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nc.duckdb'</span>):</span>
<span id="cb18-6">  os.remove(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nc.duckdb'</span>)</span>
<span id="cb18-7"></span>
<span id="cb18-8"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># create new duckdb files </span></span>
<span id="cb18-9">con <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> duckdb.<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">connect</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nc.duckdb'</span>)</span>
<span id="cb18-10"></span>
<span id="cb18-11"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># generate SQL to register tables</span></span>
<span id="cb18-12">template <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb18-13"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  CREATE VIEW </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{view_name}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> as </span></span>
<span id="cb18-14"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  (select * from read_parquet('</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{path}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{opts}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">))</span></span>
<span id="cb18-15"><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">  """</span></span>
<span id="cb18-16">data_dict <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb18-17">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'early_vote'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data/early_vt.parquet'</span>,</span>
<span id="cb18-18">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist_gen'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data/history_general/*/*.parquet'</span>,</span>
<span id="cb18-19">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist_oth'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data/history_other/*/*.parquet'</span>,</span>
<span id="cb18-20">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'register'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data/register/*/*.parquet'</span>,</span>
<span id="cb18-21">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'cps_suppl'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'data/cps_suppl.parquet'</span></span>
<span id="cb18-22">}</span>
<span id="cb18-23">partitioned <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist_gen'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'hist_pri'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'register'</span>]</span>
<span id="cb18-24"></span>
<span id="cb18-25"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> k,v <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> data_dict.items():</span>
<span id="cb18-26"></span>
<span id="cb18-27">  <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Loading </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{view_name}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> data..."</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(view_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k))</span>
<span id="cb18-28">  opt <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">', HIVE_PARTITIONING=1'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> k <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> partitioned <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">''</span></span>
<span id="cb18-29">  cvas <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> template.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(view_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> k, path <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> v, opts <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> opt)</span>
<span id="cb18-30">  con.execute(cvas)</span>
<span id="cb18-31"></span>
<span id="cb18-32">con.close()</span></code></pre></div>
</div>
<p>Similarly, other views could be defined as desired that query these views to do further data transformation.</p>
</section>
<section id="ongoing-usage" class="level3">
<h3 class="anchored" data-anchor-id="ongoing-usage">Ongoing usage</h3>
<p>Due to the decoupling of storage and compute, ongoing data management is nearly trivial. With this “infrastructure” set-up, analysts would need only to selectively redownload any changed datasets (in my project, using the <code>extract-.*.py</code> scripts as needed) to allow their queries to pull in the latest data.</p>
<p>Big picture, that means that (after initial set-up) an analyst would have no more overhead “managing their database” than they would with a “typical” CSV-driven workflow. Specifically for this project, the early votes data is the only dataset that changes frequently. For ease-of-use, it could even be left in CSV format to make the download process even easier for any users.</p>
</section>
</section>
<section id="data-access-patterns" class="level2">
<h2 class="anchored" data-anchor-id="data-access-patterns">Data access patterns</h2>
<p>With this set-up in place, analysts can then use their favorite tools to query the data.</p>
<section id="python" class="level3">
<h3 class="anchored" data-anchor-id="python">python</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> duckdb</span>
<span id="cb19-2">con <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> duckdb.<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">connect</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nc.duckdb'</span>)</span>
<span id="cb19-3">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> con.execute(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'select count(1) from early_vote'</span>).fetchdf()</span>
<span id="cb19-4">con.close()</span></code></pre></div>
</div>
</section>
<section id="r" class="level3">
<h3 class="anchored" data-anchor-id="r">R</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(duckdb)</span>
<span id="cb20-2">con <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbConnect</span>( <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">duckdb</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nc.duckdb'</span>) )</span>
<span id="cb20-3">df <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbGetQuery</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'select count(1) from early_vote'</span>)</span>
<span id="cb20-4"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbDisconnect</span>(con, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">shutdown =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>)</span></code></pre></div>
</div>
</section>
<section id="cli" class="level3">
<h3 class="anchored" data-anchor-id="cli">CLI</h3>
<pre><code>duckdb my-db.duckdb
&gt; select count(1) from early_vote</code></pre>
</section>
<section id="sql-ide-dbeaver" class="level3">
<h3 class="anchored" data-anchor-id="sql-ide-dbeaver">SQL IDE (DBeaver)</h3>
<p>DuckDB also works with open-source database IDEs like <a href="https://dbeaver.com/">DBeaver</a> for the full, “traditional” database experience. The <a href="https://duckdb.org/docs/guides/sql_editors/dbeaver">DuckDB website</a> gives full set-up instructions. With DBeaver, analysts get the “full” database experience with navigable access to table schemas and metadata.</p>
<p><img src="https://emilyriederer.com/post/duckdb-carolina/dbeaver-query.png" class="img-fluid"></p>
<p><img src="https://emilyriederer.com/post/duckdb-carolina/dbeaver-er.png" class="img-fluid"></p>
<p>Notably <strong>if you are using relative file paths in your view definitions, you have to launch DBeaver from your command line after moving into the appropriate working directory</strong>. (Thanks to <a href="https://twitter.com/Mause_me/status/1571126401482510336?s=20&amp;t=uYOnuHSjZcjkrbwYb0aXvA">Elliana May on Twitter</a> for the pointer.) (In the terminal: <code>cd my/dir/path; dbeaver</code>)</p>
</section>
</section>
<section id="codespaces-demo" class="level2">
<h2 class="anchored" data-anchor-id="codespaces-demo">Codespaces Demo</h2>
<p>So can DuckDB help analysts wrangle the whole state of North Carolina with 8GB RAM? To find out, launch a GitHub Codespaces from the <a href="https://github.com/emilyriederer/nc-votes-duckdb">nc-votes-duckdb</a> repo and see for yourself!</p>
<ol type="1">
<li><p>Launch on Codespaces</p></li>
<li><p>Set-up environment:</p></li>
</ol>
<pre><code>python3 -m venv venv
source venv/bin/activate
python3 -m pip install -r requirements.txt</code></pre>
<ol start="3" type="1">
<li>Pull all raw data:</li>
</ol>
<pre><code>chmod +x etl/extract-all.sh
etl/extract-all.sh</code></pre>
<ol start="4" type="1">
<li>Transform all raw data:</li>
</ol>
<pre><code>chmod +x etl/transform-all.sh
etl/transform-all.sh</code></pre>
<ol start="5" type="1">
<li>Create duckdb database:</li>
</ol>
<pre><code>python etl/load-db.py</code></pre>
<ol start="6" type="1">
<li>(Optional) Install duckdb CLI</li>
</ol>
<pre><code>chmod +x get-duckdb-cli.sh
./get-duckdb-cli.sh</code></pre>
<ol start="7" type="1">
<li>Run sample queries</li>
</ol>
<p>7a. Run sample queries in CLI</p>
<p>Launch the CLI:</p>
<pre><code>./duckdb nc.duckdb
.timer on</code></pre>
<p>(Note: you can exit CLI with Ctrl+D)</p>
<p>Try out some sample queries. For example, we might wonder how many past general elections that early voters have voted in before:</p>
<pre><code>with voter_general as (
select early_vote.ncid, count(1) as n
from 
  early_vote 
  left join 
  hist_gen 
  on early_vote.ncid = hist_gen.ncid 
group by 1)
select n, count(1) as freq
from voter_general
group by 1
order by 1
;</code></pre>
<p>And, this question is more interesting if we join on registration data to learn how many prior general elections each voter was eligible to vote in:</p>
<pre><code>with voter_general as (
select 
  early_vote.ncid, 
  extract('year' from register.registr_dt) as register_year, 
  count(1) as n
from 
  early_vote 
  left join 
  hist_gen 
  on early_vote.ncid = hist_gen.ncid 
  left join
  register
  on early_vote.ncid = register.ncid
group by 1,2)
select
  n, 
  case 
  when register_year &lt; 2012 then 'Pre-2012'
  else register_year
  end as register_year,
  count(1) as freq
from voter_general
group by 1,2
order by 1,2
;</code></pre>
<p>(Yes, of course <em>date</em> matters more than year here, etc. etc. This is purely to demonstrate <code>duckdb</code> not rigorous analysis!)</p>
<p>7b. Run sample queries in python</p>
<p>In python: See sample queries in <code>test-query.py</code> file</p>
<ol start="8" type="1">
<li>Run <code>free</code> in the terminal to marvel at what 8GB of RAM can do!</li>
</ol>


</section>

 ]]></description>
  <category>data</category>
  <category>sql</category>
  <guid>https://emilyriederer.com/post/duckdb-carolina/</guid>
  <pubDate>Sun, 25 Sep 2022 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/duckdb-carolina/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Oh, I’m sure it’s probably nothing</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/nulls-polyglot/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/nulls-polyglot/featured.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Photo credit to <a href="https://unsplash.com/@davideragusa">Davide Ragusa</a> on Unsplash</figcaption>
</figure>
</div>
<p>Language interoperability and different ways of enabling “polyglot” workflows have seemed to take centerstage in the data world recently:</p>
<ul>
<li><a href="https://arrow.apache.org/">Apache Arrow</a> promises a language-independent memory format for interoperability, - <a href="https://www.rstudio.com/blog/rstudio-is-becoming-posit/">RStudio</a> its rebranding as Posit to cement their place as a leader in language-agnostic data tooling,</li>
<li>RStudio simultaneously announced <a href="https://quarto.org/">Quarto</a> as an interoperable alternative to RMarkdown which will treat python, Julia, and JS as first-class citizens</li>
<li>dbt has released its beta of <a href="https://docs.getdbt.com/docs/building-a-dbt-project/building-models/python-models">python models</a> to extend is previously SQL-focused paradigm</li>
</ul>
<p>As a general matter, these are all exciting advances with great potential to aid in different workflows <em>when used judiciously</em>. However, it also poses the question: what cognitive burdens do we alleviate and which do we add when our projects begin to leverage multiple languages?</p>
<p>Despite common data analysis tools like SQL, R, and python being high-level languages with declarative interfaces (in the case of R’s <code>tidyverse</code> and python’s <code>pandas</code>), successful usage still requires understanding the underlying assumptions and operations of each tool. There is not such thing as a truly declarative language; only those that generally make decisions that the user likes well-enough to ask for the “what” and delegate the “how”. These differences can emerge at many different levels: such as foundational issues like whether data structures are copied or modified in-place or broader design choices like default hyperparameters in machine learning libraries (e.g.&nbsp;python’s <code>scikitlearn</code> notoriously uses regularized logistic regression as the default for logistic regression.) Somewhere along that spectrum lies the fickle issue of handling null values.</p>
<p>In this post, I recap a quick case study of how incautious null handling risks data analysis validity. Then, taking a step back, I compare how R, python, and SQL behave differently when confront with null values and the implications for analysts switching between languages.</p>
<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TLDR</h2>
<p>A summary of these different behaviors is provided below:</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><strong>R</strong></th>
<th style="text-align: center;"><strong>python</strong></th>
<th style="text-align: center;"><strong>SQL</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><em>Column Aggregation</em></td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">np: NA<br>pd: Value</td>
<td style="text-align: center;">Value</td>
</tr>
<tr class="even">
<td style="text-align: center;"><em>Row-wise Transformation</em></td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><em>Joining</em></td>
<td style="text-align: center;">Match by default</td>
<td style="text-align: center;">Match</td>
<td style="text-align: center;">No match</td>
</tr>
<tr class="even">
<td style="text-align: center;"><em>Filtering</em></td>
<td style="text-align: center;">No match</td>
<td style="text-align: center;">Match</td>
<td style="text-align: center;">No match</td>
</tr>
</tbody>
</table>
</section>
<section id="case-study" class="level2">
<h2 class="anchored" data-anchor-id="case-study">Case Study</h2>
<p>Before comparing different languages, let’s walk through a brief case study to see all the way that “lurking” nulls can surprise a junior analyst in any one language and observe a few different “contours” of the problem space.</p>
<p>Consider two tables in a retailer’s database. The <code>spend</code> table reports total sales by month and store identifier (null if online).</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  STORE_ID MONTH AMT_SPEND
1        1     1 100.12011
2        2     1 100.31441
3       NA     1 100.40517
4        1     2  99.67098
5        2     2  98.39703
6       NA     2  98.81231
7        1     3 102.27124
8        2     3 100.20843
9       NA     3        NA</code></pre>
</div>
</div>
<p>Similarly, the <code>returns</code> table reports returned sales at the same grain.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  STORE_ID MONTH AMT_RETURN
1        1     1         NA
2        2     1   9.972159
3       NA     1  10.071639
4        1     2   9.798444
5        2     2  10.254347
6       NA     2   9.881071
7        1     3  10.108880
8        2     3   9.951398
9       NA     3   9.849277</code></pre>
</div>
</div>
<p>In both cases, nulls are used in the <code>'AMT_*'</code> fields to denote zeros for the respective <code>month x store_id</code> combinations`.</p>
<p>To calculate something as simple as the average gross spend per store across months, an analyst might attempt to write:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb3-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span> </span>
<span id="cb3-2">  store_id, </span>
<span id="cb3-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">avg</span>(amt_spend)</span>
<span id="cb3-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> spend</span>
<span id="cb3-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">group</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">by</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb3-6"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">order</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">by</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">store_id</th>
<th style="text-align: right;">avg(amt_spend)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">NA</td>
<td style="text-align: right;">99.60874</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">100.68744</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">99.63996</td>
</tr>
</tbody>
</table>
</div>
<p>However, because SQL silently drops nulls in column aggregations, the online spend is not appropriately “penalized” for its lack of March spend. The averages across all three stores look nearly equal.</p>
<p>Not only is this answer “wrong”, it can also be thought of as fundamentally changing the <strong>computand</strong> (a word I just made up. In statistics, we talk about estimands as “the conceptual thing we are trying to estimate with an estimator”. Here, we aren’t estimating anything – just computing. But, there’s still a concentual “thing we are trying to measure” and in this case, it’s our <em>tools</em> and not our <em>methods</em> that are imposing assumptions on that) to one that answers a fundamentally different question:</p>
<p>Instead of measuring “average monthly spend in Q1 by store”, we’re measuring “averaging monthly spend in Q1 by store <em>conditional on</em> there being spend”.</p>
<p>To obtain the correct result, one would write:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb4-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span> </span>
<span id="cb4-2">  store_id, </span>
<span id="cb4-3">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-- wrong answers</span></span>
<span id="cb4-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">avg</span>(amt_spend) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> wrong1,  </span>
<span id="cb4-5">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(amt_spend) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">count</span>(amt_spend) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> wrong2,</span>
<span id="cb4-6">  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-- right answers</span></span>
<span id="cb4-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(amt_spend) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">count</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> right1,</span>
<span id="cb4-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">avg</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(amt_spend, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> right2</span>
<span id="cb4-9"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> spend</span>
<span id="cb4-10"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">group</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">by</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb4-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">order</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">by</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">store_id</th>
<th style="text-align: right;">wrong1</th>
<th style="text-align: right;">wrong2</th>
<th style="text-align: right;">right1</th>
<th style="text-align: right;">right2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">NA</td>
<td style="text-align: right;">99.60874</td>
<td style="text-align: right;">99.60874</td>
<td style="text-align: right;">66.40583</td>
<td style="text-align: right;">66.40583</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">100.68744</td>
<td style="text-align: right;">100.68744</td>
<td style="text-align: right;">100.68744</td>
<td style="text-align: right;">100.68744</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">99.63996</td>
<td style="text-align: right;">99.63996</td>
<td style="text-align: right;">99.63996</td>
<td style="text-align: right;">99.63996</td>
</tr>
</tbody>
</table>
</div>
<p>With a better understand of gross sales, the analyst might next proceed to compute net sales.</p>
<p>This first requires joining the <code>spend</code> and <code>returns</code> tables. Naively, they might attempt:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb5-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span> </span>
<span id="cb5-2">  spend.<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>,</span>
<span id="cb5-3">  returns.amt_return</span>
<span id="cb5-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> </span>
<span id="cb5-5">  spend</span>
<span id="cb5-6">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">inner</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">join</span></span>
<span id="cb5-7">  returns </span>
<span id="cb5-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">on</span></span>
<span id="cb5-9">  spend.store_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> returns.store_id <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span></span>
<span id="cb5-10">  spend.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> returns.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span></span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">STORE_ID</th>
<th style="text-align: right;">MONTH</th>
<th style="text-align: right;">AMT_SPEND</th>
<th style="text-align: right;">amt_return</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">100.12011</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">100.31441</td>
<td style="text-align: right;">9.972159</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">99.67098</td>
<td style="text-align: right;">9.798444</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">98.39703</td>
<td style="text-align: right;">10.254347</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">102.27124</td>
<td style="text-align: right;">10.108880</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">100.20843</td>
<td style="text-align: right;">9.951398</td>
</tr>
</tbody>
</table>
</div>
<p>However, this once again fails. Why? Although SQL handled nulls “permissively” when aggregating a column, it took a stricted stance when making the comparison on <code>spend.store_id = returns.store_id</code> in the join clause. SQL doesn’t recognize different nulls as equal. To the extent than null means “I dunno” versus “The field is not relevant to this observation”, it’s reasonable that SQL should find it hard to decide whether two “I dunno”s are equal.</p>
<p>Once again, this isn’t a “random” or inconsequential error. Continuing to use this corrupted dataset changes the computand from “net sales by month” to “net sales by month at physical retail locations”.</p>
<p>To remedy this, we can force <code>store_id</code> to take on a value:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb6-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span></span>
<span id="cb6-2">  spend.<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>,</span>
<span id="cb6-3">  returns.amt_return</span>
<span id="cb6-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> </span>
<span id="cb6-5">  spend</span>
<span id="cb6-6">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">inner</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">join</span></span>
<span id="cb6-7">  returns </span>
<span id="cb6-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">on</span></span>
<span id="cb6-9">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(spend.store_id, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(returns.store_id, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span></span>
<span id="cb6-10">  spend.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> returns.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span></span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">STORE_ID</th>
<th style="text-align: right;">MONTH</th>
<th style="text-align: right;">AMT_SPEND</th>
<th style="text-align: right;">amt_return</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">100.12011</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">100.31441</td>
<td style="text-align: right;">9.972159</td>
</tr>
<tr class="odd">
<td style="text-align: right;">NA</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">100.40517</td>
<td style="text-align: right;">10.071639</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">99.67098</td>
<td style="text-align: right;">9.798444</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">98.39703</td>
<td style="text-align: right;">10.254347</td>
</tr>
<tr class="even">
<td style="text-align: right;">NA</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">98.81231</td>
<td style="text-align: right;">9.881071</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">102.27124</td>
<td style="text-align: right;">10.108880</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">100.20843</td>
<td style="text-align: right;">9.951398</td>
</tr>
<tr class="odd">
<td style="text-align: right;">NA</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">9.849277</td>
</tr>
</tbody>
</table>
</div>
<p>And next we proceed with computing sales by month net of returns across all stores:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb7-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span></span>
<span id="cb7-2">  spend.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span>, </span>
<span id="cb7-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(amt_spend <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> amt_return) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> net_spend</span>
<span id="cb7-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> </span>
<span id="cb7-5">  spend</span>
<span id="cb7-6">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">inner</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">join</span></span>
<span id="cb7-7">  returns </span>
<span id="cb7-8">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">on</span></span>
<span id="cb7-9">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(spend.store_id, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(returns.store_id, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span></span>
<span id="cb7-10">  spend.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> returns.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span></span>
<span id="cb7-11"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">group</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">by</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb7-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">order</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">by</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">month</th>
<th style="text-align: right;">net_spend</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">180.6758</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">266.9465</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">182.4194</td>
</tr>
</tbody>
</table>
</div>
<p>However, by now, you should not be surprised that this result is also incorrect. If we inspect the sequence of computations, we realize that SQL is also stricter in its null handing in <em>rowwise computations</em> than <em>column-wise aggregations</em>. The subtraction of <code>amt_spend</code> and <code>amt_return</code> obliterates the total when either is null. So, we fail to include the gross spend at Store 1 in January simply because there were no returns (and vice versa for Internet sales in March).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb8-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span></span>
<span id="cb8-2">  spend.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span>, </span>
<span id="cb8-3">  spend.store_id,</span>
<span id="cb8-4">  amt_spend,</span>
<span id="cb8-5">  amt_return,</span>
<span id="cb8-6">  amt_spend <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> amt_return <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> net_spend</span>
<span id="cb8-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> </span>
<span id="cb8-8">  spend</span>
<span id="cb8-9">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">inner</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">join</span></span>
<span id="cb8-10">  returns </span>
<span id="cb8-11">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">on</span></span>
<span id="cb8-12">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(spend.store_id, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(returns.store_id, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span></span>
<span id="cb8-13">  spend.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> returns.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span></span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">month</th>
<th style="text-align: right;">store_id</th>
<th style="text-align: right;">amt_spend</th>
<th style="text-align: right;">amt_return</th>
<th style="text-align: right;">net_spend</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">100.12011</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">NA</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">100.31441</td>
<td style="text-align: right;">9.972159</td>
<td style="text-align: right;">90.34225</td>
</tr>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">100.40517</td>
<td style="text-align: right;">10.071639</td>
<td style="text-align: right;">90.33353</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">99.67098</td>
<td style="text-align: right;">9.798444</td>
<td style="text-align: right;">89.87254</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">98.39703</td>
<td style="text-align: right;">10.254347</td>
<td style="text-align: right;">88.14268</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">98.81231</td>
<td style="text-align: right;">9.881071</td>
<td style="text-align: right;">88.93124</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">102.27124</td>
<td style="text-align: right;">10.108880</td>
<td style="text-align: right;">92.16236</td>
</tr>
<tr class="even">
<td style="text-align: right;">3</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">100.20843</td>
<td style="text-align: right;">9.951398</td>
<td style="text-align: right;">90.25704</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">NA</td>
<td style="text-align: right;">9.849277</td>
<td style="text-align: right;">NA</td>
</tr>
</tbody>
</table>
</div>
<p>A few ways to get the correct answer are shown below:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb9-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span></span>
<span id="cb9-2">  spend.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span>, </span>
<span id="cb9-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(amt_spend,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(amt_return,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> right1,</span>
<span id="cb9-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(amt_spend) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(amt_return) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> right2</span>
<span id="cb9-5"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> </span>
<span id="cb9-6">  spend</span>
<span id="cb9-7">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">inner</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">join</span></span>
<span id="cb9-8">  returns </span>
<span id="cb9-9">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">on</span></span>
<span id="cb9-10">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(spend.store_id, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">coalesce</span>(returns.store_id, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">999</span>) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span></span>
<span id="cb9-11">  spend.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> returns.<span class="dt" style="color: #AD0000;
background-color: null;
font-style: inherit;">month</span></span>
<span id="cb9-12"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">group</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">by</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb9-13"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">order</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">by</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">month</th>
<th style="text-align: right;">right1</th>
<th style="text-align: right;">right2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">280.7959</td>
<td style="text-align: right;">280.7959</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">266.9465</td>
<td style="text-align: right;">266.9465</td>
</tr>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">172.5701</td>
<td style="text-align: right;">172.5701</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="observations" class="level2">
<h2 class="anchored" data-anchor-id="observations">Observations</h2>
<p>The preceding example hopefully illustrates a few points:</p>
<ul>
<li>Nulls can cause issues in the most basic of analyses</li>
<li>Beyond causing random or marginal errors, null handling changes the questions being answered</li>
<li>Even within a language, null handling may feel inconsistent (w.r.t. strictness) across different operations</li>
</ul>
<p>So, with that, let’s compare languages!</p>
</section>
<section id="comparison" class="level2">
<h2 class="anchored" data-anchor-id="comparison">Comparison</h2>
<p>Below, we compare how R, SQL, and python handle column aggregation, rowwise transformation, joining, and filtering.</p>
<section id="aggregation" class="level3">
<h3 class="anchored" data-anchor-id="aggregation">Aggregation</h3>
<p>SQL, as we saw before, simply ignores nulls in aggregation functions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb10-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span> </span>
<span id="cb10-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(x) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> sum_x, </span>
<span id="cb10-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(<span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span>(x <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">null</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)) <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> n_null_x</span>
<span id="cb10-4"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> tbl</span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">sum_x</th>
<th style="text-align: right;">n_null_x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
</tr>
</tbody>
</table>
</div>
<p>Built by and for statistician’s, R is scandalized at the very idea of attempting to do math with null columns. For aggregation functions, it returns <code>NA</code> as a form of protest should any entry of the vector provided be null. (This can be overridden with the <code>na.rm</code> parameter.)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1">x <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>)</span>
<span id="cb11-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(x)</span>
<span id="cb11-3"></span>
<span id="cb11-4">df <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> x)</span>
<span id="cb11-5">dplyr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(df, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sum</span>(x))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] NA</code></pre>
</div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">NA</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>When it comes to python, well, it depends. Base and <code>numpy</code> operations act more like R whereas <code>pandas</code> aggregation acts more like SQL.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb13-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb13-3">x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,np.nan]</span>
<span id="cb13-4">y <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>]</span>
<span id="cb13-5">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span>:x,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'y'</span>:y})</span>
<span id="cb13-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(x)</span>
<span id="cb13-7">np.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>(x)</span>
<span id="cb13-8">df.agg({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'sum'</span>]})</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>nan
nan
       x
sum  3.0</code></pre>
</div>
</div>
</section>
<section id="transformation" class="level3">
<h3 class="anchored" data-anchor-id="transformation">Transformation</h3>
<p>All of SQL, R, and python return NA when <code>NA</code>s are used in atomic or rowwise transformations.</p>
<p>In SQL:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb15-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>, x<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>y <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">as</span> z</span>
<span id="cb15-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> tbl</span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">x</th>
<th style="text-align: right;">y</th>
<th style="text-align: right;">z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">-2</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">-2</td>
</tr>
<tr class="odd">
<td style="text-align: right;">NA</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">NA</td>
</tr>
</tbody>
</table>
</div>
<p>In R:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1">df <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>)</span>
<span id="cb16-2">dplyr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(df, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">z =</span> x<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>y)</span>
<span id="cb16-3"></span>
<span id="cb16-4">df<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>z <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">with</span>(df, x<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>y)</span>
<span id="cb16-5">df</span></code></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">x</th>
<th style="text-align: right;">y</th>
<th style="text-align: right;">z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">-2</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">-2</td>
</tr>
<tr class="odd">
<td style="text-align: right;">NA</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">NA</td>
</tr>
</tbody>
</table>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">x</th>
<th style="text-align: right;">y</th>
<th style="text-align: right;">z</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">-2</td>
</tr>
<tr class="even">
<td style="text-align: right;">2</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">-2</td>
</tr>
<tr class="odd">
<td style="text-align: right;">NA</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">NA</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>In python:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">np.array(x) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> np.array(y)</span>
<span id="cb17-2">df.assign(z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> d: d.x <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> d.y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>array([-2., -2., nan])
     x  y    z
0  1.0  3 -2.0
1  2.0  4 -2.0
2  NaN  5  NaN</code></pre>
</div>
</div>
</section>
<section id="joining" class="level3">
<h3 class="anchored" data-anchor-id="joining">Joining</h3>
<p>The situation with joins may feel like the opposite of aggregation. Here, R and python’s most popular data wrangling packages are more permissive than SQL.</p>
<p>As we saw in the case study, SQL does not match on nulls.</p>
<p>Consider <code>tbl1</code> and <code>tbl2</code> as shown below:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb19-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> tbl1</span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">A</th>
<th style="text-align: left;">B</th>
<th style="text-align: left;">X</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: left;">NA</td>
<td style="text-align: left;">TRUE</td>
</tr>
</tbody>
</table>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb20-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> tbl2</span></code></pre></div>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">A</th>
<th style="text-align: left;">B</th>
<th style="text-align: left;">Y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: left;">NA</td>
<td style="text-align: left;">FALSE</td>
</tr>
</tbody>
</table>
</div>
<p>Attempts to join return no results:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb21-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span> tbl1.<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>, tbl2.Y </span>
<span id="cb21-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> </span>
<span id="cb21-3">  tbl1 <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">inner</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">join</span> tbl2 </span>
<span id="cb21-4">  <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">on</span> </span>
<span id="cb21-5">  tbl1.A <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tbl2.A <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">and</span> </span>
<span id="cb21-6">  tbl1.B <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tbl2.B</span></code></pre></div>
<table>
<thead>
<tr class="header">
<th style="text-align: right;">A</th>
<th style="text-align: left;">B</th>
<th style="text-align: left;">X</th>
<th style="text-align: left;">y</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
</div>
<p>In contrast, default behavior for base R’s <code>merge</code> and <code>dplyr</code> <em>does match</em> on nulls. (Although, either behavior can be altered with the <code>incomparables</code> or <code>na_matches</code> arguments, respectively.)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1">df1 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">A =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">B =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>)</span>
<span id="cb22-2">df2 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">A =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">B =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">Y =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">FALSE</span>)</span>
<span id="cb22-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">merge</span>(df1, df2, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B"</span>))</span>
<span id="cb22-4">dplyr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">inner_join</span>(df1, df2, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"B"</span>))</span></code></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">A</th>
<th style="text-align: left;">B</th>
<th style="text-align: left;">X</th>
<th style="text-align: left;">Y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: left;">NA</td>
<td style="text-align: left;">TRUE</td>
<td style="text-align: left;">FALSE</td>
</tr>
</tbody>
</table>
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: right;">A</th>
<th style="text-align: left;">B</th>
<th style="text-align: left;">X</th>
<th style="text-align: left;">Y</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">1</td>
<td style="text-align: left;">NA</td>
<td style="text-align: left;">TRUE</td>
<td style="text-align: left;">FALSE</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Similarly, <code>pandas</code> also matches on nulls for joining.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb23-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb23-3">df1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, np.nan, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>]], columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'X'</span>])</span>
<span id="cb23-4">df2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, np.nan, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>]], columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Y'</span>])</span>
<span id="cb23-5">pd.merge(df1, df2, on <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   A   B     X      Y
0  1 NaN  True  False</code></pre>
</div>
</div>
<p><code>R</code> and <code>python</code>’s behavior here seems most surprising. One might expect joining to work the same as raw logical evaluation works. However, neither language “likes” null comparison in its raw form. Instead, the default behavior is intentionally altered in these higher-level joining functions.</p>
<p>In R:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span></span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] NA</code></pre>
</div>
</div>
<p>In python:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">np.nan <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> np.nan</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>False</code></pre>
</div>
</div>
</section>
<section id="filtering" class="level3">
<h3 class="anchored" data-anchor-id="filtering">Filtering</h3>
<p>Finally, both SQL and R drop null records used in filtering statements since comparisons with these values are incapable of returning a TRUE/FALSE value that is used to subset the rows. In python, however, pandas does preserve nulls in filter conditions.</p>
<p>Using the same <code>tbl1</code> shown above, we can also confirm that SQL proactively drops nulls in where clauses where they cannot be readily compared to non-null values. This seems quite consistent with its behavior in the joining case.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb29-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">select</span> A, B, X </span>
<span id="cb29-2"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">from</span> tbl1 </span>
<span id="cb29-3"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">where</span> B <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span></code></pre></div>
<table>
<thead>
<tr class="header">
<th style="text-align: right;">a</th>
<th style="text-align: left;">b</th>
<th style="text-align: left;">x</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
</div>
<p>Both base R and <code>dplyr</code> paradigms follow suit here.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1">df1 <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">A =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">B =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NA</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">X =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>)</span>
<span id="cb30-2">df1[df1<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>B <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,]</span>
<span id="cb30-3">dplyr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(df1, B <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span></code></pre></div>
<div class="cell-output-display">
<table class="table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">A</th>
<th style="text-align: left;">B</th>
<th style="text-align: left;">X</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">NA</td>
<td style="text-align: right;">NA</td>
<td style="text-align: left;">NA</td>
<td style="text-align: left;">NA</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th style="text-align: right;">A</th>
<th style="text-align: left;">B</th>
<th style="text-align: left;">X</th>
</tr>
</thead>
<tbody>
</tbody>
</table>
</div>
</div>
<p>However, bucking the trend, multiple approaches to subsetting <code>pandas</code> data will not drop nulls in filtering comparisons.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1">df1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame([[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, np.nan, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>]], columns <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'A'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'X'</span>])</span>
<span id="cb31-2">df1[df1.B <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb31-3">df1.query(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'B != 1'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   A   B     X
0  1 NaN  True
   A   B     X
0  1 NaN  True</code></pre>
</div>
</div>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In data computation and analysis, the devil is often in the details. It’s not breaking news that low-level reasoning on the careful handling of null values can jeopardize the resulting analyses. However, as analysts take on increasingly complex tasks and using a plehora of different tools, it’s more important than ever for both data producers and consumers to consider the choices they are making in encoding and handling these values across the stack.</p>


</section>

 ]]></description>
  <category>rstats</category>
  <category>python</category>
  <category>sql</category>
  <category>data</category>
  <category>data-disasters</category>
  <guid>https://emilyriederer.com/post/nulls-polyglot/</guid>
  <pubDate>Mon, 05 Sep 2022 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/nulls-polyglot/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Update: grouped data quality check PR merged to dbt-utils</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/grouping-data-quality-update/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/grouping-data-quality-update/featured.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Photo credit to <a href="https://unsplash.com/@greysonjoralemon">Greyson Joralemon</a> on Unsplash</figcaption>
</figure>
</div>
<p>Last fall, I wrote about the <a href="https://www.emilyriederer.com/post/grouping-data-quality/">unreasonably effectiveness of grouping in data quality checks</a>. In this follow-up, I want to share that my <a href="https://github.com/dbt-labs/dbt-utils/pull/633">pull request</a> for such features has just been merged into the development branch of the <code>dbt-utils</code> package, a common add-on to the <code>dbt</code> data transformation stack. This feature will officially “go live” in the 1.0.0 version release that is planned for later this fall.</p>
<p>In this brief post, I’ll recall the benefits of such checks (which my original post further illustrates with NYC subway data) and demonstrate how these checks can now be implemented in <code>dbt-utils</code>.</p>
<p>For those interested, I’ll also provide a brief overview of how I implemented this change, but I recommend checking out the PR itself for complete details.</p>
<section id="recap" class="level2">
<h2 class="anchored" data-anchor-id="recap">Recap</h2>
<p>To recap the benefits of such checks from my initial post:</p>
<ul>
<li>Some data checks can only be expressed within a group (e.g.&nbsp;ID values should be unique within a group but can be repeated between groups)</li>
<li>Some data checks are more precise when done by group (e.g.&nbsp;not only should table row-counts be equal but the counts within each group should be equal)</li>
</ul>
<p>Of course, these benefits are more or less relevant to different types of data checks. My PR updates the following tests:</p>
<ul>
<li>equal_rowcount()</li>
<li>recency()</li>
<li>fewer_rows_than()</li>
<li>at_least_one()</li>
<li>not_constant()</li>
<li>non_null_proportion()</li>
<li>sequential_values()</li>
</ul>
<p>Of these checks, most fall in the category of providing more rigor when being conducted at the group level. Only the <code>sequential_values()</code> test is often unable to be expressed without grouping.</p>
</section>
<section id="demo" class="level2">
<h2 class="anchored" data-anchor-id="demo">Demo</h2>
<p><a href="https://docs.getdbt.com/docs/building-a-dbt-project/tests">Data tests</a> in <code>dbt</code> are specified in the <code>schema.yml</code> file for relevant models. Adding grouping to the tests listed above will now be as simple as adding a <code>group_by_columns</code> key-value pair to the tests, as desired, which accepts either a single variable name or a list of variables to be used for grouping.</p>
<p><code>{yaml eval = FALSE}   - name: data_test_at_least_one     columns:       - name: field         tests:           - dbt_utils.at_least_one:               group_by_columns: ['grouping_column']</code></p>
<p>For those that have not used <code>dbt</code>’s data testing framework before, this configuration is then used to generate SQL (now with the custom <code>GROUP BY</code> clause) which are evaluated when <code>dbt test</code> is run.</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<p>In implementing this PR, I considered a few core principles:</p>
<ul>
<li>Make this feature as unobtrusive and isolated as possible with respect to the macros broader implementation</li>
<li>Follow standard DRY principles (e.g.&nbsp;specifically, render needed text as few times as possible)</li>
<li>Implement consistently across macros</li>
</ul>
<p>With these principles in mind, the majority of implementations are like that of the <code>recency</code> macro where all relevant SQL strings are pre-computed:</p>
<pre><code>{% set threshold = dbt_utils.dateadd(datepart, interval * -1, dbt_utils.current_timestamp()) %}
{% if group_by_columns|length() &gt; 0 %}
  {% set select_gb_cols = group_by_columns|join(' ,') + ', ' %}
  {% set groupby_gb_cols = 'group by ' + group_by_columns|join(',') %}
{% endif %}</code></pre>
<p>The main deviations to this were the sequential() macro (requiring a window function) and the equal_rowcount()/fewer_rows_than() (requiring joins)</p>


</section>

 ]]></description>
  <category>data</category>
  <category>dbt</category>
  <guid>https://emilyriederer.com/post/grouping-data-quality-update/</guid>
  <pubDate>Fri, 26 Aug 2022 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/grouping-data-quality-update/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>The Data Engineering Podcast: Column Names as Contracts</title>
  <link>https://emilyriederer.com/talk/pod-tdep-colnames/</link>
  <description><![CDATA[ 



<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Listen</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-play"></i> <a href="https://www.dataengineeringpodcast.com/controlled-vocabulary-with-dbtplyr-episode-252/">Pod</a> </span><br>
<span><i class="bi bi-download"></i><a href="tdep_col-name-contracts.mp3">Download MP3</a></span><br>
<span><i class="bi bi-pencil"></i> <a href="../../post/column-name-contracts/">Post - Column Names as Contracts</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>Communication and shared context are the hardest part of any data system. In recent years the focus has been on data catalogs as the means for documenting data assets, but those introduce a secondary system of record in order to find the necessary information. In this episode Emily Riederer shares her work to create a controlled vocabulary for managing the semantic elements of the data managed by her team and encoding it in the schema definitions in her data warehouse. She also explains how she created the dbtplyr package to simplify the work of creating and enforcing your own controlled vocabularies.</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<iframe src="https://player.fireside.fm/v2/fE6B9ALy+6SCH5nMW?theme=dark" width="740" height="200" frameborder="0" scrolling="no">
</iframe>
</div>
</div>
</div>



 ]]></description>
  <category>data</category>
  <guid>https://emilyriederer.com/talk/pod-tdep-colnames/</guid>
  <pubDate>Wed, 12 Jan 2022 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/pod-tdep-colnames/featured.PNG" medium="image"/>
</item>
<item>
  <title>Using databases with Shiny</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/shiny-db/</link>
  <description><![CDATA[ 




<p>Shiny apps are R’s answer to building interface-driven applications that help expose important data, metrics, algorithms, and more with end-users. However, the more interesting work that your Shiny app allows users to do, the more likely users are to want to save, return to, and alter some of the ways that they interacted with your work.</p>
<p>This creates a need for <strong>persistent storage</strong> in your Shiny application, as opposed to the ephemeral in-memory of basic Shiny applications that “forget” the data that they generated as soon as the application is stopped.</p>
<p>Relational databases are a classic form of persistent storage for web applications. Many analysts may be familiar with <em>querying</em> relational databases to retrieve data, but <em>managing</em> a database for use with a web application is slightly more complex. You’ll find yourself needing to define tables, secure data, and manage connections. More importantly, you might worry about what things that you do not know you should be worrying about.</p>
<p>This post provides some tips, call-outs, and solutions for using a relational database for persistent storage with Shiny. In my case, I rely on a Shiny app built with the <a href="https://thinkr-open.github.io/golem/"><code>golem</code> framework</a> and served on the Digital Ocean App platform.</p>
<section id="databases-options-for-storage" class="level2">
<h2 class="anchored" data-anchor-id="databases-options-for-storage">Databases &amp; Options for Storage</h2>
<p>Dean Attali’s <a href="https://deanattali.com/blog/shiny-persistent-data-storage/">blog post on persistent storage</a> compares a range of options for persistent storage including databases, S3 buckets, Google Drive, and more.</p>
<p>For my application, I anticipated the need to store and retrieve sizable amounts of structured data, so using a relational database seemed like a good option. Since I was hosting my application on <a href="https://m.do.co/c/6c5fdc198503">Digital Ocean App Platform</a>, I could create a <a href="https://www.digitalocean.com/products/managed-databases/">managed Postgres database</a> with just a few button clicks. As I share in the “Key Issues” section, this solution offers some significant benefits in terms of security.</p>
<p>For more information on different options for hosting Shiny apps and some insight into why I chose Digital Ocean, check out Peter Solymos’ excellent blog on <a href="https://hosting.analythium.io/">Hosting Data Apps</a>.</p>
</section>
<section id="talking-to-your-database-through-shiny" class="level2">
<h2 class="anchored" data-anchor-id="talking-to-your-database-through-shiny">Talking to your database through Shiny</h2>
<p>General information on working with databases with R is included on RStudio’s <a href="https://db.rstudio.com/">excellent website</a>. Below, I focus on a few topics specific to databases with Shiny, Shiny apps built in the <code>{golem}</code> framework, and Shiny apps served on Digital Ocean in particular.</p>
<section id="creating-a-database" class="level3">
<h3 class="anchored" data-anchor-id="creating-a-database">Creating a database</h3>
<p>To create a database for my application in DigitalOcean, I simply went to:</p>
<p><code>Settings &gt; Add Component &gt; Database</code></p>
<p>This creates a fully-managed Postgres databases so you do not have to thing a ton about the underlying set-up or configuration.</p>
<p>At the time on writing, I was able to add a 1GB Dev Database for /$7 / month. For new users, DigitalOcean offers a generous number of free credits for use in the first 60 days. For a more mature product, one can add or switch to a production-ready Managed Database.</p>
<p>After a few minutes, the database has launched and its Connection Parameters are provided, which look something like this:</p>
<pre><code>host     : abc.b.db.ondigitalocean.com
port     : 25060
username : db
password : abc123
database : db
sslmode  : require</code></pre>
<p>By default, the Dev Database registers your application as a Trusted Source, meaning that only traffic from the application can attempt to access the database. As the <a href="https://docs.digitalocean.com/products/databases/postgresql/how-to/secure/#firewalls">documentation</a> explains, this type of firewall improves security by preventing against brute-force password or denial-of-service attacks from the outside.</p>
<p><em>Note: If you just want to experiment with databases and Shiny but aren’t using an in-production, served application, you can mostly skip this step and use the “Dev” approach that is discuss in “Dev versus Prod” subsection of “Key Issues” below.</em></p>
</section>
<section id="connecting-to-the-database" class="level3">
<h3 class="anchored" data-anchor-id="connecting-to-the-database">Connecting to the database</h3>
<p>We can use the connection parameters provided to connect to the database using R’s <code>DBI</code> package.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">con <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbConnect</span>(RPostgres<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">Postgres</span>(),</span>
<span id="cb2-2">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">host   =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"aabc.b.db.ondigitalocean.com"</span>,</span>
<span id="cb2-3">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dbname =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"db"</span>,</span>
<span id="cb2-4">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">user      =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"db"</span>,</span>
<span id="cb2-5">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">password  =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"abc123"</span>,</span>
<span id="cb2-6">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">port     =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25060</span>)</span></code></pre></div>
</div>
<p>We will talk about ways to not hardcode one’s password in the last section.</p>
</section>
<section id="creating-tables" class="level3">
<h3 class="anchored" data-anchor-id="creating-tables">Creating tables</h3>
<p>Next, you can set up tables in your database that your application will require.</p>
<p>If you know SQL DDL, you can write a <a href="https://www.tutorialspoint.com/sql_certificate/using_ddl_statements.html">CREATE TABLE statement</a> which defines a tables names, fields, and data types. However, this can feel verbose or uncomfortable to analysts who mostly use DML (e.g.&nbsp;<code>SELECT</code>, <code>FROM</code>, <code>WHERE</code>).</p>
<p>Fortunately, you can also define a table using R’s <code>DBI</code> package. First, create a simple dataframe with a single record to help R infer the appropriate and expected data types. Then pass the first <em>zero</em> rows of the table (essentially, only the schema) to <code>DBI::dbCreateTable()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">df <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">z =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.Date</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"2022-01-01"</span>))</span>
<span id="cb3-2">DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbCreateTable</span>(con, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">name =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"my_data"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">fields =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">head</span>(df, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span></code></pre></div>
</div>
<p>To prove that this works, I show a “round trip” of the data using an in-memory SQLite database. Note that this is <em>not</em> an option for persistent storage because in-memory databases are not persistent. This is only to “prove” that this approach can create database tables.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">con_lite <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbConnect</span>(RSQLite<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">SQLite</span>(), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">":memory:"</span>)</span>
<span id="cb4-2">df <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">z =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.Date</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"2022-01-01"</span>))</span>
<span id="cb4-3">DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbCreateTable</span>(con_lite, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">name =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"my_data"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">fields =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">head</span>(df, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>))</span>
<span id="cb4-4">DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbListTables</span>(con_lite)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "my_data"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbReadTable</span>(con_lite, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"my_data"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] x y z
&lt;0 rows&gt; (or 0-length row.names)</code></pre>
</div>
</div>
<p>But where should you run this script? You do <em>not</em> want to put this code in your app to run every time the app launches, but we just limited database traffic to the app so we cannot run it locally. Instead, you can run this code from the app’s <a href="https://docs.digitalocean.com/products/app-platform/concepts/console/">console</a>. (Alternatively, if you upgrade to a Managed Database, I believe you can also whitelist your local IP as another trusted source.)</p>
</section>
<section id="forming-the-connection-within-your-app" class="level3">
<h3 class="anchored" data-anchor-id="forming-the-connection-within-your-app">Forming the connection within your app</h3>
<p>Once your database is set-up and ready to go, you can begin to integrate it into your application.</p>
<p>I was using the <a href="https://thinkr-open.github.io/golem/"><code>golem</code> framework</a> for my application, so I connected to the database and made the initial data pull by adding the following lines in my top-level <code>app_server.R</code> file.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">con <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">db_con</span>()</span>
<span id="cb8-2">tbl_init <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbReadTable</span>(con, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"my_data"</span>)</span></code></pre></div>
</div>
<p>The custom <code>db_con()</code> function contains <em>roughly</em> the <code>DBI::dbConnect()</code> code we saw above, but I turned it into a function to incorporate some added complexity which I will describe shortly.</p>
<p>Most of the rest of my application uses Shiny modules, and this connection object and initial data pull can be seamless passed into either.</p>
<p>To see similar code in a full app, check out Colin Fay’s <a href="https://github.com/ColinFay/golemexamples/blob/master/golemqlite/R/app_server.R#L7"><code>golemqlite</code></a> project on Github.</p>
</section>
<section id="crud-operations" class="level3">
<h3 class="anchored" data-anchor-id="crud-operations">CRUD operations</h3>
<p>CRUD operations (Create, Read, Update, Delete) are at the heart of any interactive application with persistent data storage.</p>
<p>Interacting with your database within Shiny begins to look like more rote Shiny code. I do not describe this process in much detail since it is quite specific to what your app is trying to accomplish, but <a href="https://www.tychobra.com/posts/2020-01-29-shiny-crud-traditional/">this blog post</a> provides some nice examples.</p>
<p>In short:</p>
<ul>
<li>To add records to the table, you can use <code>DBI::dbAppendTable()</code></li>
<li>To remove records from the table, you can construct a <code>DELETE FROM my_data WHERE &lt;conditions&gt;</code> statement and run it with <code>DBI::dbExecute()</code></li>
</ul>
<p>Some cautions on the second piece are included in the “Key Issues” section.</p>
</section>
</section>
<section id="key-issues" class="level2">
<h2 class="anchored" data-anchor-id="key-issues">Key Issues</h2>
<p>Adding a permanent data store to your application can open up a lot of exciting new functionality. However, it may create some challenges that your typical data analyst or Shiny developer has not faced before. In this last section, I highlight a few key issues that you should be aware of and provide some recommendations.</p>
<section id="securing-data-transfer" class="level3">
<h3 class="anchored" data-anchor-id="securing-data-transfer">Securing data transfer</h3>
<p>Already, we have one safeguard in place for data security since our application is the only Trusted Source able to interface with our database.</p>
<p>But, just like we secure our database credentials, it becomes important to think about securing the database itself. This is made easy with DigitalOcean because data is <a href="https://docs.digitalocean.com/products/databases/">end-to-end encrypted</a>, but depending on how or by whom your data is managed, this is something to bear in mind.</p>
</section>
<section id="securing-database-credentials" class="level3">
<h3 class="anchored" data-anchor-id="securing-database-credentials">Securing database credentials</h3>
<p>No matter how safe the data itself is, it still may be at risk if anyone can obtain our database credentials.</p>
<p>Previously, I demonstrated how to connect to a database from R like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">con <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbConnect</span>(RPostgres<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">Postgres</span>(),</span>
<span id="cb9-2">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">host   =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"aabc.b.db.ondigitalocean.com"</span>,</span>
<span id="cb9-3">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dbname =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"db"</span>,</span>
<span id="cb9-4">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">user      =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"db"</span>,</span>
<span id="cb9-5">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">password  =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"abc123"</span>,</span>
<span id="cb9-6">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">port     =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25060</span>)</span></code></pre></div>
</div>
<p>However, you should never ever put your password in plaintext like this. Instead, you can use <em>environment variables</em> to store the value of sensitive credentials like a password or even a username like this:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1">con <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbConnect</span>(RPostgres<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">Postgres</span>(),</span>
<span id="cb10-2">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">host   =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"aabc.b.db.ondigitalocean.com"</span>,</span>
<span id="cb10-3">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dbname =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"db"</span>,</span>
<span id="cb10-4">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">user      =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"db"</span>,</span>
<span id="cb10-5">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">password  =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">Sys.getenv</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"DB_PASS"</span>),</span>
<span id="cb10-6">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">port     =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25060</span>)</span></code></pre></div>
</div>
<p>Then, you can define that same environment variable more securely in <a href="https://docs.digitalocean.com/products/app-platform/how-to/use-environment-variables/">within the App Platform</a>.</p>
</section>
<section id="securing-input-integrity-sql-injection" class="level3">
<h3 class="anchored" data-anchor-id="securing-input-integrity-sql-injection">Securing input integrity (SQL injection)</h3>
<p>Finally, it’s also important to be aware of <a href="https://www.w3schools.com/sql/sql_injection.asp">SQL injection</a> to ensure that your database does not get corrupted.</p>
<p>SQL injection is usually discussed in the concept of malicious attacks. For example, W3 schools shows the following example where an application could be tricked into providing data on <em>all</em> users instead of a single user:</p>
<pre><code>txtUserId = getRequestString("UserId");
txtSQL = "SELECT * FROM Users WHERE UserId = " + txtUserId;</code></pre>
<p>If the entered <code>UserId</code> is <code>"UserId = 105 OR 1=1"</code>, then the full SQL string will be <code>"SELECT * FROM Users WHERE UserId = 105 OR 1=1;"</code>.</p>
<p>SQL injection is also at jokes you make have heard about “little Bobby Drop Tables” (<a href="https://xkcd.com/327/">xkcd</a>).</p>
<p><img src="https://imgs.xkcd.com/comics/exploits_of_a_mom.png" class="img-fluid"></p>
<p>That joke also, in some odd way, highlights that SQL injection need not be malicious. Rather, whenever we have software opened up to users beyond ourselves, they will likely use it in unexpected ways that push the system to its limit. For example, a user might try to enter or remove values from our database with double quotes, semicolons, or other features that mean something different to SQL than in human parlance and corrupt the code. Regardless of intent, we can protect against bad SQL that will break our application by using the <code>DBI::sqlInterpolate()</code> function.</p>
<p>A demonstration of this function and how it can protect against bad query generation is shown in <a href="https://shiny.rstudio.com/articles/sql-injections.html">this post</a> by RStudio.</p>
</section>
<section id="dev-versus-prod" class="level3">
<h3 class="anchored" data-anchor-id="dev-versus-prod">Dev versus Prod</h3>
<p>However, you may have realized a flaw in this approach. Our entire app now depends on forming a connection <em>that can only be made by the in-production app.</em> This meams you cannot test your application locally. However, even if our local traffic was not categorically blocked, we wouldn’t <em>want</em> to test our app on the production database and recklessly add and remove entries.</p>
<p>Instead, we would ideally have <em>separate</em> databases: one for development and one for production. Ideally, these would be the same type of database (e.g.&nbsp;both Postgres) to catch nuances of different SQL syntax and database operations. However, to keep things simpler (and cheaper), I decided to use an in-memory SQLite database locally.</p>
<p>To accomplish this, I wrapped my database connection in a custom <code>db_con()</code> function that checks if the app is running in development or production (using <code>golem::app_prod()</code> which in turn checks the <code>R_CONFIG_ACTIVE</code> environment variable) and connects to different databases in either case. In the development case, it creates an in-memory SQLite database and remakes the empty table.</p>
<p>(Another alternative to creating the database on-the-fly is to <a href="https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html#Creating_a_new_SQLite_database">pre-make a SQLite database</a> saved to a <code>.sqlite</code> file and connect to that. But for this example, my sample table is so simple, creating it manually takes a negligible amount of time and keeps things quite readable, so I left it as-is.)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">db_con <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">prod =</span> golem<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">app_prod</span>()) {</span>
<span id="cb12-2">  </span>
<span id="cb12-3">  <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> (prod) {</span>
<span id="cb12-4">    </span>
<span id="cb12-5">    con <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbConnect</span>(RPostgres<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">Postgres</span>(),</span>
<span id="cb12-6">                          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">host   =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"abc.b.db.ondigitalocean.com"</span>,</span>
<span id="cb12-7">                          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">dbname =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"db"</span>,</span>
<span id="cb12-8">                          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">user      =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"db"</span>,</span>
<span id="cb12-9">                          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">password  =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">Sys.getenv</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"DB_PASS"</span>),</span>
<span id="cb12-10">                          <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">port     =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">25060</span>)</span>
<span id="cb12-11">    </span>
<span id="cb12-12">  } <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> {</span>
<span id="cb12-13">    </span>
<span id="cb12-14">    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">stopifnot</span>( <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">require</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"RSQLite"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">quietly =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>) )</span>
<span id="cb12-15">    con <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbConnect</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">SQLite</span>(), <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">":memory:"</span>)</span>
<span id="cb12-16">    df <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a"</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">z =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.Date</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"2022-01-01"</span>))</span>
<span id="cb12-17">    DBI<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">dbWriteTable</span>(con, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"my_data"</span>, df)</span>
<span id="cb12-18">    </span>
<span id="cb12-19">  }</span>
<span id="cb12-20">  </span>
<span id="cb12-21">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">return</span>(con)</span>
<span id="cb12-22">  </span>
<span id="cb12-23">}</span></code></pre></div>
</div>
</section>
<section id="managing-connections" class="level3">
<h3 class="anchored" data-anchor-id="managing-connections">Managing connections</h3>
<p>So, you’ve built a robust app that can run against a database locally or on your production server. Great! It’s time to share your application with the world. But what if it is <em>so</em> popular that you have a lot of concurrent users and they are all trying to work with the database at once?</p>
<p>To maintain good application performance, you have to be careful about managing the database connection objects that you create (with <code>DBI::dbConnect()</code>) and to close them when you are doing using them.</p>
<p>If this sounds manual and tedious, you’re in luck! The <a href="https://rstudio.github.io/pool/">{pool}</a> package adds a layer of abstraction to manage a <em>set</em> of connections and execute new queries to an available idle collection. Full examples are given on the package’s website, but in short <code>{pool}</code> is quite easy to implement due to it’s <code>DBI</code>-like syntax. You can replace <code>DBI::dbConenct()</code> with <code>pool::dbPool()</code> and proceed as usual!</p>


</section>
</section>

 ]]></description>
  <category>rstats</category>
  <category>shiny</category>
  <category>data</category>
  <guid>https://emilyriederer.com/post/shiny-db/</guid>
  <pubDate>Sun, 02 Jan 2022 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/shiny-db/featured.png" medium="image" type="image/png" height="107" width="144"/>
</item>
<item>
  <title>Make grouping a first-class citizen in data quality checks</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/grouping-data-quality/</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/grouping-data-quality/featured.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Photo credit to <a href="https://unsplash.com/@greysonjoralemon">Greyson Joralemon</a> on Unsplash</figcaption>
</figure>
</div>
<p>Which of these numbers doesn’t belong? -1, 0, 1, NA.</p>
<p>It may be hard to tell. If the data in question should be non-negative, -1 is clearly wrong; if it should be complete, the NA is problematic; if it represents the signs to be used in summation, 0 is questionable. In short, there is no data quality without data context.</p>
<p>The past few years have seen an explosion in different solutions for monitoring in-production data quality. These tools, including software like <code>dbt</code> and <code>Great Expectations</code> as well as platforms like <code>Monte Carlo</code>, bring a more DevOps flavor to data production with important functionality like automated testing <em>within</em> pipelines (not just at the end), expressive and configurable semantics for common data checks, and more.</p>
<p>However, despite all these features, I notice a common gap across the landscape which may limit the ability of these tools to encode richer domain context and detect common classes of data failures. I previously wrote about the importance of <a href="https://emilyriederer.netlify.app/post/data-error-gen/">validating data based on its data generating process</a> – both along the technical and conceptual dimensions. Following this logic, an important and lacking functionality<sup>1</sup> across the data quality monitoring landscape, is the ability to readily apply checks separately to groups of data. On a quick survey, I count about 8/14 <code>dbt</code> tests (from the add-on <a href="https://github.com/dbt-labs/dbt-utils">dbt-utils</a> package), 15/37 <a href="https://docs.greatexpectations.io/docs/reference/glossary_of_expectations">Great Expectations</a> column tests, and most all of the <a href="https://docs.getmontecarlo.com/docs/field-health-metrics">Monte Carlo</a> field health metrics that would be improved with first-class grouping functionality. (Lists at the bottom of the post.)</p>
<p>Group-based checks can be important for fully articulating good “business rules” against which to assess data quality. For example, groups could reflect either computationally-relevant dimensions of the ETL process (e.g.&nbsp;data loaded from different sources) or semantically-relevant dimensions of the real-world process that our data captures (e.g.&nbsp;repeated measures pertaining to many individual customers, patients, product lines, etc.)</p>
<p>In this post, I make my case for why grouping should be a first-class citizen in data quality tooling.</p>
<section id="use-cases" class="level2">
<h2 class="anchored" data-anchor-id="use-cases">Use Cases</h2>
<p>There are three main use-cases for enabling easy data quality checks by group: checks that can only be expressed by group, checks that can be more rigorous by group, and checks that are more semantically intuitive by group.</p>
<p><strong>Some checks can be more rigorous by group.</strong> Consider a recency check (i.e.&nbsp;that the maximum date represented in the data is appropriately close to the present.) If the data loads from multiple sources (e.g.&nbsp;customer acquisitions from web and mobile perhaps logging to different source systems), the maximum value of the field could pass the check if any one source loaded, but unless the data is grouped in such a way that reflects different data sources and <em>each</em> group’s maximum date is checked, stale data could go undetected.</p>
<p><strong>Some types of checks can only be expressed by group.</strong> Consider a check for consecutive data values. If a table that measures some sort of user engagements, there might be fields for the <code>USER_ID</code> and <code>MONTHS_SINCE_ACQUISITION</code>. A month counter will most certainly <em>not</em> be strictly increasing across the entire dataset but absolutely should be monotonic within each user.</p>
<p><strong>Some checks are more semantically intuitive by group.</strong> Consider a uniqueness check for the same example as above. The month counter is also not unique across the whole table but could be checked for uniqueness within each user. Group semantics would not be required to accomplish this; a simple <code>USER_ID x MONTHS_SINCE_ACQUISITION</code> composite variable could be produced and checked for uniqueness. However, it feels cumbersome and less semantically meaningful to derive additional fields just to fully check the properties of existing fields. (But for the other two categories, this alone would not justify adding such new features.)</p>
<p>These categories demonstrate the specific use cases for grouped data checks. However, there are also soft benefits.</p>
<p>Most prevalent, having group-based checks be first class citizens opens up a new <a href="https://english.stackexchange.com/questions/77535/what-does-falling-into-the-pit-of-success-mean"><strong>pit of success</strong></a><sup>2</sup>. If you believe, as do I, that this is a often a fundamentally important aspect of confirming data quality and not getting “false promises” (as in the first case where non-grouped checks are less rigorous), configuring checks this way should be as close to zero-friction as possible.</p>
</section>
<section id="demo-nyc-turnstile-data" class="level2">
<h2 class="anchored" data-anchor-id="demo-nyc-turnstile-data">Demo: NYC turnstile data</h2>
<section id="context" class="level3">
<h3 class="anchored" data-anchor-id="context">Context</h3>
<p>To better motivate this need, we will look at a real-world example. For this, I turn to New York City’s <a href="http://web.mta.info/developers/turnstile.html">subway turnstile data</a> which I can always count on to have plenty of data quality quirks (which, to be clear, I do not say as a criticism. There’s nothing unexpected about this in real-world “data as residue” data.)</p>
<p>Specifically, we’ll pull down data extracts for roughly the first quarter of 2020 (published on Jan 11 - April 11, corresponding to weeks ending Jan 10 - April 10.)<sup>3</sup></p>
<p>This data contains 2869965 records, corresponding to one unique record per unique control area (<code>CA</code>), turnstile unit (<code>UNIT</code>), and individual turnstile device (<code>SCP</code>) at internals of four hours of the day.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(dplyr)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(full_data)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2869965</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">nrow</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">distinct</span>(full_data, CA, UNIT, SCP, DATE, TIME))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2869965</code></pre>
</div>
</div>
<p>Because this is raw turnstile data, the values for <code>ENTRIES</code> and <code>EXITS</code> may not be what one first expects. These fields contain the <em>cumulative number of turns of the turnstile</em> since it was last zeroed-out. Thus, to get the actual number of <em>incremental</em> entries during a given time period, one must take the difference between the current and previous number of entries <em>at the turnstile level</em>. Thus, missing or corrupted values<sup>4</sup> could cascade in unpredictable ways throughout the transformation process.</p>
<p>Looking at the data for a single turnstile unit makes the way this data is encoded more clear:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">full_data <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(CA <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"A002"</span>, </span>
<span id="cb5-3">         UNIT <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"R051"</span>, </span>
<span id="cb5-4">         SCP <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"02-00-00"</span>, </span>
<span id="cb5-5">         DATE <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"2020-01-04"</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(TIME) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb5-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">select</span>(TIME, ENTRIES, EXITS)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 x 3
  TIME       ENTRIES   EXITS
  &lt;hms&gt;        &lt;int&gt;   &lt;int&gt;
1 10800 secs 7331213 2484849
2 25200 secs 7331224 2484861
3 39600 secs 7331281 2484936
4 54000 secs 7331454 2485014
5 68400 secs 7331759 2485106
6 82800 secs 7331951 2485166</code></pre>
</div>
</div>
</section>
<section id="quality-checks" class="level3">
<h3 class="anchored" data-anchor-id="quality-checks">Quality checks</h3>
<p>So how does this map to the use cases above?</p>
<p>First, lets consider checks that are <strong>more rigorous by group</strong>. One example of this is checking for completeness of the range of data.</p>
<p>Looking at the aggregate level, the data appears complete. The minimum data is the correct start date for the week ending Jan 10 and the maximum date is the correct end date for the week ending April 10.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(full_data,</span>
<span id="cb7-2">          <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">min</span>(DATE),</span>
<span id="cb7-3">          <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(DATE))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 x 2
  `min(DATE)` `max(DATE)`
  &lt;date&gt;      &lt;date&gt;     
1 2020-01-04  2020-04-10 </code></pre>
</div>
</div>
<p>This check might provide a false sense of security.</p>
<p>However, what happens if we repeat this check at the actual grain of records which we <em>need</em> to be complete and subsequent calculations probably assume<sup>5</sup> are complete? We find many individual units whose data does not appear appropriately recent.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1">full_data <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb9-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(CA, UNIT, SCP) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb9-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">summarize</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">MAX =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">max</span>(DATE)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb9-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(MAX <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">!=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"2020-04-10"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`summarise()` has grouped output by 'CA', 'UNIT'. You can override using the
`.groups` argument.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 54 x 4
# Groups:   CA, UNIT [24]
   CA    UNIT  SCP      MAX       
   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;date&gt;    
 1 C021  R212  00-00-00 2020-01-06
 2 C021  R212  00-00-01 2020-01-06
 3 C021  R212  00-00-02 2020-01-06
 4 C021  R212  00-00-03 2020-01-06
 5 H007  R248  00-00-00 2020-03-07
 6 H007  R248  00-00-01 2020-02-15
 7 H007  R248  00-03-00 2020-02-15
 8 H007  R248  00-03-01 2020-02-15
 9 H007  R248  00-03-02 2020-02-15
10 H009  R235  00-03-04 2020-03-20
# i 44 more rows</code></pre>
</div>
</div>
<p>Next, consider checks that are <strong>only expressible by group</strong>. One example of this is a monotonicity (value always increasing) check.</p>
<p>For example, <code>ENTRIES</code> is certainly <em>not</em> monotonically increasing at the row level. Each individual turnstile device is counting up according to its own usage. However, in an ideal world, these fields should be monotonic over time at the level of individual devices. (Spoiler alert: due to the maintenance, malfunction, and maxing-out scenarios mentioned above, it’s not.) Thus, this type of check is only possible at the grouped level.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1">full_data <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-2">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">group_by</span>(CA, UNIT, SCP) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(DATE, TIME) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">LAG_ENTRIES =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lag</span>(ENTRIES)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-5">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(ENTRIES <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&lt;</span> LAG_ENTRIES, DATE <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'2020-01-25'</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">select</span>(CA, UNIT, SCP, DATE, TIME, ENTRIES, LAG_ENTRIES) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb12-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">arrange</span>(ENTRIES <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> LAG_ENTRIES)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 19,281 x 7
# Groups:   CA, UNIT, SCP [262]
   CA    UNIT  SCP      DATE       TIME        ENTRIES LAG_ENTRIES
   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;date&gt;     &lt;hms&gt;         &lt;int&gt;       &lt;int&gt;
 1 R231  R176  00-00-05 2020-03-09 75600 secs       99  1054865694
 2 R412  R146  00-03-03 2020-02-04 43200 secs 51627403   318991420
 3 N029  R333  01-00-00 2020-03-15 75600 secs       18   168628048
 4 R327  R361  01-06-00 2020-03-03 72000 secs   524397   135382887
 5 R312  R405  00-05-00 2020-02-16 57600 secs   131089   118174528
 6 N091  R029  02-05-00 2020-02-01 54000 secs   524368   118146213
 7 A025  R023  01-06-00 2020-03-04 82800 secs 11525743    67822764
 8 A025  R023  01-00-00 2020-03-04 82800 secs  5276291    28448967
 9 R238  R046  00-03-00 2020-02-15 82800 secs        5    16336060
10 R533  R055  00-03-04 2020-03-14 43200 secs      104    15209650
# i 19,271 more rows</code></pre>
</div>
</div>
</section>
</section>
<section id="alternatives-considered" class="level2">
<h2 class="anchored" data-anchor-id="alternatives-considered">Alternatives Considered</h2>
<p>Given that this post is, to some extent, a feature request across all data quality tools ever, it’s only polite to discuss downsides and alternative solutions that I considered. Clearly, finer-grained checks incur a greater computational cost and could, in some cases, be achieved via other means.</p>
<p><strong>Grouped data check might seem excessive.</strong> After all, data quality checks do not, perhaps, aim to guarantee every field is 100% correct. Rather, they are higher-level metrics which aim to catch signals of deeper issues. My counterargument is largely based in the first use case listed above. Without testing data at the right level of granularity, checks could almost do more harm than good if they promote a false sense of data quality by masking issues.</p>
<p><strong>Not all grains of data are equally likely to break.</strong> Taking the previous point into account, we likely cannot check everything, so we ought to focus our attention on some combination of the most “important” errors and the most “likely” errors. In the subway example, turnstile-level failures are likely because each individual turnstile is a sensor that is independently involved in the <em>data collection process</em> and can break in its own unique ways. However, for something like a clickstream for different users on a website, the data collection process is centralized, so it would be less like (and infeasible to check) for individual customer-level data to break in dramatically different ways.</p>
<p><strong>High-risk grouped data is possibly ungrouped further upstream.</strong> Following the logic that grouped data is more dangerous if groups denote units responsible for their own data collection, in theory this data is being transmitted separately at some point in the pipeline. Thus, in some cases it could be checked before it is grouped. However, we cannot always get infinitely far upstream in the data pipeline as some pieces may be outside of our control or produced atomically by a third-party platform.</p>
<p><strong>Some grouped checks can be achieved in other ways.</strong> Some (but not all) of these checks can be mocked by creating composite variables, using other built-in features<sup>6</sup>, or writing custom checks <sup>7</sup>. However, there solutions seem to defy part of the benefits of these tools: semantically meaningful checks wrapped in readable syntax and ready for use out-of-the-box. This also implies that grouped operations are far less than first-class citizens. This also limits the ability to make use of some of the excellent functionality these tools offer for documenting data quality checks in metadata and reporting on their outcomes.</p>
<p>I also considered the possibility that this is a niche, personal need moreso than a general one because I work with a <em>lot</em> of panel data. However, I generally believe <em>most</em> data is nested in some way. I can at least confirm that I’ve not completely alone in this desire with a peak at GitHub issue feature requests in different data quality tools. For example, three stale stale GitHub issues on the <code>Great Expectations</code> repo (<a href="https://github.com/great-expectations/great_expectations/issues/351">1</a>, <a href="https://github.com/great-expectations/great_expectations/issues/402">2</a>, <a href="https://github.com/great-expectations/great_expectations/issues/373">3</a>) request similar functionality.</p>
</section>
<section id="downsides" class="level2">
<h2 class="anchored" data-anchor-id="downsides">Downsides</h2>
<p>There’s no such thing as a free lunch or a free feature enhancement. My point is in no way to criticize existing data quality tools that do not have this functionality. Designing any tool is a process of trade-offs, and it’s only fair to discuss the downsides. These issues are exacerbated further when adding “just one more thing” to mature, heavily used tools as opposed to developing new ones.</p>
<p><strong>Grouped checks are more computational expensive.</strong> Partitioning and grouping can make data check operations more expensive by disabling certain computational shortcuts<sup>8</sup> and requiring more total data to be retained. This is particularly true if the data is indexed or partitioned along different dimensions than the groups used for checks. The extra time required to run more fine-grained checks could become intractable or at least unappealing, particularly in an interactive or continuous integration context. However, in many cases it could be a better use of time to more rigorously test recently loaded data as opposed to (or in conjunction with) running higher-level checks across larger swaths of data.</p>
<p><strong>API bloat makes tools less navigable.</strong> Any new feature has to be documented by developers and comprehended by users. Having too many “first-class citizen” features can lead to features being ignored, unknown, or misused. It’s easy to point to any one feature in isolation and claim it is important; it’s much harder to stare at a full backlog and decide where the benefits are worth the cost.</p>
<p><strong>Incremental functionality adds more overhead.</strong> Every new feature demands careful programming and testing. Beyond that, there’s a substantial mental tax in thinking through how that feature needs to interact with existing functionality while, at the same time, preserving backwards compatibility.</p>
<p><strong>Every feature built means a different one isn’t.</strong> As a software user, it’s easy to have a great idea for a feature that should absolutely be added. That’s a far different challenge than that faced by the developers and maintainers who must prioritize a rich backlog full of competing priorities.</p>
</section>
<section id="survey-of-available-tools" class="level2">
<h2 class="anchored" data-anchor-id="survey-of-available-tools">Survey of available tools</h2>
<p>My goal is in no way to critique any of the amazing, feature-rich data quality tools available today. However, to further illustrate my point, I pulled down key data checks from a few prominent packages to assess how many of their tests would be potentially enhanced with the ability to provided grouping parameters. Below are lists for <code>dbt-utils</code>, <code>Great Expectations</code>, and <code>Monte Carlo</code> with relevant tests <em>in bold</em>.</p>
<section id="dbt-utils-8-14" class="level3">
<h3 class="anchored" data-anchor-id="dbt-utils-8-14">dbt-utils (8 / 14)</h3>
<ul>
<li><strong>equal_rowcount</strong></li>
<li>equality</li>
<li>expression_is_true</li>
<li><strong>recency</strong><br>
</li>
<li><strong>at_least_one</strong></li>
<li><strong>not_constant</strong></li>
<li><strong>cardinality_equality</strong></li>
<li><strong>unique_where</strong></li>
<li>not_null_where<br>
</li>
<li><strong>not_null_proportion</strong></li>
<li>relationships_where</li>
<li>mutually_exclusive_ranges</li>
<li><strong>unique_combination_of_columns</strong> (<em>but less important - only for semantics</em>)</li>
<li>accepted_range</li>
</ul>
</section>
<section id="great-expectations-15-37" class="level3">
<h3 class="anchored" data-anchor-id="great-expectations-15-37">Great Expectations (15 / 37)</h3>
<ul>
<li><strong>expect_column_values_to_be_unique</strong> (<em>but less important - only for semantics</em>)</li>
<li>expect_column_values_to_not_be_null<br>
</li>
<li>expect_column_values_to_be_null<br>
</li>
<li>expect_column_values_to_be_of_type<br>
</li>
<li>expect_column_values_to_be_in_type_list</li>
<li>expect_column_values_to_be_in_set</li>
<li>expect_column_values_to_not_be_in_set</li>
<li>expect_column_values_to_be_between<br>
</li>
<li><strong>expect_column_values_to_be_increasing</strong></li>
<li><strong>expect_column_values_to_be_decreasing</strong></li>
<li>expect_column_value_lengths_to_be_between</li>
<li>expect_column_value_lengths_to_equal</li>
<li>expect_column_values_to_match_regex</li>
<li>expect_column_values_to_not_match_regex</li>
<li>expect_column_values_to_match_regex_list</li>
<li>expect_column_values_to_not_match_regex_list</li>
<li>expect_column_values_to_match_like_pattern</li>
<li>expect_column_values_to_not_match_like_pattern</li>
<li>expect_column_values_to_match_like_pattern_list</li>
<li>expect_column_values_to_not_match_like_pattern_list</li>
<li>expect_column_values_to_match_strftime_format</li>
<li>expect_column_values_to_be_dateutil_parseable</li>
<li>expect_column_values_to_be_json_parseable</li>
<li>expect_column_values_to_match_json_schema</li>
<li>expect_column_distinct_values_to_be_in_set</li>
<li><strong>expect_column_distinct_values_to_contain_set</strong></li>
<li><strong>expect_column_distinct_values_to_equal_set</strong></li>
<li><strong>expect_column_mean_to_be_between</strong></li>
<li><strong>expect_column_median_to_be_between</strong></li>
<li><strong>expect_column_quantile_values_to_be_between</strong></li>
<li><strong>expect_column_stdev_to_be_between</strong></li>
<li><strong>expect_column_unique_value_count_to_be_between</strong></li>
<li><strong>expect_column_proportion_of_unique_values_to_be_between</strong></li>
<li><strong>expect_column_most_common_value_to_be_in_set</strong></li>
<li><strong>expect_column_max_to_be_between</strong></li>
<li><strong>expect_column_min_to_be_between</strong></li>
<li><strong>expect_column_sum_to_be_between</strong></li>
</ul>
</section>
<section id="monte-carlo-all" class="level3">
<h3 class="anchored" data-anchor-id="monte-carlo-all">Monte Carlo (All)</h3>
<p>Any of Monte Carlo’s checks might be more sensitive to detecting changes with subgrouping. Since these “health metrics” tend to represent distributional properties, it can be useful to ensure that “good groups” aren’t pulling down the average value and masking errors in “bad groups”.</p>
<ul>
<li>Pct NUll<br>
</li>
<li>Pct Unique</li>
<li>Pct Zero<br>
</li>
<li>Pct Negative<br>
</li>
<li>Min</li>
<li>p20</li>
<li>p40</li>
<li>p60</li>
<li>p80</li>
<li>Mean</li>
<li>Std</li>
<li>Max</li>
<li>Pct Whitespace</li>
<li>Pct Integer</li>
<li>Pct “Null”/“None”</li>
<li>Pct Float</li>
<li>Pct UUID</li>
</ul>
</section>
</section>
<section id="code-appendix" class="level2">
<h2 class="anchored" data-anchor-id="code-appendix">Code Appendix</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># data source: http://web.mta.info/developers/turnstile.html</span></span>
<span id="cb14-2"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(ggplot2)</span>
<span id="cb14-3"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">library</span>(readr)</span>
<span id="cb14-4"></span>
<span id="cb14-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># define read function with schema ----</span></span>
<span id="cb14-6">read_data <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">function</span>(url) {</span>
<span id="cb14-7">  </span>
<span id="cb14-8">  readr<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">::</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">read_csv</span>(url,</span>
<span id="cb14-9">                  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">col_names =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">TRUE</span>,</span>
<span id="cb14-10">                  <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">col_types =</span></span>
<span id="cb14-11">                    <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">cols</span>(</span>
<span id="cb14-12">                      <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span><span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">C/A</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">`</span> <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_character</span>(),</span>
<span id="cb14-13">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">UNIT =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_character</span>(),</span>
<span id="cb14-14">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">SCP =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_character</span>(),</span>
<span id="cb14-15">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">STATION =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_character</span>(),</span>
<span id="cb14-16">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">LINENAME =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_character</span>(),</span>
<span id="cb14-17">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">DIVISION =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_character</span>(),</span>
<span id="cb14-18">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">DATE =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_date</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">format =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"%m/%d/%Y"</span>),</span>
<span id="cb14-19">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">TIME =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_time</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">format =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>),</span>
<span id="cb14-20">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">DESC =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_character</span>(),</span>
<span id="cb14-21">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ENTRIES =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_integer</span>(),</span>
<span id="cb14-22">                      <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">EXITS =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_integer</span>()</span>
<span id="cb14-23">                    ))</span>
<span id="cb14-24">  </span>
<span id="cb14-25">}</span>
<span id="cb14-26"></span>
<span id="cb14-27"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># ridership data ----</span></span>
<span id="cb14-28">dates <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">seq.Date</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">from =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.Date</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'2020-01-11'</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">to =</span>, <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">as.Date</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'2020-04-11'</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">by =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'7 days'</span>)</span>
<span id="cb14-29">dates_str <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">format</span>(dates, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">format =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%y%m%d'</span>)</span>
<span id="cb14-30">dates_url <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">sprintf</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'http://web.mta.info/developers/data/nyct/turnstile/turnstile_%s.txt'</span>, dates_str)</span>
<span id="cb14-31">datasets <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">lapply</span>(dates_url, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">FUN =</span> read_data)</span>
<span id="cb14-32">full_data <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">do.call</span>(rbind, datasets)</span>
<span id="cb14-33">full_data <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> full_data[full_data<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>DESC <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"REGULAR"</span>,]</span>
<span id="cb14-34"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">names</span>(full_data)[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"CA"</span></span></code></pre></div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>With the exception of <code>pointblank</code> which kindly entertained an issue I opened on this topic: https://github.com/rich-iannone/pointblank/issues/300↩︎</p></li>
<li id="fn2"><p>The “pit of success” is the idea that well-designed tools can help nudge people to do the “right” thing by default because its also the easiest. I first learned of it in a talk by Hadley Wickham, and it is originally attributed to Microsoft program manage Rico Mariani.↩︎</p></li>
<li id="fn3"><p>Code for this pull is at the bottom of the post.↩︎</p></li>
<li id="fn4"><p>This can happen for many reasons including turnstile maintenance or replacement. My goal in this post is not to go into all of the nuances of this particular dataset, of which much has already been written, so I’m simplifying somewhat to keep it as a tractable motivating example.↩︎</p></li>
<li id="fn5"><p>Transformations should probably never assume this. Any real ETL process using this data would like have to account for incompleteness in an automated fashion because it really is not a rare event. Again, we are simplifying here for the sake of example↩︎</p></li>
<li id="fn6"><p>For example, Great Expectations does offer conditional expectations which can be executed on manually-specified subsets of data. This could be a tractable solution for applying data checks to a small number of categorical variables, but less so for large or ill-defined categories like user IDs. More here: https://legacy.docs.greatexpectations.io/en/latest/reference/core_concepts/conditional_expectations.html↩︎</p></li>
<li id="fn7"><p>Like, in the case of <code>Great Expectation</code>’s python-based API, writing custom code to partition data before applying checks, or, in the case of <code>dbt</code>/<code>dbt-utils</code> writing a custom macro.↩︎</p></li>
<li id="fn8"><p>For example, the maximum of a set of numbers is the maximum of the maximums of the subsets. Thus, if my data is distributed, I can find the max by comparing only summary statistics from the distributed subsets instead of pulling all of the raw data back together.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>data</category>
  <guid>https://emilyriederer.com/post/grouping-data-quality/</guid>
  <pubDate>Sat, 27 Nov 2021 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/grouping-data-quality/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>A lightweight data validation ecosystem with R, GitHub, and Slack</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/data-valid-lightweight/</link>
  <description><![CDATA[ 




<p>Data quality monitoring is an essential part of any data analysis or business intelligence workflow. As such, an increasing number of promising tools<sup>1</sup> have emerged as part of the <a href="https://moderndatastack.xyz/">Modern Data Stack</a> to offer better orchestration, testing, and reporting.</p>
<p>Although I’m very excited about the developments in this space, I realize that emerging products may not be the best fit for every organization. Enterprise tools can be financial costly, and, more broadly, even free and open-source offerings bring costs in the time and risks associated with vetting tools for security, training associates, and committing to potential lock-in of building workflows around these tools. Additionally, data end-users may not always have the ability to get far enough “upstream” in the production process of their data to make these tools make sense.</p>
<p>“Right-sizing” technology to the problem at hand is a critical task. A “best” solution with the most polished, professional, exciting product isn’t always the best <em>fit</em> for your needs. Trade-offs must be made between feature completeness and fit-for-purpose. In other words, sometimes its more important for technology to be <em>“good enough”</em>.<sup>2</sup></p>
<p>Additionally, in an embarassment of riches of developer tools, sometimes the number of tools (no matter how good) we have to work with can become a burden. Personally, I like to leverage a core set of tools like Slack and GitHub for as many of their strengths when possible instead of allowing a creep of many different project-specific communication and project management tools.</p>
<p>With all of that in mind, in this post I explore a lightweight approach to a data quality workflow using a minimal set of tools that are likely already part of many team’s data stacks: R, GitHub, and Slack. This approach may be far from perfect, but I believe it provides a lot of “bang for the buck” by enabling scheduling data quality monitoring, instantaneous alerting, and workflow management at little-to-no incremental overhead.</p>
<p>The full code for this demo is available in my <a href="https://github.com/emilyriederer/data-validation-demo">emilyriederer/data-validation-demo</a> repo on GitHub.</p>
<section id="overall-workflow" class="level2">
<h2 class="anchored" data-anchor-id="overall-workflow">Overall Workflow</h2>
<p>To think about right-sizing, it’s first useful to think about what features from some of the “hot” data quality monitoring products make them so appealing. Key features and tradeoffs include:</p>
<ul>
<li><strong>Always-on monitoring</strong>: Monitoring any time data is loaded or changed (or preferably <em>before</em> – such as dbt’s dev schemas and Great Expectation’s in-pipeline integration)</li>
<li><strong>Reporting</strong>: Dashboards or tools to review outputs of data validation</li>
<li><strong>Alerting</strong>: Proactive logging and alerting of failures of data validation checks</li>
<li><strong>Integration in data production process</strong>: As alluded to in the “always-on” point, the more validation is not just a passive activity but part of data production itself the better (e.g.&nbsp;preventing bad data from being loaded)</li>
</ul>
<p>This approach makes some tradeoffs. It’s not orchestrated or trigger-based but can be scheduled to run on a regular basis. It’s also loosely-coupled with data production, but as we will see it can still support a better GitHub-based workflow for seeing issues through to resolution.</p>
<p>The basic idea of this workflow is to recreate as many of these strengths as possibly by maximally leveraging the strengths of existing tools. We use each for what its already good at, including:</p>
<ul>
<li><strong>R</strong>:
<ul>
<li>Data validation with the <a href="https://rich-iannone.github.io/pointblank/"><code>pointblank</code> package</a> can be run directly or “outsourced” upstream to run in-place in a database (if that is where your data lives)</li>
<li>Validation failures are logged as GitHub issues using the <a href="https://emilyriederer.github.io/projmgr/"><code>projmgr</code> package</a></li>
<li>A more aesthetic version of data quality reporting output is produced by running the above steps by rendering an <strong>R Markdown</strong> document to HTML</li>
</ul></li>
<li><strong>GitHub</strong>: Serves as the central nervous system for execution, project management, and reporting
<ul>
<li><strong>Actions</strong>: Reruns the <code>pointblank</code> checks on a regular basis and updates an RMarkdown-based website</li>
<li><strong>Pages</strong>: Hosts the resultings RMarkdown-generated HTML for accessible data quality reporting</li>
<li><strong>Issues</strong>: Record data quality errors caught by <code>pointblank</code>. This provides an easy platform to assign owners, discuss issues, and track progress. With detailed labels, closed issues can also serve as a way to catalog past errors and identify trends or needed areas of improvement (where repeat failures occur)</li>
</ul></li>
<li><strong>Slack</strong>: Integrates with GitHub to provide alerts on new issues on a Slack channel. Individual teams or team members can use Slack’s controls to determine how they receive notifications (e.g.&nbsp;email, mobile notification, etc.) for time-sensitive issues</li>
</ul>
<p>Intrigued? Next we’ll step through the technical details.</p>
</section>
<section id="detailed-implementation" class="level2">
<h2 class="anchored" data-anchor-id="detailed-implementation">Detailed Implementation</h2>
<p>This workflow revolves around a single main R Markdown document. The full version can be found <a href="https://github.com/emilyriederer/data-validation-demo/blob/master/data-valid-pipe.Rmd">on GitHub</a>, and we will step through key components of the code and its interaction with the GitHub and Slack platforms below.</p>
<section id="validating-data-with-pointblank" class="level3">
<h3 class="anchored" data-anchor-id="validating-data-with-pointblank">Validating data (with <code>pointblank</code>)</h3>
<p><img src="https://emilyriederer.com/post/data-valid-lightweight/report.PNG" class="img-fluid"></p>
<p>The first key step is setting up validation with <code>pointblank</code>. Here, I show a minimal example which uses a very small toy dataset. However, <code>pointblank</code> can also connect to a number of remote datasources like databases and run these checks on the data in-place<sup>3</sup> The following example just runs a few checks for data ranges, nulls, and duplicates although a wide array of premade and customizable checks are available.</p>
<p>Out of the box, we can produce an aesthetic table of validation results.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">tbl <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">data.frame</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">x =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">y =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">c</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>))</span>
<span id="cb1-2">act <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">action_levels</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">warn_at =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">notify_at =</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.01</span>, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">stop_at =</span> <span class="cn" style="color: #8f5902;
background-color: null;
font-style: inherit;">NULL</span>)</span>
<span id="cb1-3">table_name <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"my_table"</span></span>
<span id="cb1-4">agent <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span></span>
<span id="cb1-5">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">create_agent</span>(tbl, <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">actions =</span> act) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb1-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_vals_between</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">vars</span>(x), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb1-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">col_vals_not_null</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">vars</span>(x)) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb1-8">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">rows_distinct</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">vars</span>(x,y))</span>
<span id="cb1-9">res <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">interrogate</span>(agent)</span>
<span id="cb1-10">res</span></code></pre></div>
</div>
</section>
<section id="posting-results-as-github-issues-with-projmgr" class="level3">
<h3 class="anchored" data-anchor-id="posting-results-as-github-issues-with-projmgr">Posting results as GitHub issues (with <code>projmgr</code>)</h3>
<p><img src="https://emilyriederer.com/post/data-valid-lightweight/issues.PNG" class="img-fluid"></p>
<p>Beyond <code>pointblank</code>’s aesthetic output, we can also extract an underlying dataframe with all of the check information include which columns were included in the check, a human-readable description of the check, and the failure rate.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">out <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> </span>
<span id="cb2-2">  res<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">$</span>validation_set <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb2-3">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">filter</span>(warn) <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span></span>
<span id="cb2-4">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">select</span>(columns_expr, brief, column, n, n_failed, f_failed) </span></code></pre></div>
</div>
<p>With this information, we can use <code>projmgr</code> to connect to a GitHub repository<sup>4</sup>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">repo <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">create_repo_ref</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"emilyriederer"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"data-validation-demo"</span>)</span></code></pre></div>
</div>
<p>The full data wrangling steps are shown in the <a href="https://github.com/emilyriederer/data-validation-demo/blob/master/data-valid-pipe.Rmd">R Markdown</a>, but after light data wrangling of the output dataset (<code>out</code>) to convert validation results into a title, description, and labels, we can post these issues to our repository.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">issue_numbers <span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">&lt;-</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">pmap</span>(issues_df, </span>
<span id="cb4-2">                      <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">possibly</span>(<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">post_issue</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">ref =</span> repo, ...), <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">otherwise =</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>)</span>
<span id="cb4-3">                      )</span></code></pre></div>
</div>
<p>This creates the two issues shown above with labels for each table and variable.</p>
<p>The full R Markdown also shows how this collection of issues can also be pulled back into the resulting report to provide context on the status of each issue such as whether it has been assigned to an owner and the number of comments it has.</p>
</section>
<section id="running-on-github-actions" class="level3">
<h3 class="anchored" data-anchor-id="running-on-github-actions">Running on GitHub Actions</h3>
<p>Of course, monitoring isn’t useful if it doesn’t run and detect new errors at a reasonable cadence. One way to run this report regularly is using <a href="https://github.com/features/actions">GitHub Actions</a>. With a simple <a href="https://github.com/emilyriederer/data-validation-demo/blob/master/.github/workflows/run-validation.yaml">config file</a>, we are able to schedule a daily cron job. This job:</p>
<ul>
<li>Exposes the GITHUB personal access token we need for <code>projmgr</code> to be able to write issues to our repository<sup>5</sup></li>
<li>Sets up R and pandoc to be able to knit an R Markdown</li>
<li>Installs needed packages</li>
<li>Renders the R Markdown to the file <code>docs/index.html</code> (Why this name? See the next step)</li>
<li>Pushes the results back to the repo</li>
</ul>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">on<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb5-2">  schedule<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb5-3">    <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> cron<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"30 4 * * 3"</span></span>
<span id="cb5-4">  push<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb5-5">    branches<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb5-6">      <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> master</span>
<span id="cb5-7"></span>
<span id="cb5-8">jobs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb5-9">  render<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb5-10">    name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> Render my document</span>
<span id="cb5-11">    runs<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>on<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> macOS<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>latest</span>
<span id="cb5-12">    steps<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb5-13">      <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> Create and populate .Renviron file</span>
<span id="cb5-14">        run<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">|</span></span>
<span id="cb5-15">          echo GITHUB_PAT<span class="ot" style="color: #003B4F;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"$GH_PAT"</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span><span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">&gt;</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">~/</span>.Renviron</span>
<span id="cb5-16">        shell<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> bash</span>
<span id="cb5-17">        env<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span></span>
<span id="cb5-18">          GH_PAT<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">$</span>{{secrets.GH_PAT}}</span>
<span id="cb5-19">      <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> uses<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> actions<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>checkout<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>v2</span>
<span id="cb5-20">      <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> uses<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> r<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>lib<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>actions<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>setup<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>r<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>v1</span>
<span id="cb5-21">      <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> uses<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> r<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>lib<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>actions<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>setup<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>pandoc<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>v1</span>
<span id="cb5-22">      <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> uses<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> r<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>lib<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>actions<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>setup<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>tinytex<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>v1</span>
<span id="cb5-23">      <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> Install rmarkdown</span>
<span id="cb5-24">        run<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> Rscript <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>e <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'install.packages(c("pointblank", "projmgr", "dplyr", "purrr", "glue", "rmarkdown", "knitr"))'</span></span>
<span id="cb5-25">      <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> Render my document to all types</span>
<span id="cb5-26">        run<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> Rscript <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>e <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rmarkdown::render("data-valid-pipe.Rmd", output_file = "index.html", output_dir = "docs")'</span></span>
<span id="cb5-27">      <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> Commit results</span>
<span id="cb5-28">        run<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:</span> <span class="er" style="color: #AD0000;
background-color: null;
font-style: inherit;">|</span></span>
<span id="cb5-29">          git add <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">--</span>force docs</span>
<span id="cb5-30">          git commit <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>m <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Rerun validation checks'</span> <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">||</span> echo <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"No changes to commit"</span></span>
<span id="cb5-31">          git push origin <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">||</span> echo <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"No changes to commit"</span></span></code></pre></div>
</div>
</section>
<section id="publishing-on-github-pages" class="level3">
<h3 class="anchored" data-anchor-id="publishing-on-github-pages">Publishing on GitHub Pages</h3>
<p>Now that we’ve created an HTML report from our R Markdown, we can easily host it on <a href="https://pages.github.com/">GitHub Pages</a> by going to our repo’s <code>Settings &gt; Pages</code> and selecting as a <code>Source</code> the <code>main</code> or <code>master</code> branch and, specifically, the <code>docs</code> folder. GitHub will then provide a URL to our pages where the <code>docs/index.html</code> file serves as the main page.</p>
<p>In the case of my repo <code>emilyriederer/data-validation-demo</code>, the URL is https://emilyriederer.github.io/data-validation-demo/.</p>
</section>
<section id="setting-up-slack-notifications" class="level3">
<h3 class="anchored" data-anchor-id="setting-up-slack-notifications">Setting up Slack notifications</h3>
<p><img src="https://emilyriederer.com/post/data-valid-lightweight/featured.png" class="img-fluid"></p>
<p>Of course, no one wants to go check one more report every single day. While you can watch a repository on GitHub and receive emails about new issues, you might prefer not to fill up your own inbox or have more control over how you manage these notifications. In your team is already using Slack, <a href="https://github.com/integrations/slack">GitHub + Slack integration</a> offers a great alternative.</p>
<p>You can consider making a dedicated Slack channel for data issues and automate instant Slack notifications when any new issues are opened. First, as described in the link above, install GitHub integration for Slack. Then, the following commands (typed simply as if you are writing a message on the Slack channel) connect to your GitHub repo and unsubscribe from all notifications except for issues.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>invite <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">@</span>github</span>
<span id="cb6-2"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>github subscribe your<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>org<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>your<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>repo</span>
<span id="cb6-3"><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>github unsubscribe your<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>org<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span>your<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>repo pulls commits releases deployments</span></code></pre></div>
</div>
<p>Slack messages can both allow teams to customize how and when they are notified about emerging issues across different devices. This also allows a space for “meta” discussions, such as who is equipped to handle an issue, before someone is assigned and the conversation moves to GitHub itself.</p>
</section>
</section>
<section id="trade-offs" class="level2">
<h2 class="anchored" data-anchor-id="trade-offs">Trade Offs</h2>
<p>There’s always a fine line between exploiting the synergies of different tools or creating an incoherent Rube Goldberg machine with rough edges and new problems. However, different solutions are best suited for different organizations, teams, and data needs. I’m very excited about all of the emerging data quality tools and platforms, and for large enterprises I suspect that may be the way to go. However, if you’re looking for scaling up your data management practices with minimal new tools, infrastructure, or tech debt, I hope this set of powerful but lightweight tools can be a first step in a good direction.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Just to name a few: dbt, datafold, Soda, Great Expectations, and Monte Carlo↩︎</p></li>
<li id="fn2"><p>With love and admiration, I borrow this phrase from the excellent paper “Good Enough Practices in Scientific Computing”: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510↩︎</p></li>
<li id="fn3"><p>Logic is translated to SQL via <code>dbplyr</code>.↩︎</p></li>
<li id="fn4"><p>Assuming you have your personal access token set per the documentation: https://emilyriederer.github.io/projmgr/articles/github-pat.html↩︎</p></li>
<li id="fn5"><p>This assumes that within GitHub, the PAT is defined as a secret called GH_PAT. Curiously, GitHub does not allow secrets that start with the word “GITHUB”. Who knew? Additionally, depending on the privacy level of your repository, you might not need a PAT to make issues and could skip this step.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>rstats</category>
  <category>data</category>
  <category>elt</category>
  <guid>https://emilyriederer.com/post/data-valid-lightweight/</guid>
  <pubDate>Thu, 26 Aug 2021 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/data-valid-lightweight/featured.png" medium="image" type="image/png" height="105" width="144"/>
</item>
<item>
  <title>97 Things Every Data Engineer Should Know: Collective Wisdom from the Experts</title>
  <dc:creator>Tobias Macey</dc:creator>
  <link>https://emilyriederer.com/publication/data-eng-97-things/</link>
  <description><![CDATA[ 



<p>I contributed six chapters to the book:</p>
<ul>
<li><strong>Develop communities - not just code</strong>: On building developing communities along with code bases and empowering versus patronizing your data product’s customers</li>
<li><strong>Give data products a front-end with latent documentation</strong>: On low effort practices for improving data documentation and usability</li>
<li><strong>There’s no such thing as data quality</strong>: On the value of data “fit for purpose”</li>
<li><strong>The many meanings of missingness</strong>: On causes and consequences of null field encoding</li>
<li><strong>Column names as contracts</strong>: On embedding metadata and performance “contracts” in column names</li>
<li><strong>Data validation needs more than summary statistics</strong>: On the importance of context-informed data validation</li>
</ul>



 ]]></description>
  <category>data</category>
  <guid>https://emilyriederer.com/publication/data-eng-97-things/</guid>
  <pubDate>Sun, 01 Aug 2021 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/publication/data-eng-97-things/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Understanding the data (error) generating processes for data validation</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/data-error-gen/</link>
  <description><![CDATA[ 





<style>
table {
  width: 100%;
}

table strong { 
   color: darkred;
}

</style>
<p>Statistics literature often makes reference to the <em>data generating process</em> (DGP): an idealized description of a real-world system responsible for producing observed data. This leads to a modeling approach focused on describing that system as opposed to blindly fitting observations to a common functional form.<sup>1</sup></p>
<p>As a trivial example, if one wished to model the height of a group of adults, they might suppose that the height of women and the height of men each is normally distributed with separate means and standard deviations. Then the overall distribution of population heights could be models as a mixture of samples from these two distributions.<sup>2</sup></p>
<p>However, DGPs are not only useful for modeling. <strong>Conceptualizing the DGP of our observations can also lead to more principled data validation</strong> if we broaden the scope of the DGP to include the subsequent <em>manufacturing</em> of the data not just the originating <em>mechanism</em>.</p>
<p>Unfortunately, consumers of analytical data may not always be familiar with the craft of data production (including data engineering, data modeling, and data management). Without an understanding of the general flow of data processing between collection and publication to a data warehouse, data consumers are less able to theorize about failure modes. Instead, similarly to blindly fitting models without an underlying theory, consumers may default to cursory checks of summary statistics without hypotheses for the kind of errors they are trying to detect or how these checks might help them.</p>
<p>This post explores the DGP of system-generated data and the common ways that these processes can introduce risks to data quality. As we discuss data validation, we will make reference to the six dimensions of data quality defined by <a href="https://damauk.wildapricot.org/resources/Documents/DAMA%20UK%20DQ%20Dimensions%20White%20Paper2020.pdf">DAMA</a>: completeness, uniqueness, timeliness, validity, accuracy, and consistency. Along the way, we will explore how understanding how understanding key failure modes in the data production process can lead to more principled analytical data validation.<sup>3</sup></p>
<section id="the-four-dgps-for-data-management" class="level2">
<h2 class="anchored" data-anchor-id="the-four-dgps-for-data-management">The Four DGPs for Data Management</h2>
<p>To better theorize about data quality issues, it’s useful to think of four DGPs: the real-world DGP, the data collection/extraction DGP<sup>4</sup>, the data loading DGP, and the data transformation DGP.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/dgp.png" class="img-fluid"></p>
<p>For example, consider the role of each of these four DGPs for e-commerce data:</p>
<ul>
<li><strong>Real-world DGP</strong>: Supply, demand, marketing, and a range of factors motivate a consumer to visit a website and make a purchase</li>
<li><strong>Data collection DGP</strong>: Parts of the website are instrumented to log certain customer actions. This log is then extracted from the different operational system (login platforms, payment platforms, account records) to be used for analysis</li>
<li><strong>Data loading DGP</strong>: Data recorded by different systems is moved to a data warehouse for further processing through some sort of manual, scheduled, or orchestrated job. These different systems may make data available at different frequencies.</li>
<li><strong>Data transformation DGP</strong>: To arrive at that final data presentation requires creating a <a href="https://en.wikipedia.org/wiki/Data_model">data model</a> to describe domain-specific attributes with key variables crafted with data transformations</li>
</ul>
<p>Or, consider the role of each of these four DGPs for subway ridership data<sup>5</sup>:</p>
<ul>
<li><strong>Real-world DGP</strong>: Riders are motivated to use public transportation to commute, run errands, or visit friends. Different motivating factors may cause different weekly and annual seasonality</li>
<li><strong>Data collection DGP</strong>: To ride the subway, riders go to a station and enter and exit through turnstiles. The mechanical rotation of the turnstile caused by a rider passing through is recorded</li>
<li><strong>Data loading DGP</strong>: Data recorded at each turnstile is collected through a centralized computer system at the station. Once a week, each station uploads a flat file of this data to a data lake owned by the city’s Department of Transportation</li>
<li><strong>Data transformation DGP</strong>: Turnstiles from different companies may have different data formats. Transformation may include harmonizing disparate sources, coding system-generated codes (e.g.&nbsp;Station XYZ) to semantically meaningful names (e.g.&nbsp;Main Street Station), and publishing a final unified representation across stations and across time</li>
</ul>
<p>In the next sections, we’ll explore how understanding key concepts about each of these DGPs can help build a consumers’ intuition on where to look for problems.</p>
</section>
<section id="data-collection" class="level2">
<h2 class="anchored" data-anchor-id="data-collection">Data Collection</h2>
<p>Data collection is necessarily the first step in data production, but the very goal of data collection: translating complex human concepts into tabular data records is fraught with error. Data collection is effectively dimensionality reduction, and just like statistical dimensionality reduction, it must sometimes sacrifice accuracy for clarity.</p>
<p>This tradeoff makes data collection vulnerable to one of the largest risks to data validity: not that the data itself is incorrect <em>given its stated purpose</em> but rather that users misconstrue the population or metrics it includes. Thus, understanding what systems are intending to capture, publish, and extract and how they chose to encode information for those observations is essential for data validation and subsequent analysis.</p>
<p>Data collection can happen in countless different ways: experimentation, surveys, observation, sensors, etc. In many business settings, data is often extracted from source systems whose primary purpose is to execute some sort of real-world process.<sup>6</sup> Such systems may naturally collect data for operational purposes or may be <em>instrumented</em> to collect and log data as they are used. This production data is then often extracted from a source system to an alternative location such as a data warehouse for analysis.</p>
<section id="what-counts" class="level3">
<h3 class="anchored" data-anchor-id="what-counts">What Counts</h3>
<p>One of the tricky nuances of data collection is understanding what precisely is getting captured and logged in the first place.</p>
<p>Consider something as simple as a login system where users must enter their credentials, endure a Captcha-like verification process to prove that they are not a robot, and enter a multi-factor authentication code.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/login-log.png" class="img-fluid"></p>
<p>Which of these events gets collected and recorded has a significant impact on subsequent data processing. In a technical sense, no inclusion/exclusion decision here is <em>incorrect</em>, persay, but if the producers’ choices don’t match the consumers’ understandings, it can lead to misleading results.</p>
<p>For example, an analyst might seek out a <code>logins</code> table in order to calculate the rate of successful website logins. Reasonably enough, they might compute this rate as the sum of successful events over the total. Now, suppose two users attempt to login to their account, and ultimately, one succeeds in accessing their private information and the other doesn’t. The analyst would probably hope to compute and report a 50% login success rate. However, depending on how the data is represented, they could quite easily compute nearly any value from 0% to 100%.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/login-rate.png" class="img-fluid"></p>
<ul>
<li><strong>Per Attempt</strong>: If data is logged once per overall login attempt, successful attempts only trigger one event, but a user who forgot their password may try (and fail) to login multiple times. In the case illustrated above, that deflates the successful login rate to <strong>25%</strong>.</li>
<li><strong>Per Event</strong>: If the logins table contains a row for every login-related event, each ‘success’ will trigger a large number of positive events and each ‘failure’ will trigger a negative event preceded by zero or more positive events. In the case illustrated below, this inflates our successful login rate to <strong>86%</strong>.</li>
<li><strong>Per Conditional</strong>: If the collector decided to only look at downstream events, perhaps to circumvent record duplication, they might decide to create a record only to denote the success or failure of the final step in the login process (MFA). However, login attempts that failed an upstream step would not generate any record for this stage because they’ve already fallen out of the funnel. In this case, the computed rate could reach <strong>100%</strong></li>
<li><strong>Per Intermediate</strong>: Similarly, if the login was defined specifically as successful password verification, the computed rate could his <strong>100%</strong> even if some users subsequently fail MFA</li>
</ul>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Session</th>
<th>Attempt</th>
<th>Attempt</th>
<th>Outcome</th>
<th>Intermediate</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Success</td>
<td>1</td>
<td>1</td>
<td>6</td>
<td>1</td>
<td>2</td>
</tr>
<tr class="even">
<td>Total</td>
<td>2</td>
<td>4</td>
<td>7</td>
<td>1</td>
<td>2</td>
</tr>
<tr class="odd">
<td>Rate</td>
<td>50%</td>
<td><strong>25%</strong></td>
<td><strong>86%</strong></td>
<td><strong>100%</strong></td>
<td><strong>100%</strong></td>
</tr>
</tbody>
</table>
<p>While humans have a shared intuition of what concepts like a user, session, or login are, the act of collecting data forces us to map that intuition onto an atomic event . Any misunderstanding in precisely what that definition is can have massive impact on the perceived data quality; “per event” data will appear heavily duplicated if it is assumed to be “per session” data.</p>
<p>In some cases, this could be obvious to detect. If the system outputs fields that are incredibly specific (e.g.&nbsp;with some hyperbole, imagine a <code>step_in_the_login_process</code> field with values taking any of the human-readable descriptions of the fifteen processes listed in the image above), but depending how this source is organized (e.g.&nbsp;in contrast to above, if we only have fields like <code>sourceid</code> and <code>processid</code> with unintuitive alphanumeric encoded values) and defined, it could be nearly impossible to understand the nuances without uncovering quality metadata or talking to a data producer.<sup>7</sup></p>
</section>
<section id="what-doesnt-count" class="level3">
<h3 class="anchored" data-anchor-id="what-doesnt-count">What Doesn’t Count</h3>
<p>Along with thinking about what <em>does</em> count (or gets logged), it’s equally important to understand what systemically does not generate a record. Consider users who have the intent or desire to login (motivated by a real-world DGP) but cannot find the login page, or users who load the login page but never click a button because they know that they’ve forgotten their password and see no way to request it. Often, some of these corner cases may be some of the most critical and informative (e.g.&nbsp;here, demonstrating some major flaws in our UI). It’s hard to <em>computationally</em> validate what data doesn’t exist, so <em>conceptual</em> data validation is critical.</p>
</section>
<section id="the-many-meanings-of-null" class="level3">
<h3 class="anchored" data-anchor-id="the-many-meanings-of-null">The Many Meanings of Null</h3>
<p>Related to the presence and absence of full <em>records</em> is the presence or absence of individual <em>fields</em>. If records contain some but not all relevant information, they may be published with explicitly missing fields or the full record may not be published at all.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/missing-imp-exp.png" class="img-fluid"></p>
<p>Understanding what the system implies by each <em>explicitly</em> missing data field is also critical for validation and analysis. Checks for data completeness usually include counting null values, but null data isn’t always incorrect. In fact, null data can be highly informative if we know what it means. Some meanings of null data might include:</p>
<ul>
<li><strong>Field is not relevant</strong>: Perhaps our <code>logins</code> table reports the mobile phone operating system (iOS or Android) that was used to access the login page to track platform-specific issues. However, there is no valid value for this</li>
<li><strong>Relevant value is not known</strong>: Our <code>logins</code> table might also have an <code>account_id</code> field which attempts to match login attempts to known accounts/customers using different metadata like cookies or IP addresses. In theory, almost everyone trying to log in should have an account identifier, but our methods may not be good enough to identify them in all cases</li>
<li><strong>Relevant value is null</strong>: Of course, sometimes someone without an account at all might try to log in for some reason. In this case, the correct value for an <code>account_id</code> field truly <em>is</em> null</li>
<li><strong>Relevant value was recorded incorrectly</strong>: Sometimes systems have glitches. Without a doubt, every single login attempt <em>should</em> have a timestamp, but such a field could be null if this data was somehow lost or corrupted at the source</li>
</ul>
<p>Similarly, different systems might or might not report out these nulls in different ways such as:</p>
<ul>
<li><strong>True nulls</strong>: Literally the entry in the resulting dataset is null</li>
<li><strong>Null-like non-nulls</strong>: Blank values like an empty string (<code>''</code>) that contain a null amount of information but won’t be detected when counting null values</li>
<li><strong>Placeholder values</strong>: Meaningless values like an <code>account_id</code> of <code>00000000</code> for all unidentified accounts which preserve data <em>validity</em> (the expected structure) but have no intrinsic meaning</li>
<li><strong>Sentinel/shadow values</strong>: Abnormal values which attempt to indicate the reasons for null-ness such as an <code>account_id</code> of <code>-1</code> when no browser cookies were found or <code>-2</code> when cookies were found but did not help link to any specific customer record</li>
</ul>
<p>Each of these encoding choices changes the definitions of appropriate completeness and validity for each field and, even more critically, impacts the expectations and assertions we should form for data accuracy. We can’t expect 100% completeness if nulls are a relevant value; we can’t check validity of ranges as easily if sentinel values are used with values that are outside the normal range (hopefully, or we have much bigger problems!) So, understanding how upstream systems <em>should</em> work is essential for assessing if they <em>do</em> work.</p>
</section>
</section>
<section id="data-loading" class="level2">
<h2 class="anchored" data-anchor-id="data-loading">Data Loading</h2>
<p>Checking that data contains expected and <em>only</em> expected records (that is, completeness, uniqueness, and timeliness) is one of the most common first steps in data validation. However, the superficially simple act of loading data into a data warehouse or updating data between tables can introduce a variety of risks to data completeness which require different strategies to detect. Data loading errors can result in data that is stale, missing, duplicate, inconsistently up-to-date across sources, or complete but for only a subset of the range you think.</p>
<p>While the data quality principles of <strong>completeness</strong>, <strong>uniqueness</strong>, and <strong>timeliness</strong> would suggest that records should exist once and only once, the reality of many haphazard data loading process means data may appear sometime between zero and a handful of times. Data loads can occur in many different ways. For example, they might be:</p>
<ul>
<li>manually executed</li>
<li>scheduled (like a <a href="https://en.wikipedia.org/wiki/Cron">cron</a> job)</li>
<li>orchestrated (with a tool like <a href="https://airflow.apache.org/">Airflow</a> or <a href="https://www.prefect.io/">Prefect</a>)</li>
</ul>
<p>No approach is free from challenges. For example, scheduled jobs risk executing before an upstream process has completed (resulting in stale or missing data); poorly orchestrated jobs may be prevented from working due to one missing dependency or might allow multiple stream to get out of sync (resulting in multisource missing data). Regardless of the method, all approaches must be carefully configured to handle failures gracefully to avoid creating duplicates, and the frequency at which they are executed may cause partial loading issues if it is incompatible with the granularity of the source data.</p>
<section id="data-load-failure-modes" class="level3">
<h3 class="anchored" data-anchor-id="data-load-failure-modes">Data Load Failure Modes</h3>
<p>For example, suppose in the diagram below that each row of boxes represents one day of records in a table.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/data-load.png" class="img-fluid"></p>
<ul>
<li><strong>Stale data</strong> occurs when the data is not as up-to-date as would be expected from is regular refresh cadence. This could happen if a manual step was skipped, a scheduled job was executed before the upstream source was available, or orchestrated data checks found errors and quarantined new records</li>
<li><strong>Missing data</strong> occurs when one data load fails but subsequent loads have succeeded</li>
<li><strong>Duplicate data</strong> occurs when one data load is executed multiple times</li>
<li><strong>Multisource missing data</strong> occurs when a table is loaded from multiple sources, and some have continued to update as expected while others have not</li>
<li><strong>Partial data</strong> occurs when a table is loaded correctly as intended by the producer but contains less data than expected by the consumer (e.g.&nbsp;a table loads ever 12 hours but because there is some data for a given date, the user assumes that all relevant records for that date have been loaded)</li>
</ul>
<p>The differences in these failure modes become important when an analyst attempts to assess data completeness. One of the first approaches an analyst might consider is simply to check the <code>min()</code> and <code>max()</code> event dates in their table. However, this can only help detect stale data. To catch missing data, an analyst might instead attempt to count the number of <code>distinct</code> days represented in the data; to detect duplicate data, that analyst might need to count records by day and examine the pattern.</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Stale</th>
<th>Missing</th>
<th>Duplicate</th>
<th>Multi</th>
<th>Partial</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>min(date)</code><br> <code>max(date)</code></td>
<td><strong>1<br>3</strong></td>
<td>1<br>4</td>
<td>1<br>4</td>
<td>1<br>4</td>
<td>1<br>4</td>
</tr>
<tr class="even">
<td><code>count(distinct date)</code></td>
<td><strong>3</strong></td>
<td><strong>3</strong></td>
<td>4</td>
<td>4</td>
<td>4</td>
</tr>
<tr class="odd">
<td><code>count(1) by date</code></td>
<td><strong>100<br>100<br>100<br>0</strong></td>
<td><strong>100<br>100<br>0<br>100</strong></td>
<td><strong>100<br>100<br>200<br>100</strong></td>
<td>100<br>100<br>66<br>66</td>
<td>100<br>100<br>100<br>50</td>
</tr>
<tr class="even">
<td><code>count(1)</code><br> <code>count(distinct PKs)</code></td>
<td>300<br>300</td>
<td>300<br>300</td>
<td><strong>400<br>300</strong></td>
<td>332<br>332</td>
<td>350<br>350</td>
</tr>
</tbody>
</table>
<p>In a case like the toy example above where the correct number of rows per date is highly predictable and the number of dates is small, such eyeballing is feasible; however when the expected number of records varies day-to-day or time series are long, this approach becomes subjective, error-prone, and intractable. Additionally, it still might be hard to catch errors in mutli-source data or partial loads if the lower number of records was still within the bounds of reasonable deviation for a series. These last two types deserve further exploration.</p>
</section>
<section id="multi-source" class="level3">
<h3 class="anchored" data-anchor-id="multi-source">Multi-Source</h3>
<p>A more effective strategy for assessing data completeness requires a better understanding of how data is being collected and loaded. In the case of multi-source data, one single source stopping loading may not be a big enough change to disrupt aggregate counts but could still jeopardize meaningful analysis. It would be more useful to conduct completeness checks by <em>subgroup</em> to identify these discrepancies.</p>
<p>But not any subgroup will do; the subgroup must correspond to the various data sources. For example, suppose we run an e-commerce store and wish to look at sales from the past month by category. Naturally, we might think to check the completeness of the data by category. But what if sales data is sourced from three separate locations: our Shopify site (80%), our Amazon Storefront (15%), and phone sales (5%). Unless we explicitly check completeness by channel (a dimension we don’t particularly care about for our analysis), it would be easy to miss if our data source for phone sales has stopped working or loads at a different frequency.</p>
<p>Another interesting aspect of multi-source data, is multiple sources can contribute either to different <em>rows/records</em> or different <em>columns/variables</em>. Table-level frequency counts won’t help us in the latter case since other sources might create the right total number of records but result in some specific fields in those records being missing or inaccurate.</p>
</section>
<section id="partial-loads" class="level3">
<h3 class="anchored" data-anchor-id="partial-loads">Partial Loads</h3>
<p>Partial loads really are not data errors at all, but are still important to detect since they can jeopardize an analysis. A common scenario might occur if a job loads new data every 12 hours (say, data from the morning and afternoon of day n-1 loads on day n at 12AM and 12PM, respectively). An analyst retrieving data at 11AM may be concerned to see an approximate ~50% drop in sales in the past day, despite confirming that their data looks to be “complete” since the maximum record date is, in fact, day n-1.<sup>8</sup></p>
</section>
<section id="delayed-or-transient-records" class="level3">
<h3 class="anchored" data-anchor-id="delayed-or-transient-records">Delayed or Transient Records</h3>
<p>The interaction between choices made in the data collection and data loading phases can introduce their own sets of problems.</p>
<p>Consider an <code>orders</code> table for an e-commerce company that analysts may use to track customer orders. It might contain one record per <code>order_id</code> x <code>event</code> (placement, processing, shipment), one record per order placed, one record per order shipping, or one record per order with a <code>status</code> field that changes over time to denote the order’s current stage of life.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/order-log.png" class="img-fluid"></p>
<p>Any of these modeling choices seem reasonable and the difference between them might appear immaterial. But consider the <em>collection</em> choice to record and report <em>shipped</em> events. Perhaps this might be operationally easier if shipment come from one source system whereas orders could come from many. However, an interesting thing about shipments is that they are often lagged in a variable way from the order date.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/order-date.png" class="img-fluid"></p>
<p>Suppose the e-commerce company in question offers three shipping speeds at checkout. The chart below shows the range of possible shipment dates based on the order dates for the three different speeds (shown in different bars/colors). How might this effect our perceived data quality?</p>
<ul>
<li>Order data could appear <strong>stale</strong> or not timely since orders with a given <code>order_date</code> would only load days later once shipped</li>
<li>Similar to <strong>missing</strong> or <strong>multisource</strong> data, the data <em>range</em> in the table could lead to deceptive and incomplete data validation because some orders from a later order date might ship (and thus be logged) before all orders from a previous order date</li>
<li>Put another way, we could have multiple order dates demonstrating <strong>partial</strong> data loads</li>
<li>These features of the data might behave inconsistently across time due to seasonality (e.g.&nbsp;no shipping on weekends or federal holidays), so heuristics developed to clean the data based on a small number of observations could fail</li>
<li>From an analytical perspective, orders with faster shipping would be disproportionately overrepresented in the “tail” (most recent) data. If shipping category correlated with other characteristics like total order spend, this could create an artificial trend in the data</li>
</ul>
<p>Once again, understanding that data is <em>collected</em> at point of shipment and reasoning how shipment timing varies and impacts <em>loading</em> is necessary for successful validation.</p>
</section>
</section>
<section id="data-transformation" class="level2">
<h2 class="anchored" data-anchor-id="data-transformation">Data Transformation</h2>
<p>Finally, once the data is roughly where we want it, it likely undergoes many transformations to translate all of the system-generated fields we discussed in data collection into semantically-relevant dimensions for analytical consumers. Of course, the types of transformations that could be done are innumerable with far more variation than data loading. So, we’ll just look at a few examples of common failure patterns.</p>
<section id="pre-aggregation" class="level3">
<h3 class="anchored" data-anchor-id="pre-aggregation">Pre-Aggregation</h3>
<p>Data transformations may include aggregating data up to higher levels of granularity for easier analysis. For example, a transformation might add up item-level purchase data to make it easier for an analyst to look at spend per <em>order</em> of a specific user.</p>
<p>Data transformations not only transform our data, but they also transform how the dimensions of data quality manifest. If data with some of the <strong>completeness</strong> or <strong>uniqueness</strong> issues we discussed with data loading is pre-aggregated, these problems can turn into problems of <strong>accuracy</strong>. For example, the duplicate or partial data loads that we discussed when aggregated could suggest inaccurately high or low quantities respectively.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/completeness-accuracy.png" class="img-fluid"></p>
</section>
<section id="field-encoding" class="level3">
<h3 class="anchored" data-anchor-id="field-encoding">Field Encoding</h3>
<p>When we assess data consistency across tables,</p>
<p>Categorical fields in a data set might be created in any number of ways including:</p>
<ul>
<li>Directly taken from the source</li>
<li>Coded in a transformation script</li>
<li>Transformed with logic in a shared user-defined function (<a href="https://docs.snowflake.com/en/sql-reference/user-defined-functions.html">UDFs</a>) or <a href="https://docs.getdbt.com/docs/building-a-dbt-project/jinja-macros/#macros">macro</a></li>
<li>Joined from a shared look-up table</li>
</ul>
<p>Each approach has different implications on data consistency and usability.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/field-encoding.png" class="img-fluid"></p>
<p>Using fields from the source simply is what it is – there’s no subjectivity or room for manual human error. If multiple tables come from the same source, it’s likely but not guaranteed that they will be encoded in the same way.</p>
<p>Coding transformations in the ELT process is easy for data producers. There’s no need to coordinate across multiple processes or use cases, and the transformation can be immediately modified when needed. However, that same lack of coordination can lead to different results for fields that should be the same.</p>
<p>Alternatively, macros, UDFs, and look-up tables provided centralized ways to map source data inputs to desired analytical data outputs in a systemic and consistent way. Of course, centralization has its own challenges. If something in the source data changes, the process of updating a centralized UDF or look-up table may be slowed down by the need to seek consensus and collaborate. So, data is more <em>consistent</em> but potentially less <em>accurate</em>.</p>
<p>Regardless, such engineered values require scrutiny – paticularly if they are being used as a key to join multiple tables – and the distinct values in them should be carefully examined.</p>
</section>
<section id="updating-transformations" class="level3">
<h3 class="anchored" data-anchor-id="updating-transformations">Updating Transformations</h3>
<p>Of course, data consistency is not only a problem across different data sources but within one data source. Regardless of the method of field encoding used in the previous step, the intersection of data loading and data transformation strategies can introduce data consistency errors over time.</p>
<p>Often, for computation efficiency, analytical tables are loaded using an <em>incremental</em> loading strategy. This means that only new records (determined by time period, a set of unique keys, or other criteria) from the upstream source are loaded to the downstream table. This is in contrast to a <em>full refresh</em> where the entire downstream table is recreated on each update.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/incr-full-good.png" class="img-fluid"></p>
<p>Incremental loads have many advantages. Rebuilding tables in entirety can be very time consuming and computationally expensive. In particular, in non-cloud data warehouses that are not able to scale computing power on demand, this sort of heavy duty processing job can noticeably drain resources from other queries that are trying to run in the database. Additionally, if the upstream staging data is ephemeral, fully rebuilding the table could mean failing to retain history.</p>
<p>However, in the case that our data transformations change, incremental loads may introduce inconsistency in our data overtime as only new records are created and inserted with the new logic.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/incr-full-bad-col.png" class="img-fluid"></p>
<p>This is also a problem more broadly if some short-term error is discovered either with data loading or transformation in historical data. Incremental strategies may not always update to include the corrected version of the data.</p>
<p><img src="https://emilyriederer.com/post/data-error-gen/incr-full-bad-row.png" class="img-fluid"></p>
<p>Regardless, this underscores the need to validate entire datasets and to re-validate when repulling data.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In statistical modeling, the goal of considering the data generating process is not to understand an encode every single nuance of the complete DGP. After all, if all of that were known, we wouldn’t need a model: we could simulate the universe.</p>
<p>Similarly for data validation, data consumers cannot know everything about the data production DGP without taking over the data production process in its entirety. But understanding some of the key failure modes faced by data producers can support data validation by helping consumers develop more realistic theories and expectations for the ways data may ‘break’ and how to refine strategies for detection them.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Michael Betancourt’s <a href="https://betanalpha.github.io/assets/case_studies/modeling_and_inference.html#1_probabilistic_modeling">tutorial</a> is a lovely example. Thanks to <a href="https://twitter.com/josephlewis1992">Joseph Lewis</a> on Twitter for the reference.↩︎</p></li>
<li id="fn2"><p>The open-source text <a href="https://web.stanford.edu/class/bios221/book/Chap-Mixtures.html"><em>Modern Statistics for Modern Biology</em></a> by Susan Holmes and Wolfgang Huber contains more examples.↩︎</p></li>
<li id="fn3"><p>Of course, strategies for collection, moving, transforming, storing, and validating data are innumerable. This is not intended to be a comprehensive guide on any of these topics but simply to illustrate why its important for analysts to keep in mind the <em>interplay</em> between these steps.↩︎</p></li>
<li id="fn4"><p>I don’t mean to imply statisticians do not regularly think about the data collection DGP! The rich literatures on missing data imputation, censored data in survival analysis, and non-response bias is survey data collection are just a few examples of how carefully statisticians think about how data collection impacts analysis. I chose to break it out here to discuss the more technical aspects of collection↩︎</p></li>
<li id="fn5"><p>Like NYC’s infamously messy <a href="http://web.mta.info/developers/turnstile.html">turnstile data</a>. I don’t claim to know precisely how this dataset is created, but many of the specific challenges it contains are highly relevant.↩︎</p></li>
<li id="fn6"><p>As Angela Bass so aptly <a href="https://medium.com/@angebassa/data-alone-isnt-ground-truth-9e733079dfd4">writes</a>: “Data isn’t ground truth. Data are artifacts of systems.”↩︎</p></li>
<li id="fn7"><p>Of course, this isn’t the only potential type of issue in data collection. While <em>instrumentation</em> often leads to these definitional challenges, other types of data collection like <em>sensors</em> can have other types of challenges like systematically failing to capture certain observations. Consider, for example, bus ridership data collected as riders scan their pass upon entering. If students can ride free by showing the driver their student ID, these observations may be systemically not recorded. Again, relying on an <em>operational</em> system could lead <em>analytics</em> uses astray (like failing to account for peak usage times for this demographic.)↩︎</p></li>
<li id="fn8"><p>Of course, this concern could be somewhat easily allayed if they then checked a timestamp field, but such a field might not exists or might not have been used for validation since its harder to anticipate the appropriate maximum timestamp than it is the maximum date.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>data</category>
  <category>elt</category>
  <guid>https://emilyriederer.com/post/data-error-gen/</guid>
  <pubDate>Thu, 27 May 2021 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/data-error-gen/featured.png" medium="image" type="image/png" height="62" width="144"/>
</item>
</channel>
</rss>
