<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Emily Riederer</title>
<link>https://emilyriederer.com/</link>
<atom:link href="https://emilyriederer.com/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.4.554</generator>
<lastBuildDate>Wed, 25 Jun 2025 05:00:00 GMT</lastBuildDate>
<item>
  <title>Casual Inference Pod - Optimizing Data Workflows with Emily Riederer (Season 6, Episode 8)</title>
  <link>https://emilyriederer.com/talk/casual-pod/</link>
  <description><![CDATA[ 




<section id="quick-links" class="level2">
<h2 class="anchored" data-anchor-id="quick-links">Quick Links</h2>
<p><span><i class="bi bi-mic"></i> <a href="https://casualinfer.libsyn.com/site.pdf">Podcast Episode</a> </span></p>
<p>Casual Inference is a podcast on all things epidemiology, statistics, data science, causal inference, and public health. Sponsored by the American Journal of Epidemiology. As a guest on this episode, I discuss data science communication, the different challenges of causal analysis in industry versus academia, and much more.</p>


</section>

 ]]></description>
  <category>causal</category>
  <category>data</category>
  <guid>https://emilyriederer.com/talk/casual-pod/</guid>
  <pubDate>Wed, 25 Jun 2025 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/casual-pod/featured.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>A different type of DAG - data pipelines for epidemiology</title>
  <link>https://emilyriederer.com/talk/epi-pipes/</link>
  <description><![CDATA[ 




<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides.pdf">Slides</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>This talk was part of a symposium on data science tools and opportunities for adoption in epidemiology. The full session description is provided below:</p>
<p>Most applied research and education in epidemiology does not yet benefit from modern data science. Fledgling epidemiologists may receive cutting-edge education on the theory of epidemiologic methods, but remain largely untrained in how to collect data effectively, how to apply modern analytical methods to real data sets, how to reproducibly document code and results, and how to effectively work in teams in a digital workplace. Despite their own nagging concerns, they may rely on Dr.&nbsp;Google as their training on algorithms, document study procedures in e-mail chains, store data in spreadsheets, copy-paste analytical code, hard-code observations per person into separate variables, and manually type out estimates into results tables – only to discover that they are requested to do it all over when three study participants turn out to be ineligible for an analysis.</p>
<p>This symposium will illustrate success stories on how to efficiently practice data science in epidemiology and how to teach it along the way. There will be no exhortations how Excel is bad and that good people practice code sharing. Instead, the symposium will discuss cutting-edge approaches and real-life use cases of how modern data science has made research and teaching more efficient. The goal is for attendees to bring home a sparkling, vetted toolkit of new ideas and tools for research and teaching.</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div id="slides" style="width:100%; aspect-ratio:16/11;">
<embed src="slides.pdf#zoom=Fit" width="100%" height="100%">
</div>
</div>
</div>
</div>



 ]]></description>
  <category>workflow</category>
  <category>data</category>
  <guid>https://emilyriederer.com/talk/epi-pipes/</guid>
  <pubDate>Wed, 11 Jun 2025 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/epi-pipes/featured.png" medium="image" type="image/png" height="82" width="144"/>
</item>
<item>
  <title>Python Rgonomics - 2025 Update</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/py-rgo-2025/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/py-rgo-2025/featured.jpg" class="img-fluid figure-img"></p>
<figcaption>Photo credit to the inimitable <a href="https://allisonhorst.com/">Allison Horst</a></figcaption>
</figure>
</div>
<p>About a year ago, I wrote the <a href="post/py-rgo">original version of Python Rgonomics</a> to help fellow former R users who were entering into the world of python. The general point of the article was that new python tooling (e.g.&nbsp;<code>polars</code> versus <code>pandas</code>) has evolved to a point where there are tools that remain truly performant and pythonic while still having a more similar user experience for those coming from the R world. I also discussed this at <a href="talk/python-rgnomics">posit::conf(2025)</a>.</p>
<p>Ironically, the thesis held so true that it condemned my first 2024 post on the topic. 2024 saw the release of a few game-changing tools that further streamline and simplify the python workflow. This post provides an updated set of recommendations. Specifically, it highlights:</p>
<ul>
<li><strong>Consolidating installation and environment management tooling</strong>: Previously, I recommended <code>pyenv</code> for instaling python versions and <code>pdm</code> for project and environment management. Then, last year saw the release of <a href="https://docs.astral.sh/uv/">Astral’s excellent <code>uv</code></a> which nicely consolidates this functionality into a single highly performant tool.</li>
<li><strong>Considering multiple IDE options</strong>: In addition to <code>VS Code</code>, I submit Posit PBC’s <a href="https://positron.posit.co/"><code>Positron</code></a> for consideration depending on comfort, needs, and use cases. Both are backed by the open-source Code OSS with different layers of flexibility or customization. Positron is mostly interoperable with VS Code extensions, but provides a bit more of a “batteries included” opinionated design for the data analyst persona that may not want to navigate through the customization afforded by VS Code.</li>
</ul>
<p>It is important to have a stable stack and not always jump to the next bright, shiny object; however, as I’ve watched these projects evolve throughout 2024, I feel confident to say they are not just a flash in the pan.</p>
<p><code>uv</code> is supported by the Charlie Marsh’s Astral, which formerly made <code>ruff</code> to consolidate a number of code quality tools. Astral’s commitment to open source, the careful design, and the incredible performance becnhmarks of <code>uv</code> speak for itself. Similarly, Positron is backed by the reliable Posit PBC (formerly RStudio) as an open source extension of Code OSS (which is also the open-source skeleton for Microsoft’s VS Code).</p>
<p>The rest of this post is reproduced in full with relevant updates so it reads end-to-end instead of referencing the changes from old to new recommendations.</p>
<section id="now-lets-get-started" class="level2">
<h2 class="anchored" data-anchor-id="now-lets-get-started">Now let’s get started</h2>
<p>The “expert-novice” duality is an uncomfortable part of switching between languages like R and python. Learning a new language is easily enough done; programming 101 concepts like truth tables and control flow translate seamlessly. But ergonomics of a language do not. The tips and tricks we learn to be hyper productive in a primary language are comfortable, familiar, elegant, and effective. They just <em>feel</em> good. Working in a new language, developers often face a choice between forcing their favored workflows into a new tool where they may not “fit”, writing technically correct yet plodding code to get the job done, or approaching a new language as a true beginner to learn it’s “feel” from the ground up.</p>
<p>Fortunately, some of these higher-level paradigms have begun to bleed across languages, enriching previously isolated tribes with the and enabling developers to take their advanced skillsets with them across languages. For any R users who aim to upskill in python in 2024, recent tools and versions of old favorites have made strides in converging the R and python data science stacks. In this post, I will overview some recommended tools that are both truly pythonic while capturing the comfort and familiarity of some favorite R packages of the <code>tidyverse</code> variety.<sup>1</sup></p>
</section>
<section id="what-this-post-is-not" class="level2">
<h2 class="anchored" data-anchor-id="what-this-post-is-not">What this post is not</h2>
<p>Just to be clear:</p>
<ul>
<li>This is not a post about why python is better than R so R users should switch all their work to python</li>
<li>This is not a post about why R is better than python so R semantics and conventions should be forced into python</li>
<li>This is not a post about why python <em>users</em> are better than R users so R users need coddling</li>
<li>This is not a post about why R <em>users</em> are better than python users and have superior tastes for their toolkit</li>
<li>This is not a post about why these python tools are the only good tools and others are bad tools</li>
</ul>
<p>If you told me you liked the New York’s Museum of Metropolitan Art, I might say that you might also like Chicago’s Art Institute. That doesn’t mean you should only go to the museum in Chicago or that you should never go to the Louvre in Paris. That’s not how recommendations (by human or recsys) work. This is an “opinionated” post in the sense that “I like this” and not opinionated in the sense that “you must do this”.</p>
</section>
<section id="on-picking-tools" class="level2">
<h2 class="anchored" data-anchor-id="on-picking-tools">On picking tools</h2>
<p>The tools I highlight below tend to have two competing features:</p>
<ul>
<li>They have aspects of their workflow and ergonomics that should feel very comfortable to users of favored R tools</li>
<li>They should be independently accepted, successful, and well-maintained python projects with the true pythonic spirit</li>
</ul>
<p>The former is important because otherwise there’s nothing tailored about these recommendations; the latter is important so users actually engage with the python language and community instead of dabbling around in its more peripheral edges. In short, these two principles <em>exclude</em> tools that are direct ports between languages with that as their sole or main benefit.<sup>2</sup></p>
<p>For example, <code>siuba</code> and <code>plotnine</code> were written with the direct intent of mirroring R syntax. They have seen some success and adoption, but more niche tools come with liabilities. With smaller user-bases, they tend to lack in the pace of development, community support, prior art, StackOverflow questions, blog posts, conference talks, discussions, others to collaborate with, cache in a portfolio, etc. Instead of enjoying the ergonomics of an old language or embracing the challenge of learning a new one, ports can sometimes force developers to invest energy into a “secret third thing” of learning tools that isolate them from both communities and facing inevitable snags by themselves.</p>
<p>When in Rome, do as the Romans do – but if you’re coming from the U.S. that doesn’t mean you can’t bring a universal adapter that can help charge your devices in European outlets.</p>
</section>
<section id="the-stack" class="level2">
<h2 class="anchored" data-anchor-id="the-stack">The stack</h2>
<p>WIth that preamble out of the way, below are a few recommendations for the most ergonomic tools for getting set up, conducting core data analysis, and communication results.</p>
<p>To preview these recommendations:</p>
<p><strong>Set Up</strong></p>
<ul>
<li>Installation: <a href="https://docs.astral.sh/uv/"><code>uv</code></a></li>
<li>IDE:
<ul>
<li><a href="https://code.visualstudio.com/docs/languages/python">VS Code</a>, or</li>
<li><a href="https://positron.posit.co/">Positron</a></li>
</ul></li>
</ul>
<p><strong>Analysis</strong></p>
<ul>
<li>Wrangling: <a href="https://pola.rs/"><code>polars</code></a></li>
<li>Visualization: <a href="https://seaborn.pydata.org/"><code>seaborn</code></a></li>
</ul>
<p><strong>Communication</strong></p>
<ul>
<li>Tables: <a href="https://posit-dev.github.io/great-tables/articles/intro.html">Great Tables</a></li>
<li>Notebooks: <a href="https://quarto.org/">Quarto</a></li>
</ul>
<p><strong>Miscellaneous</strong></p>
<ul>
<li>Environment Management: <a href="https://docs.astral.sh/uv/"><code>uv</code></a></li>
<li>Code Quality: <a href="https://docs.astral.sh/ruff/"><code>ruff</code></a></li>
</ul>
<section id="for-setting-up" class="level3">
<h3 class="anchored" data-anchor-id="for-setting-up">For setting up</h3>
<p>The first hurdle is often getting started – both in terms of installing the tools you’ll need and getting into a comfortable IDE to run them.</p>
<ul>
<li><strong>Installation</strong>: R keeps installation simple; there’s one way to do it so you do and it’s done<sup>3</sup>. But before python converts can <code>print("hello world")</code>, they face a range of options (system Python, Python installer UI, Anaconda, Miniconda, etc.) each with its own kinks. These decisions are made harder in Python since projects tend to have stronger dependencies of the language, requiring one to switch between versions. Fortunately, <code>uv</code> now makes this task easy with <a href="https://docs.astral.sh/uv/concepts/python-versions/#installing-a-python-version">many different commands for</a>:
<ul>
<li>Installing one or more specific versions: <code>uv python install &lt;version, constraints, etc.&gt;</code></li>
<li>Listing all available installations: <code>uv python list</code></li>
<li>Returning path of python executables: <code>uv python find</code></li>
<li>Spinning up a quick REPL with a <a href="https://valatka.dev/2025/01/12/on-killer-uv-feature.html">temporary python version</a> and packages: e.g.&nbsp;<code>uv run --python 3.12 --with pandas python</code></li>
</ul></li>
<li><strong>Integrated Development Environment</strong>: Once R is install, R users are typically off to the races with the intuitive RStudio IDE which helps them get immediately hands-on with the REPL. With the UI divided into quadrants, users can write an R script, run it to see results in the console, conceptualize what the program “knows” with the variable explorer, and navigate files through a file explorer. Once again, python is not lacking in IDE options, but users are confronted with yet another decision point before they even get started. Pycharm, Sublime, Spyder, Eclipse, Atom, Neovim, oh my! For python, I’d recommend either VS Code or Positron, which are both extensions of Code OSS.
<ul>
<li><a href="https://code.visualstudio.com/docs/languages/python">VS Code</a> is an industry standard tool for software development. This means it has a rich set of features for coding, debugging, navigating large projects, etc. It’s rich extension ecosystem also means that most major tools (e.g.&nbsp;Quarto, git, linters and stylers, etc.) have nice add-ons so, like RStudio, you can customize your platform to perform many side-tasks in plaintext or with the support of extra UI components.<sup>4</sup></li>
<li><a href="https://positron.posit.co/">Positron</a> is a newer entrant from Posit PBC (formerly RStudio). It streamlines the offerings of VS Code to center the features most useful for data analysis. Positron may feel easier to go from zero-to-one. It does a great job finding and consistently using the right versions of R, python, Quarto, etc. and prioritizes many of the IDE elements that make RStudio wonderful for working with data (e.g.&nbsp;object preview pane). Additionally, <em>most</em> VS Code extensions will work in Positron; however, Positron cannot use extensions <a href="https://mastodon.social/@emilyriederer/112853049023389552">that rely on Microsoft’s PyLance</a> meaning some realtime linting and error detection tools like ErrorLens do not work out-of-the-box. Ultimately, your comfort navigating VS Code and your mix of dev versus data work may determine which is best for you.</li>
</ul></li>
</ul>
</section>
<section id="for-data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="for-data-analysis">For data analysis</h3>
<p>As data practitioners know, we’ll spend most of our time on cleaning and wrangling. As such, R users may struggle particularly to abandon their favorite tools for exploratory data analysis like <code>dplyr</code> and <code>ggplot2</code>. Fans of those packages often appreciate how their functional paradigm helps achieve a “flow state”. Precise syntax may differ, but new developments in the python wrangling stack provide increasingly close analogs to some of these beloved Rgonomics.</p>
<ul>
<li><strong>Data Wrangling</strong>: (<a href="post/py-rgo-polars">See my separate post on <code>polars</code></a>)Although <code>pandas</code> is undoubtedly the best-known wrangling tool in the python space, I believe the growing <a href="https://pola.rs/"><code>polars</code></a> project offers the best experience for a transitioning developer (along with other nice-to-have benefits like being dependency free and blazingly fast). <code>polars</code> may feel more natural and less error-prone to R users for may reasons:
<ul>
<li>it has more internal consistent (and similar to <code>dplyr</code>) syntax such as <code>select</code>, <code>filter</code>, etc. and has demonstrated that the project values a clean API (e.g.&nbsp;recently renaming <code>groupby</code> to <code>group_by</code>)</li>
<li>it does not rely on the distinction between columns and indexes which can feel unintuitive and introduces a new set of concepts to learn</li>
<li>it consistently returns copies of dataframes (while <code>pandas</code> sometimes alters in-place) so code is more idempotent and avoids a whole class of failure modes for new users</li>
<li>it enables many of the same “advanced” wrangling workflows in <code>dplyr</code> with high-level, semantic code like making the transformation of multiple variables at once fast with <a href="https://docs.pola.rs/py-polars/html/reference/selectors.html">column selectors</a>, concisely expressing <a href="https://docs.pola.rs/user-guide/expressions/window/">window functions</a>, and working with nested data (or what <code>dplyr</code> calls “list columns”) with <a href="https://docs.pola.rs/user-guide/expressions/lists/">lists</a> and <a href="https://docs.pola.rs/user-guide/expressions/structs/">structs</a></li>
<li>supporting users working with increasingly large data. Similar to <code>dplyr</code>’s many backends (e.g.&nbsp;<code>dbplyr</code>), <code>polars</code> can be used to write lazily-evaluated, optimized transformations and it’s syntax is reminiscent of <code>pyspark</code> should users ever need to switch between</li>
</ul></li>
<li><strong>Visualization</strong>: Even some of R’s critics will acknowledge the strength of <code>ggplot2</code> for visualization, both in terms of it’s intuitive and incremental API and the stunning graphics it can produce. <a href="https://seaborn.pydata.org/tutorial/objects_interface"><code>seaborn</code>’s object interface</a> seems to strike a great balance between offering a similar workflow (which <a href="https://seaborn.pydata.org/whatsnew/v0.12.0.html">cites <code>ggplot2</code> as an inspiration</a>) while bringing all the benefits of using an industry-standard tool</li>
</ul>
</section>
<section id="for-communication" class="level3">
<h3 class="anchored" data-anchor-id="for-communication">For communication</h3>
<p>Historically, one possible dividing line between R and python has been framed as “python is good at working with computers, R is good at working with people”. While that is partially inspired by reductive takes that R is not production-grade, it is not without truth that the R’s academic roots spurred it to overinvest in a rich “communication stack” and translating analytical outputs into human-readable, publishable outputs. Here, too, the gaps have begun to close.</p>
<ul>
<li><strong>Tables</strong>: R has no shortage of packages for creating nicely formatted tables, an area that has historically lacked a bit in python both in workflow and outcomes. Barring strong competition from the native python space, the one “port” I am bullish about is the recently announced <a href="https://posit-dev.github.io/great-tables/articles/intro.html">Great Tables</a> package. This is a pythonic clone of R’s <code>gt</code> package. I’m more comfortable recommending this since it’s maintained by the same developer as the R version (to support long-term feature parity), backed by an institution not just an individual (to ensure it’s not a short-lived hobby project), and the design feels like it does a good job balancing R inspiration with pythonic practices</li>
<li><strong>Computational notebooks</strong>: Jupyter Notebooks are widely used, widely critiqued parts of many python workflows. While the ability to mix markdown and code chunks. However, notebooks can introduce new types of bugs for the uninitiated; for example, they are hard to version control and easy to execute in the wrong environment. For those coming from the world of R Markdown, plaintext computational notebooks like <a href="https://quarto.org/">Quarto</a> may provide a more transparent development experience. While Quarto allows users to write in <code>.qmd</code> files which are more like their <code>.rmd</code> predecessors, its renderer can also handle Jupyter notebooks to enable collaboration across team members with different preferences</li>
</ul>
</section>
<section id="miscellaneous" class="level3">
<h3 class="anchored" data-anchor-id="miscellaneous">Miscellaneous</h3>
<p>A few more tools may be helpful and familiar to <em>some</em> R users who tend towards the more “developer” versus “analyst” side of the spectrum. These, in my mind, have even more varied pros and cons, but I’ll leave them for consideration:</p>
<ul>
<li><strong>Environment Management</strong>: There’s a truly overwhelming number of ways<sup>5</sup> to manage project-level dependencies in python. As a consequence, there’s also a lot of outdated advice weighing pros and cons of feature sets that have since evolved. Here again, <code>uv</code> takes the cake as a swiss army knife tool. It features fast installation, auto-updating of the <code>pyproject.toml</code> and <code>uv.lock</code> files (so you don’t need to remember to <code>pip freeze</code>), separate trakcing of primary dependencies from the fully resolved environment (so you can cleanly and completely remove dependencies-of-dependencies you no longer need), and so much more. <code>uv</code> can operate as a drop in replacement for <code>pip</code> and generate a <code>requirements.txt</code> if needed for compatability; however, given it’s explosive popularity and ergonomic design, I doubt you’ll have trouble convincing collaborators to adopt the same.</li>
<li><strong>Developer Tools</strong>: <a href="https://docs.astral.sh/ruff/"><code>ruff</code></a> (another Astral project) provides a range of linting and styling options (think R’s <code>lintr</code> and <code>styler</code>) and provides a one-stop-shop over what can be an overwhelming number of atomic tools in this space (<code>isort</code>, <code>black</code>, <code>flake8</code>, etc.). <code>ruff</code> is super fast, has a nice VS Code extension, and, while this class of tools is generally considered more advanced, I think linters can be a fantastic “coach” for new users about best practices</li>
</ul>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Of course, languages have their own subcultures too. The <code>tidyverse</code> and <code>data.table</code> parts of the R world tend to favor different semantics and ergonomics. This post caters more to the former.↩︎</p></li>
<li id="fn2"><p>There is no doubt a place for language ports, especially for earlier stage project where no native language-specific standard exists. For example, I like Karandeep Singh’s lab work on <a href="https://github.com/TidierOrg/Tidier.jl">a tidyverse for Julia</a> and maintain my own <a href="https://github.com/emilyriederer/dbtplyr"><code>dbtplyr</code></a> package to port <code>dplyr</code>’s select helpers to <code>dbt</code>↩︎</p></li>
<li id="fn3"><p>However, to highlight some advances here, Posit’s newer <a href="https://github.com/r-lib/rig"><code>rig</code></a> project seems to be inspired by python install management tools and offers a convenient CLI for managing multiple version of R↩︎</p></li>
<li id="fn4"><p> If anything, the one challenge of VS Code is the sheer number of set up options, but to start out, you can see these excellent tutorials from Rami Krispin on recommended <a href="https://github.com/RamiKrispin/vscode-python">python</a> and <a href="https://github.com/RamiKrispin/vscode-r">R</a> configurations ↩︎</p></li>
<li id="fn5"><p><code>pdm</code>, <code>virtualenv</code>, <code>conda</code>, <code>piptools</code>, <code>pipenv</code>, <code>poetry</code>, and that doesn’t even scratch the surface↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>rstats</category>
  <category>python</category>
  <guid>https://emilyriederer.com/post/py-rgo-2025/</guid>
  <pubDate>Sun, 26 Jan 2025 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/py-rgo-2025/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Role-Based Access Control for Quarto sites with Netlify Identity</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/quarto-auth-netlify/</link>
  <description><![CDATA[ 





<p>Literate programming tools like R Markdown and Quarto make it easy to convert analyses into aesthetic documents, dashbaords, and websites for public sharing. But what if you don’t want your results <em>too</em> public?</p>
<p>I recently was working on a project that required me to set up a large number of dashboards with similar content but different data for about 10 small, separate organizations. As I considered by tech stack, I found that many Quarto users were <a href="https://github.com/quarto-dev/quarto-cli/discussions/8393">asking similar questions</a>, but understandably the Quarto team had no one slam-dunk answer because authentication management (a serving / hosting problem) would be a substantial scope creep beyond the goals and core functionality of Quarto (an open-source publishing system).</p>
<p>After evaluating my options, I found the best solution for my use case was role-based access controls with <a href="https://docs.netlify.com/security/secure-access-to-sites/identity/">Netlify Identity</a>. In this post, I’ll briefly describe how this solution works, how to set it up, and some of the pros and cons.</p>
<section id="demo" class="level2">
<h2 class="anchored" data-anchor-id="demo">Demo</h2>
<p>Using a minimal Netlify Identity set-up, you can be up and running with the following UX in about 10 minutes. For this post, I show the true “minimum viable deployment”, although the styling and aesthetics could be made much fancier.</p>
<p>When users first visit your site’s homepage, they will be prompted that they need to sign-up or login to continue.</p>
<p><img src="https://emilyriederer.com/post/quarto-auth-netlify/logged-out.png" class="img-fluid"></p>
<p>If users navigate to any other part of the site before logging in, they’ll receive an error message prompting them to return to the home screen. (This could be customized as you would a <code>404 Not Found</code> error.)</p>
<p><img src="https://emilyriederer.com/post/quarto-auth-netlify/not-found.png" class="img-fluid"></p>
<p>After clicking either button, an in-browser popup modal allows them to sign up, login in, or request forgotten credentials.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://emilyriederer.com/post/quarto-auth-netlify/signup.png" class="img-fluid"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="https://emilyriederer.com/post/quarto-auth-netlify/login.png" class="img-fluid"></p>
</div>
</div>
</div>
<p>The example above shows the option to create a custom login or use Google to authenticate. Netlify also allows for the options to use other free (e.g.&nbsp;GitHub, GitLab) or paid (e.g.&nbsp;Okta) third-party login services.</p>
<p>For new signups, Netlify can automatically trigger confirmation emails with <a href="https://docs.netlify.com/security/secure-access-to-sites/identity/identity-generated-emails/">customized content</a> based on a templated text or HTML file in your repository.</p>
<p>Once logged in, the homepage then offers the option to log back out.</p>
<p><img src="https://emilyriederer.com/post/quarto-auth-netlify/logged-in.png" class="img-fluid"></p>
<p>Otherwise, users can then proceed to the rest of the site as if it were publicly available.</p>
</section>
<section id="set-up" class="level2">
<h2 class="anchored" data-anchor-id="set-up">Set Up</h2>
<p>The basics of how Netlify Identity works are described at length in <a href="https://docs.netlify.com/security/secure-access-to-sites/role-based-access-control/#create-users-and-set-roles">this blog post</a>. If you decide to implement this solution, I recommend reading those official documents for a more robust mental model. In short, Netlify Identity works by attaching a token to each user after they log in. This user-specific token can be assigned different roles on the backend, and depending on which roles a user has, they can be redirected to (or gated from) seeing different content.</p>
<p>Setting up Netlify Identify requires a few small tweaks throughout your site:</p>
<ol type="1">
<li>Add Javascript to each page to handle the JSON Web Tokens (JWTs) set by Identity. This is done most easily through the <code>_quarto.yml</code></li>
<li>Configure site redirects to response to the JWTs. This is contained in its own <code>_redirects</code> file</li>
<li>Ensure you have a user interface that allows users to sign up and login, thus changing their JWTs and access. I put this in my <code>index.qmd</code></li>
</ol>
<p>Then, finally, within the Netlify admin panel, you must:</p>
<ol start="4" type="1">
<li>Configure the user signup workflow (e.g.&nbsp;by invitation, open sign-up)</li>
<li>Assign users to roles that determine what content they can see</li>
<li>Optionally, enable third-party forms of authentication (e.g.&nbsp;Google, GitHub)</li>
</ol>
<p>Let’s take these one at a time.</p>
<section id="configure-role-authentiation" class="level3">
<h3 class="anchored" data-anchor-id="configure-role-authentiation">Configure Role Authentiation</h3>
<p>Netlify maintains <a href="https://github.com/netlify/netlify-identity-widget">an Identity widget</a> that handles recognizing authenticated users and their roles from their JWTs. To inject this Javascript snippet into every page, open the <code>_quarto.yml</code> file and add the Javascript snippet to the <code>include-in-header:</code> key under the HTML format, e.g.:</p>
<pre><code>format:
  html: 
    include-in-header: 
      text: |
        &lt;script type="text/javascript" src="https://identity.netlify.com/v1/netlify-identity-widget.js"&gt;&lt;/script&gt;
        &lt;script&gt;
        window.netlifyIdentity.on('login', (user) =&gt; {
        window.netlifyIdentity.refresh(true).then(() =&gt; {
          console.log(user);
        });
        });
        window.netlifyIdentity.on('logout', (user) =&gt; {
        window.location.href = '/login';
        });
        window.netlifyIdentity.init({ container: '#netlify' });
        &lt;/script&gt;</code></pre>
<p>Note, the official widget is injected using the <code>src</code> field of the first <code>script</code> tag.</p>
</section>
<section id="configure-site-redirects" class="level3">
<h3 class="anchored" data-anchor-id="configure-site-redirects">Configure Site Redirects</h3>
<p>Next, create a <a href="https://docs.netlify.com/security/secure-access-to-sites/role-based-access-control/#redirect-visitors-based-on-roles"><code>_redirects</code> file</a> at the top level of your project (or open the existing file) and add the following lines:</p>
<pre><code>/login /
/*  /:splat  200!  Role=admin
/site_libs/* /site_libs/:splat 200!
/   /        200!
/*  /  401!</code></pre>
<p>Syntax for the <code>_redirects</code> file is described <a href="https://docs.netlify.com/routing/redirects/#syntax-for-the-redirects-file">here</a>, but basically each line defines a rule with the structure:</p>
<pre><code>&lt;what was requested&gt; &lt;where to go&gt; &lt;the result to give&gt; &lt;conditional on role&gt;</code></pre>
<p>And, like a <code>case when</code> statement, the first “matching” rule dominates.</p>
<p>So, the example above can roughly be read in English as:</p>
<pre><code>If users go to the /login page, take them back to home
If users try to go anywhere else on my site and they have role admin, let them do that 
If users try to go to the hompage of my site (regardless of their role), let them do that
If users otherwise try to go to other parts of the site (and they don't have admin), give an error</code></pre>
<p>Of course, this could be further customized to set different rules for different subdirectories.</p>
</section>
<section id="create-user-interface" class="level3">
<h3 class="anchored" data-anchor-id="create-user-interface">Create User Interface</h3>
<p>To create the user interface for the login screen, I added code to inject a Netlify-maintained login widget to my site’s <code>index.qmd</code>, e.g.:</p>
<pre><code>---
date: last-modified
---

# Home {.unnumbered}

&lt;div data-netlify-identity-menu&gt;&lt;/div&gt;

Welcome! Please sign in to view the dashboard. 

If you are a first time user, please create a login and email [emilyriederer@gmail.com](mailto:emilyriederer@gmail.com?subject=Dashboard%20Access%20Request) to elevate your access.</code></pre>
</section>
<section id="user-onboarding" class="level3">
<h3 class="anchored" data-anchor-id="user-onboarding">User Onboarding</h3>
<p>After the changes above to your actual Quarto site, the rest of the work lies in the Netlify admin panel. For a small number of users, you can manually change their role in the user interface.</p>
<p><img src="https://emilyriederer.com/post/quarto-auth-netlify/user-mgmt.png" class="img-fluid"></p>
<p>However, to work at any scale, you may need a more automated solution. For that, Netlify’s docs explain how to configure initial role assignment via <a href="https://www.netlify.com/blog/2019/02/21/the-role-of-roles-and-how-to-set-them-in-netlify-identity/">lambda functions</a>. However, out-of-the box functionality that I found to be lacking was assigning default roles for new users or the ability to configure basic logic such as assigning the same role to any new users onboarding from a certain email domain.</p>
</section>
</section>
<section id="is-it-for-you" class="level2">
<h2 class="anchored" data-anchor-id="is-it-for-you">Is it for you?</h2>
<p>Netlify Identity isn’t the perfect solution for all use cases, but for many small websites and blogs it’s possibly one of the lowest friction solutions available.</p>
<p>This solution is easy to set up initially, allows some degree of self-service for users (account set-up and password resets), user communication (email management), and third-party integration (e.g.&nbsp;authenticate with GitHub or Google). It also has a robust free tier, allowing 1K users to self register (and 5 registrations-by-invitation), and is a substantial step up over locking down HTML content with a single common password.</p>
<p>However, Netlify Identity is not a bullet-proof end-to-end security solution and could become painful or expensive at large scale. This solution, for example, doesn’t contemplate securing your website’s full “supply chain” (e.g.&nbsp;if the source code in in a public GitHub repo) and certainly is less secure than hosting your site completely within a sanboxed environment or intranet. For a large number of users, I also feel there’s a large opportunity to allow simple business rules to configure initial roles.</p>
<p>In summary, I would generally recommend Netlify Identity if you’re already using Netlify, expect a small number of users, and are comfortable adding <em>friction</em> to your sign-in process versus absolute security. For larger projects with higher usage and more bullet-proof security needs, it may be worth considering alternatives.</p>


</section>

 ]]></description>
  <category>quarto</category>
  <category>rmarkdown</category>
  <category>workflow</category>
  <guid>https://emilyriederer.com/post/quarto-auth-netlify/</guid>
  <pubDate>Sun, 10 Nov 2024 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/quarto-auth-netlify/featured.PNG" medium="image"/>
</item>
<item>
  <title>Python Rgonomics</title>
  <link>https://emilyriederer.com/talk/python-rgonomics/</link>
  <description><![CDATA[ 




<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" aria-controls="tabset-1-4" aria-selected="false">Video</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides.pdf">Slides</a> </span><br>
<span><i class="bi bi-play"></i> <a href="https://www.youtube.com/watch?v=ILxK92HDtvU&amp;list=PL9HYL-VRX0oSFkdF4fJeY63eGDvgofcbn">Video</a> </span><br>
<span><i class="bi bi-pencil"></i> <a href="../..\post/py-rgo/">Post - Python Rgonomics</a> </span><br>
<span><i class="bi bi-pencil"></i> <a href="../..\post/py-rgo-polars/">Post - Advanced <code>polars</code> versus <code>dplyr</code></a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Tooling changes quickly. Since this talk occured, Astral’s <code>uv</code> project has come out as a very strong contender to replace <code>pyenv</code>, <code>pdm</code>, and more of the devtools part of a python stack.</p>
</div>
</div>
<p>Data science languages are increasingly interoperable with advances like Arrow, Quarto, and Posit Connect. But data scientists are not. Learning the basic syntax of a new language is easy, but relearning the ergonomics that help us be hyperproductive is hard. In this talk, I will explore the influential ergonomics of R’s tidyverse. Next, I will recommend a curated stack that mirrors these ergonomics while also being genuinely truly pythonic. In particular, we will explore packages (polars, seaborn objects, greattables), frameworks (Shiny, Quarto), dev tools (pyenv, ruff, and pdm), and IDEs (VS Code extensions). The audience should leave feeling inspired to try python while benefiting from their current knowledge and expertise.</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div id="slides" style="width:100%; aspect-ratio:16/11;">
<embed src="slides.pdf#zoom=Fit" width="100%" height="100%">
</div>
</div>
<div id="tabset-1-4" class="tab-pane" aria-labelledby="tabset-1-4-tab">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ILxK92HDtvU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>



 ]]></description>
  <category>workflow</category>
  <category>python</category>
  <category>rstats</category>
  <guid>https://emilyriederer.com/talk/python-rgonomics/</guid>
  <pubDate>Thu, 15 Aug 2024 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/python-rgonomics/featured.png" medium="image" type="image/png" height="78" width="144"/>
</item>
<item>
  <title>Crosspost: Data discovery doesn’t belong in ad hoc queries</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/data-discovery-ad-hoc/</link>
  <description><![CDATA[ 





<p>Credible documentation is the best tool for working with data. Short of that, labor (and computational) intensive validation may be required. Recently, I had the opportunity to expand on these ideas in a <a href="https://www.selectstar.com/resources/data-discovery-doesnt-belong-in-ad-hoc-queries">cross-post with Select Star</a>. I explore how a “good” data analyst can interrogate a dataset with expensive queries and, more importantly, how best-in-class data products eliminate the need for this.</p>
<p>My post is reproduced below.</p>
<hr>
<p>In the current environment of decreasing headcount and rising cloud costs, the benefits of data management are more objective and tangible than ever. Done well, data management can reduce the cognitive and computational costs of working with enterprise-scale data.</p>
<p>Analysts often jump into new-to-them tables to answer business questions. Without a robust data platform, this constant novelty leads analysts down one of two paths. Either they boldly gamble that they have found intuitive and relevant data or they painstakingly hypothesize and validate assumptions for each new table. The latter approach leads to more trustworthy outcomes, but it comes at the cost of human capital and computational power.</p>
<p>Consider an analyst at an e-commerce company asking the question “How many invoices did we generate for fulfilled orders to Ohio in June?” while navigating unfamiliar tables. In this post, we explore prototypical queries analysts might have to run to validate a new-to-them table. Many of these are “expensive” queries requiring full table scans. Next, we’ll examine how a data discovery platform can obviate this effort.</p>
<p>The impact of this inefficiency may range from a minor papercut to a major cost sink depending on the sizes of your analyst community, historical enterprise data, and warehouse.</p>
<section id="preventable-data-discovery-queries" class="level2">
<h2 class="anchored" data-anchor-id="preventable-data-discovery-queries">6 Preventable Data Discovery Queries</h2>
<section id="what-columns-are-in-the-table" class="level3">
<h3 class="anchored" data-anchor-id="what-columns-are-in-the-table">1. What columns are in the table?</h3>
<p>Without a good data catalog, analysts will first need to check what fields exist in a table. While there may be lower cost ways to do this like looking at a pre-rendered preview (ala BigQuery), using a DESCRIBE statement (ala Spark), or limiting their query to the first few rows, some analysts may default to requesting all the data.</p>
<pre><code>select *
from invoices;</code></pre>
</section>
<section id="is-the-table-still-live-and-updating" class="level3">
<h3 class="anchored" data-anchor-id="is-the-table-still-live-and-updating">2. Is the table still live and updating?</h3>
<p>After establishing that a table has potentially useful information, analysts should next wonder if the data is still live and updating. First they might check a date field to see if the table seems “fresh”.</p>
<pre><code>select max(order_date) 
from invoices;</code></pre>
<p>But, of course, tables often have multiple date fields. For example, an e-commerce invoice table might have fields for both the date an order was placed and the date the record was last modified. So, analysts may guess-and-check a few of these fields to determine table freshness.</p>
<pre><code>select max(updated_date) 
from invoices;</code></pre>
<p>After identifying the correct field, there’s still a question of refresh cadence. Are records added hourly? Daily? Monthly? Lacking system-level metrics and metadata on the upstream table freshness, analysts are still left in the dark. So, once again, they can check empirically by looking at the frequency of the date field.</p>
<pre><code>select max(updated_date), count(1) as n
from invoices
group by 1;</code></pre>
</section>
<section id="what-is-the-grain-of-the-table" class="level3">
<h3 class="anchored" data-anchor-id="what-is-the-grain-of-the-table">3. What is the grain of the table?</h3>
<p>Now that the table is confirmed to be usable, the question becomes how to use it. Specifically, to credibly query and join the table, analysts next must determine its grain. Often, they start with a guess informed by the business context and data modeling conventions, such as assuming an invoice table is unique by order_id.</p>
<pre><code>select count(1) as n, count(distinct order_id)
from invoices;</code></pre>
<p>‍However, if they learn that order_id has a different cardinality then the number of records, they must ask why. So, once again, they scan the full table to find examples of records with shared order_id values.</p>
<pre><code>select *
from invoices
qualify count(1) over (partition by order_id) &gt; 1
order by order_id
limit 10;</code></pre>
<p>Eyeballing the results of this query, the analysts might notice that the same order_id value can coincide with different ship_id values, as a separate invoice is generated for each part of an order when a subset of items is shipped. With this new hypothesis, the analyst iterates on the validation of the grain.</p>
<pre><code>select count(1) as n, count(distinct order_id, ship_id)
from invoices;</code></pre>
</section>
<section id="what-values-can-categorical-variables-take" class="level3">
<h3 class="anchored" data-anchor-id="what-values-can-categorical-variables-take">4. What values can categorical variables take?</h3>
<p>The prior questions all involved table structure. Only now can an analyst finally begin to investigate the table’s content. A first step might be to understand the valid values for categorical variables. For example, if our analyst wanted to ensure only completed orders were queried, they might inspect the potential values of the order_status_id field to determine which values to include in a filter.</p>
<pre><code>select distinct order_status_id
from invoices;</code></pre>
<p>They’ll likely repeat this process for many categorical variables of interest. Since our analyst is interested in shipments specifically to Ohio, they might also inspect the cardinality of the ship_state field to ensure they correctly format the identifier.</p>
<pre><code>select distinct ship_state
from invoices;</code></pre>
</section>
<section id="do-numeric-columns-have-nulls-or-sentinel-values-to-encode-nulls" class="level3">
<h3 class="anchored" data-anchor-id="do-numeric-columns-have-nulls-or-sentinel-values-to-encode-nulls">5. Do numeric columns have nulls or ‘sentinel’ values to encode nulls?</h3>
<p>Similarly, analysts may wish to audit other variables for null handling or sentinel values by inspecting column-level statistics.</p>
<pre><code>select distinct ship_state
from invoices;</code></pre>
</section>
<section id="is-the-data-stored-with-partitioning-or-clustering-keys" class="level3">
<h3 class="anchored" data-anchor-id="is-the-data-stored-with-partitioning-or-clustering-keys">6. Is the data stored with partitioning or clustering keys?</h3>
<p>Inefficient queries aren’t only a symptom of ad hoc data validation. More complex and reused logic may also be written wastefully when table metadata like partitioning and clustering keys is not available to analysts. For example, an analyst might be able to construct a reasonable query filtering either on a shipment date or an order date, but if only one of these is a partitioning or clustering key, different queries could have substantial performance differences.</p>
</section>
</section>
<section id="understanding-your-data-without-relying-on-queries" class="level2">
<h2 class="anchored" data-anchor-id="understanding-your-data-without-relying-on-queries">Understanding Your Data Without Relying on Queries</h2>
<p>Analysts absolutely should ask themselves these types of questions when working with new data. However, it should not be analysts’ job to individually answer these questions by running SQL queries. Instead, best-in-class data documentation can provide critical information through a data catalog like Select Star.</p>
<section id="what-columns-are-in-the-table-and-do-we-need-a-table" class="level3">
<h3 class="anchored" data-anchor-id="what-columns-are-in-the-table-and-do-we-need-a-table">1. What columns are in the table? And do we need a table?</h3>
<p>Comprehensive search across all of an organization’s assets can help users quickly identify the right resources based on table names, field names, or data descriptions. Even better, search can incorporate observed tribal knowledge of table popularity and common querying patterns to prioritize the most relevant results. Moreover, when search also includes downstream data products like pre-built reports and dashboards, analysts might sometimes find an answer to their question exists off the shelf.</p>
</section>
<section id="is-the-table-still-live-and-updating-and-are-its-own-sources-current" class="level3">
<h3 class="anchored" data-anchor-id="is-the-table-still-live-and-updating-and-are-its-own-sources-current">2. Is the table still live and updating? And are its own sources current?</h3>
<p>Data is not a static artifact so metadata should not be either. After analysts identify a candidate table, they should have access to real-time operational information like table usage, table size, refresh date, and upstream dependencies to help confirm whether the table is a reliable resource.</p>
<p>Ideally, analysts can interrogate not just the freshness of a final table but also its dependencies by exploring the table’s data lineage.</p>
</section>
<section id="what-is-the-grain-of-the-table-and-how-does-it-relate-to-others" class="level3">
<h3 class="anchored" data-anchor-id="what-is-the-grain-of-the-table-and-how-does-it-relate-to-others">3. What is the grain of the table? And how does it relate to others?</h3>
<p>Table grain should be clearly documented at the table level and emphasized in the data dictionary via references to primary and foreign keys. Beyond basic documentation, entity-relationship (ER) diagrams will help analysts gain a richer mental model of grains of how they can use these primary-foreign key relationships to link tables to craft information with the desired grain and fields. Alternatively, they can glean this information from the wisdom of the crowds if they have access to how others have queried and joined the data previously.</p>
</section>
<section id="what-values-can-categorical-variables-take-do-numeric-columns-have-nulls-or-sentinel-values-to-encode-nulls" class="level3">
<h3 class="anchored" data-anchor-id="what-values-can-categorical-variables-take-do-numeric-columns-have-nulls-or-sentinel-values-to-encode-nulls">4. What values can categorical variables take? Do numeric columns have nulls or ‘sentinel’ values to encode nulls?</h3>
<p>Information about proper expectations and handling of categorical and null values may be published as field definitions, pointed to lookup tables, implied in data tests, or illustrated in past queries. To drive consistency and offload redundant work from data producers, such field definitions can be propagated from upstream tables.</p>
</section>
<section id="is-the-data-stored-with-partitioning-or-clustering-keys-1" class="level3">
<h3 class="anchored" data-anchor-id="is-the-data-stored-with-partitioning-or-clustering-keys-1">‍5. Is the data stored with partitioning or clustering keys?</h3>
<p>Analysts cannot write efficient code if they don’t know where the efficiency gains lie. Table-level documentation should clearly highlight the use of clustering or partitioning files so analysts can use the most impactful variables in filters and joins. Here, consistency of documentation is paramount; analysts may not always be incented to care about query efficiency, so if this information is hard to find or rarely available, they can be easily dissuaded from looking.</p>
<p>Beyond a poor user experience, poor data discoverability creates inefficiency and added cost. Even if you don’t have large scale historical data or broad data user communities today, slow queries and tedious work still detract from data team productivity while introducing context-switching and chaos. By focusing on improving data discoverability, you can streamline workflows and enhance the overall efficiency of your data operations.</p>


</section>
</section>

 ]]></description>
  <category>data</category>
  <category>workflow</category>
  <category>elt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/data-discovery-ad-hoc/</guid>
  <pubDate>Thu, 18 Jul 2024 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/data-discovery-ad-hoc/featured.png" medium="image" type="image/png" height="76" width="144"/>
</item>
<item>
  <title>Base Python Rgonomic Patterns</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/py-rgo-base/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/py-rgo-base/featured.jpg" class="img-fluid figure-img"></p>
<figcaption>Photo credit to <a href="https://unsplash.com/@davidclode">David Clode</a> on Unsplash</figcaption>
</figure>
</div>
<p>In the past few weeks, I’ve been writing about a <a href="../..\post/py-rgo">stack of tools</a> and <a href="../..\post/py-rgo-polars/">specific packages like <code>polars</code></a> that may help R users feel “at home” when working in python due to similiar ergonomics. However, one common snag in switching languages is ramping up on common “recipes” for higher-level workflows (e.g.&nbsp;how to build a <code>sklearn</code> modeling pipeline) but missing a languages’s fundamentals that make writing glue code feel smooth (and dare I say pleasant?) It’s a maddening feeling to get code for a <em>complex</em> task to finish only to have the result wrapped in an object that you can’t suss out how to save or manipulate.</p>
<p>This post goes back to the basics. We’ll briefly reflect on a few aspects of usability that have led to the success of many workflow packages in R. Then, I’ll demonstrate a grab bag of coding patterns in python that make it feel more elegant to connect bits of code into a coherent workflow.</p>
<p>We’ll look at the kind of functionality that you didn’t know to miss until it was gone, you may not be quite sure what to search to figure out how to get it back, <em>and</em> you wonder if it’s even reasonable to hope there’s an analog<sup>1</sup>. This won’t be anything groundbreaking – just some nuts and bolts. Specifically: helper functions for data and time manipulation, advanced string interpolation, list comprehensions for more functional programming, and object serialization.</p>
<section id="what-other-r-ergonomics-do-we-enjoy" class="level2">
<h2 class="anchored" data-anchor-id="what-other-r-ergonomics-do-we-enjoy">What other R ergonomics do we enjoy?</h2>
<p>R’s passionate user and developer community has invested a lot in building tools that smooth over rough edges and provide slick, concise APIs to rote tasks. Sepcifically, a number of packages are devoted to:</p>
<ul>
<li><strong>Utility functions</strong>: Things that make it easier to “automate the boring stuff” like <code>fs</code> for naviating file systems or <code>lubridate</code> for more semantic date wrangling</li>
<li><strong>Formatting functions</strong>: Things that help us make things look nice for users like <code>cli</code> and <code>glue</code> to improve human readability of terminal output and string interpolation</li>
<li><strong>Efficiency functions</strong>: Things that help us write efficient workflows like <code>purrr</code> which provides a concise, typesafe interface for iteration</li>
</ul>
<p>All of these capabilities are things we <em>could</em> somewhat trivially write ourselves, but we don’t <em>want</em> to and we don’t <em>need</em> to. Fortunately, we don’t need to in python either.</p>
</section>
<section id="wrangling-things-date-manipulation" class="level2">
<h2 class="anchored" data-anchor-id="wrangling-things-date-manipulation">Wrangling Things (Date Manipulation)</h2>
<p>I don’t know a data person who loves dates. In the R world, many enjoy <code>lubridate</code>’s wide range of helper functions for cleaning, formatting, and computing on dates.</p>
<p>Python’s <code>datetime</code> module is similarly effective. We can easily create and manage dates in <code>date</code> or <code>datetime</code> classes which make them easy to work with.</p>
<div id="ea34065a" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> datetime</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datetime <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> date</span>
<span id="cb1-3">today <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> date.today()</span>
<span id="cb1-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(today)</span>
<span id="cb1-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(today)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2024-01-20</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>datetime.date</code></pre>
</div>
</div>
<p>Two of the most important functions are <code>strftime()</code> and <code>strptime()</code>.</p>
<p><code>strftime()</code> <em>formats</em> dates into strings. It accepts both a date and the desired string format. Below, we demonstrate by commiting the cardinal sin of writing a date in non-ISO8601.</p>
<div id="ca22a3a1" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">today_str <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datetime.datetime.strftime(today, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%m/</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%d</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">/%Y'</span>)</span>
<span id="cb4-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(today_str)</span>
<span id="cb4-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(today_str)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>01/20/2024</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="2">
<pre><code>str</code></pre>
</div>
</div>
<p><code>strptime()</code> does the opposite and turns a string encoding a date into an actual date. It can try to guess the format, or we can be nice and provide it guidance.</p>
<div id="2fe33487" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">someday_dtm <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datetime.datetime.strptime(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'2023-01-01'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%Y-%m-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%d</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>)</span>
<span id="cb7-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(someday_dtm)</span>
<span id="cb7-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(someday_dtm)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2023-01-01 00:00:00</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>datetime.datetime</code></pre>
</div>
</div>
<p>Date math is also relatively easy with <code>datetime</code>. For example, you can see we calculate the date difference simply by… taking the difference! From the resulting delta object, we can access the <code>days</code> attribute.</p>
<div id="324e084a" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">n_days_diff <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> ( today <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> someday_dtm.date() )</span>
<span id="cb10-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(n_days_diff)</span>
<span id="cb10-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(n_days_diff)</span>
<span id="cb10-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(n_days_diff.days)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>384 days, 0:00:00</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>int</code></pre>
</div>
</div>
</section>
<section id="formatting-things-f-strings" class="level2">
<h2 class="anchored" data-anchor-id="formatting-things-f-strings">Formatting Things (f-strings)</h2>
<p>R’s <code>glue</code> is beloved for it’s ability to easily combine variables and texts into complex strings without a lot of ugly, nested <code>paste()</code> functions.</p>
<p>python has a number of ways of doing this, but the most readable is the newest: f-strings. Simply put an <code>f</code> before the string and put any variable names to be interpolated in <code>{</code>curly braces<code>}</code>.</p>
<div id="bd04d21e" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Emily"</span></span>
<span id="cb13-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"This blog post is written by </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>name<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>This blog post is written by Emily</code></pre>
</div>
</div>
<p>f-strings also support formatting with formats specified after a colon. Below, we format a long float to round to 2 digits.</p>
<div id="95ac64aa" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">proportion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.123456789</span></span>
<span id="cb15-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"The proportion is </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>proportion<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.2f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The proportion is 0.12</code></pre>
</div>
</div>
<p>Any python expression – not just a single variable – can go in curly braces. So, we can instead format that propotion as a percent.</p>
<div id="7aa11b90" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">proportion <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.123456789</span></span>
<span id="cb17-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"The proportion is </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>proportion<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">:.1f}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">%"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The proportion is 12.3%</code></pre>
</div>
</div>
<p>Despite the slickness of f-strings, sometimes other string interpolation approaches can be useful. For example, if all the variables I want to interpolate are in a dictionary (as often will happen, for example, with REST API responses), the string <code>format()</code> method is a nice alternative. It allows us to pass in the dictionary, “unpacking” the argument with <code>**</code><sup>2</sup></p>
<div id="a11f7294" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">result <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> {</span>
<span id="cb19-2">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dog_name'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Squeak'</span>,</span>
<span id="cb19-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'dog_type'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Chihuahua'</span></span>
<span id="cb19-4">}</span>
<span id="cb19-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{dog_name}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;"> is a </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{dog_type}</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>.<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">format</span>(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">**</span>result))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Squeak is a Chihuahua</code></pre>
</div>
</div>
<section id="application-generating-file-names" class="level3">
<h3 class="anchored" data-anchor-id="application-generating-file-names">Application: Generating File Names</h3>
<p>Combining what we’ve discussed about <code>datetime</code> and f-strings, here’s a pattern I use frequently. If I am logging results from a run of some script, I might save the results in a file suffixed with the run timestamp. We can generate this easily.</p>
<div id="5c40ec95" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">dt_stub <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> datetime.datetime.now().strftime(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'%Y%m</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%d</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">_%H%M%S'</span>)</span>
<span id="cb21-2">file_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"output-</span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>dt_stub<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">.csv"</span></span>
<span id="cb21-3"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(file_name)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>output-20240120_071517.csv</code></pre>
</div>
</div>
</section>
</section>
<section id="repeating-things-iteration-functional-programming" class="level2">
<h2 class="anchored" data-anchor-id="repeating-things-iteration-functional-programming">Repeating Things (Iteration / Functional Programming)</h2>
<p>Thanks in part to a modern-day fiction that <code>for</code> loops in R are inefficient, R users have gravitated towards concise mapping functions for iteration. These can include the <code>*apply()</code> family<sup>3</sup>, <code>purrr</code>’s <code>map_*()</code> functions, or the parallelized version of either.</p>
<p>Python too has a nice pattern for arbitrary iteration in list comprehensions. For any iterable, we can use a list comprehension to make a list of outputs by processing a list of inputs, with optional conditional and default expressions.</p>
<p>Here are some trivial examples:</p>
<div id="2d5c5705" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1">l <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]</span>
<span id="cb23-2">[i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> l]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>[2, 3, 4]</code></pre>
</div>
</div>
<div id="c606ec83" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb25" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1">[i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> l <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>[2, 4]</code></pre>
</div>
</div>
<div id="d9ee13a4" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb27" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1">[i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">if</span> i <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">else</span> i <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> l]</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>[2, 2, 4]</code></pre>
</div>
</div>
<p>There are also closer analogs to <code>purrr</code> like python’s <code>map()</code> function. <code>map()</code> takes a function and an iterable object and applies the function to each element. Like with <code>purrr</code>, functions can be anonymous (as defined in python with lambda functions) or named. List comprehensions are popular for their concise syntax, but there are many different thoughts on the matter as expressed in <a href="https://stackoverflow.com/questions/1247486/list-comprehension-vs-map">this StackOverflow post</a>.</p>
<div id="4647a4b9" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb29" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">def</span> add_one(i): </span>
<span id="cb29-2">  <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">return</span> i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb29-3"></span>
<span id="cb29-4"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># these are the same</span></span>
<span id="cb29-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(<span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">lambda</span> i: i<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, l))</span>
<span id="cb29-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">map</span>(add_one, l))</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>[2, 3, 4]</code></pre>
</div>
</div>
<section id="application-simulation" class="level3">
<h3 class="anchored" data-anchor-id="application-simulation">Application: Simulation</h3>
<p>As a (slightly) more realistic(ish) example, let’s consider how list comprehensions might help us conduct a numerical simulation or sensitivity analysis.</p>
<p>Suppose we want to simulate 100 draws from a Bernoulli distribution with different success probabilites and see how close our empirically calculated rate is to the true rate.</p>
<p>We can define the probabilites we want to simulate in a list and use a list comprehension to run the simulations.</p>
<div id="fc8f8b46" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb31" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb31-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy.random <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> rnd</span>
<span id="cb31-3"></span>
<span id="cb31-4">probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.25</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.75</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.9</span>]</span>
<span id="cb31-5">coin_flips <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [ np.mean(np.random.binomial(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, p, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> p <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> probs ]</span>
<span id="cb31-6">coin_flips</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>[0.05, 0.3, 0.48, 0.77, 0.87]</code></pre>
</div>
</div>
<p>Alternatively, instead of returning a list of the same length, our resulting list could include whatever we want – like a list of lists! If we wanted to keep the raw simulation results, we could. The following code returns a list of 5 lists - one with the raw simulation results.</p>
<div id="12787135" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb33" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1">coin_flips <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [ <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(np.random.binomial(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, p, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> p <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> probs ]</span>
<span id="cb33-2"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"""</span></span>
<span id="cb33-3"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">  coin_flips has </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(coin_flips)<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;"> elements</span></span>
<span id="cb33-4"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">  Each element is itself a </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">type</span>(coin_flips[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb33-5"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">  Each element is of length </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(coin_flips[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>])<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span></span>
<span id="cb33-6"><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">  """</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
  coin_flips has 5 elements
  Each element is itself a &lt;class 'list'&gt;
  Each element is of length 100
  </code></pre>
</div>
</div>
<p>If one wished, they could then put these into a <code>polars</code> dataframe and pivot those list-of-lists (going from a 5-row dataset to a 500-row dataset)to conduct whatever sort of analysis with want with all the replicates.</p>
<div id="3d34880e" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb35" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> polars <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pl</span>
<span id="cb35-2"></span>
<span id="cb35-3">df_flips <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'prob'</span>: probs, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'flip'</span>: coin_flips})</span>
<span id="cb35-4">df_flips.explode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'flip'</span>).glimpse()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 500
Columns: 2
$ prob &lt;f64&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1
$ flip &lt;i32&gt; 0, 0, 0, 0, 1, 0, 1, 1, 0, 0
</code></pre>
</div>
</div>
<p>We’ll return to list comprehensions in the next section.</p>
</section>
</section>
<section id="faking-things-data-generation" class="level2">
<h2 class="anchored" data-anchor-id="faking-things-data-generation">Faking Things (Data Generation)</h2>
<p>Creating simple miniature datasets is often useful in analysis. When working with a new packages, it’s an important part of learning, developing, debugging, and eventually unit testing. We can easily run our code on a simplified data object where the desired outcome is easy to determine to sanity-check our work, or we can use fake data to confirm our understanding of how a program will handle edge cases (like the diversity of ways different programs <a href="../..\post/nulls-polyglot/">handle null values</a>). Simple datasets can also be used and spines and scaffolds for more complex data wrangling tasks (e.g.&nbsp;joining event data onto a date spine).</p>
<p>In R, <code>data.frame()</code> and <code>expand.grid()</code> are go-to functions, coupled with vector generators like <code>rep()</code> and <code>seq()</code>. Python has many similar options.</p>
<section id="fake-datasets" class="level3">
<h3 class="anchored" data-anchor-id="fake-datasets">Fake Datasets</h3>
<p>For the simplest of datasets, we can manually write a few entries as with <code>data.frame()</code> in R. Here, we define series in a named dictionary where each dictionary key turns into a column name.</p>
<div id="dfa6b79a" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb37" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> polars <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pl</span>
<span id="cb37-2"></span>
<span id="cb37-3">pl.DataFrame({</span>
<span id="cb37-4">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>],</span>
<span id="cb37-5">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'y'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'z'</span>]</span>
<span id="cb37-6">})</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (3, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>str</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>"x"</td>
</tr>
<tr class="even">
<td>2</td>
<td>"y"</td>
</tr>
<tr class="odd">
<td>3</td>
<td>"z"</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>If we need longer datasets, we can use helper functions in packages like <code>numpy</code> to generate the series. Methods like <code>arange</code> and <code>linspace</code> work similarly to R’s <code>seq()</code>.</p>
<div id="e27be38e" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb38" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> polars <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pl</span>
<span id="cb38-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb38-3"></span>
<span id="cb38-4">pl.DataFrame({</span>
<span id="cb38-5">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>: np.arange(stop <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>),</span>
<span id="cb38-6">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>: np.linspace(start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>, stop <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">24</span>, num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)</span>
<span id="cb38-7">})</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (3, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
</tr>
<tr class="odd">
<th>i32</th>
<th>f64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>9.0</td>
</tr>
<tr class="even">
<td>1</td>
<td>16.5</td>
</tr>
<tr class="odd">
<td>2</td>
<td>24.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>If we need groups in our sample data, we can use <code>np.repeat()</code> which works like R’s <code>rep(each = TRUE)</code>.</p>
<div id="a86edf0b" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb39" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1">pl.DataFrame({</span>
<span id="cb39-2">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>: np.repeat(np.arange(stop <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>), <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>),</span>
<span id="cb39-3">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>: np.linspace(start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>, stop <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">27</span>, num <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>)</span>
<span id="cb39-4">})</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (6, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
</tr>
<tr class="odd">
<th>i32</th>
<th>f64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>3.0</td>
</tr>
<tr class="even">
<td>0</td>
<td>7.8</td>
</tr>
<tr class="odd">
<td>1</td>
<td>12.6</td>
</tr>
<tr class="even">
<td>1</td>
<td>17.4</td>
</tr>
<tr class="odd">
<td>2</td>
<td>22.2</td>
</tr>
<tr class="even">
<td>2</td>
<td>27.0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>Alternatively, for more control and succinct typing, we can created a nested dataset in <code>polars</code> and explode it out.</p>
<div id="6ecf467f" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb40" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1">(</span>
<span id="cb40-2">  pl.DataFrame({</span>
<span id="cb40-3">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>: [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>],</span>
<span id="cb40-4">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"a b c"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"d e f"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"g h i"</span>]</span>
<span id="cb40-5">  })</span>
<span id="cb40-6">  .with_columns(pl.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">str</span>.split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" "</span>))</span>
<span id="cb40-7">  .explode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>)</span>
<span id="cb40-8">)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (9, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>str</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>"a"</td>
</tr>
<tr class="even">
<td>1</td>
<td>"b"</td>
</tr>
<tr class="odd">
<td>1</td>
<td>"c"</td>
</tr>
<tr class="even">
<td>2</td>
<td>"d"</td>
</tr>
<tr class="odd">
<td>2</td>
<td>"e"</td>
</tr>
<tr class="even">
<td>2</td>
<td>"f"</td>
</tr>
<tr class="odd">
<td>3</td>
<td>"g"</td>
</tr>
<tr class="even">
<td>3</td>
<td>"h"</td>
</tr>
<tr class="odd">
<td>3</td>
<td>"i"</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>Similarly, we could use what we’ve learned about <code>polars</code> list columns <em>and</em> list comprehensions.</p>
<div id="10f32c48" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb41" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1">a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]</span>
<span id="cb41-2">b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [ [q<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span>i <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> q <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>]] <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> a]</span>
<span id="cb41-3">pl.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>:a,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>:b}).explode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (9, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>2</td>
</tr>
<tr class="odd">
<td>1</td>
<td>3</td>
</tr>
<tr class="even">
<td>2</td>
<td>2</td>
</tr>
<tr class="odd">
<td>2</td>
<td>4</td>
</tr>
<tr class="even">
<td>2</td>
<td>6</td>
</tr>
<tr class="odd">
<td>3</td>
<td>3</td>
</tr>
<tr class="even">
<td>3</td>
<td>6</td>
</tr>
<tr class="odd">
<td>3</td>
<td>9</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>In fact, multidimensional list comprehensions can be used to mimic R’s <code>expand.grid()</code> function.</p>
<div id="f6c2c31c" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb42" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1">pl.DataFrame(</span>
<span id="cb42-2">  [(x, y) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">for</span> y <span class="kw" style="color: #003B4F;
background-color: null;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>)],</span>
<span id="cb42-3">  schema <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'x'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'y'</span>]</span>
<span id="cb42-4">  )</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (9, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">x</th>
<th data-quarto-table-cell-role="th">y</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>0</td>
<td>2</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>2</td>
</tr>
<tr class="odd">
<td>2</td>
<td>0</td>
</tr>
<tr class="even">
<td>2</td>
<td>1</td>
</tr>
<tr class="odd">
<td>2</td>
<td>2</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="built-in-data" class="level3">
<h3 class="anchored" data-anchor-id="built-in-data">Built-In Data</h3>
<p>R has a number of canonical datasets like <code>iris</code> built in to the core language. This can be easy to quickly grab for experimentation<sup>4</sup>. While base python doesn’t include such capabilities, many of the exact same or similar datasets can be found in <code>seaborn</code>.</p>
<p><code>seaborn.get_dataset_names()</code> provides the list of available options. Below, we load the Palmers Penguins data and, if you wish, convert it from <code>pandas</code> to <code>polars</code>.</p>
<div id="62759986" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb43" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> seaborn <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> sns</span>
<span id="cb43-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> polars <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pl</span>
<span id="cb43-3"></span>
<span id="cb43-4">df_pd <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> sns.load_dataset(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'penguins'</span>)</span>
<span id="cb43-5">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.from_pandas(df_pd)</span>
<span id="cb43-6">df.glimpse()</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Rows: 344
Columns: 7
$ species           &lt;str&gt; 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie'
$ island            &lt;str&gt; 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen'
$ bill_length_mm    &lt;f64&gt; 39.1, 39.5, 40.3, None, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0
$ bill_depth_mm     &lt;f64&gt; 18.7, 17.4, 18.0, None, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2
$ flipper_length_mm &lt;f64&gt; 181.0, 186.0, 195.0, None, 193.0, 190.0, 181.0, 195.0, 193.0, 190.0
$ body_mass_g       &lt;f64&gt; 3750.0, 3800.0, 3250.0, None, 3450.0, 3650.0, 3625.0, 4675.0, 3475.0, 4250.0
$ sex               &lt;str&gt; 'Male', 'Female', 'Female', None, 'Female', 'Male', 'Female', 'Male', None, None
</code></pre>
</div>
</div>
</section>
</section>
<section id="saving-things-object-serialization" class="level2">
<h2 class="anchored" data-anchor-id="saving-things-object-serialization">Saving Things (Object Serialization)</h2>
<p>Sometimes, it can be useful to save <em>objects</em> as they existed in RAM in an active programming environment. R users may have experienced this if they’ve used <code>.rds</code>, <code>.rda</code>, or <code>.Rdata</code> files to save individual variables or their entire environment. These objects can often be faster to reload than plaintext and can better preserve information that may be lost in other formats (e.g.&nbsp;storing a dataframe in a way that preserves its datatypes versus writing to a CSV file<sup>5</sup> or storing a complex object that can’t be easily reduced to plaintext like a model with training data, hyperparameters, learned tree splits or weights or whatnot for future predictions.) This is called object serializaton<sup>6</sup></p>
<p>Python has comparable capabilities in the <a href="https://docs.python.org/3/library/pickle.html"><code>pickle</code> module</a>. There aren’t really style points here, so I’ve not much to add beyond “this exists” and “read the documentation”. But, at a high level, it looks something like this:</p>
<div id="c0799a02" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb45" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># to write a pickle</span></span>
<span id="cb45-2"><span class="cf" style="color: #003B4F;
background-color: null;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'my-obj.pickle'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'wb'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> handle:</span>
<span id="cb45-3">    pickle.dump(my_object, handle, protocol <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pickle.HIGHEST_PROTOCOL)</span>
<span id="cb45-4"></span>
<span id="cb45-5"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># to read a pickle</span></span>
<span id="cb45-6">my_object <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pickle.load(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'my-obj.pickle'</span>,<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rb'</span>))</span></code></pre></div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I defined this odd scope to help limit the infinite number of workflow topics that could be included like “how to write a function” or “how to source code from another script”↩︎</p></li>
<li id="fn2"><p>This is called “**kwargs” and works a bit like <code>do.call()</code> in base R. You can read more about it <a href="https://www.digitalocean.com/community/tutorials/how-to-use-args-and-kwargs-in-python-3">here</a>.↩︎</p></li>
<li id="fn3"><p>Speaking of non-ergonomic things in R, the <code>*apply()</code> family is notoriously diverse in its number and order of arguments↩︎</p></li>
<li id="fn4"><p>Particularly if you want to set wildly unrealistic expectations for the efficacy of k-means clustering, but I digress↩︎</p></li>
<li id="fn5"><p>And yes, you can and should use Parquet and then my example falls apart – but that’s not the point!↩︎</p></li>
<li id="fn6"><p>And, if you want to go incredibly deep here, check out <a href="https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/">this awesome post</a> by Danielle Navarro.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>rstats</category>
  <category>python</category>
  <category>tutorial</category>
  <guid>https://emilyriederer.com/post/py-rgo-base/</guid>
  <pubDate>Sat, 20 Jan 2024 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/py-rgo-base/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Crosspost: Why You Need Data Documentation in 2024</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/docs-personas/</link>
  <description><![CDATA[ 





<p>We’ve all worked with poorly documented dataset, and we all know it isn’t pretty. However, it’s surprisingly easy for teams to continue to fall into “documentation debt” and deprioritize this foundational work in favor of flashy new projects. These tradeoff discussions may become even more painful in 2024 as teams are continually asked to do more with less.</p>
<p>Recently, I had the opportunity to articulate some of the underappreciated benefits of data documentation in a <a href="https://www.selectstar.com/blog/why-you-need-data-documentation-in-2024">cross-post with Select Star</a>. This builds on my prior post showing that <a href="../..\post/docs-closer-than-you-think/">documentation can be strategically created throughout the data development process</a>. To make the case for taking those “raw” documentation resources to a polished final form, I return to the jobs-to-be-done framework that I’ve previously employed to talk about <a href="../..\post/team-of-packages/">the value of innersource packages</a>. In this perspective, documentation is like hiring an extra resource (or more!) to your team.</p>
<p>Some of the jobs discussed are:</p>
<ul>
<li>Developer Advocacy and Product Evangelism for users
<ul>
<li>Users think data doesn’t exist if they can’t find it, they think data is broken if they misinterpret it</li>
<li>Documentation is both a “user interface” to make data usage easy and a bulwark against confusion and frustration</li>
</ul></li>
<li>Producct and Project Management for developers
<ul>
<li>Data intent can “drift” over time</li>
<li>As teams evolve and collaborate, this risks initial intent getting lost and poluted (after all, what really is a “customer”?)</li>
<li>Documentation serves as a contract and coach for one or more teams to force clarity and consistency of intent</li>
</ul></li>
<li>Chief of Staff oversight for data leaders
<ul>
<li>Leaders face increasing demands in data governance: navigating changing privacy regulations, fighting decaying data quality, and discerning their next strategic investments</li>
<li>Documentation is their command center to understand what data assets exists and where to better spot risks and opportunities</li>
</ul></li>
</ul>
<p>If you or your team works on data documentation, I’d love to hear what other “jobs” you have found that data documentation performs in your organization.</p>



 ]]></description>
  <category>data</category>
  <category>workflow</category>
  <category>elt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/docs-personas/</guid>
  <pubDate>Mon, 15 Jan 2024 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/docs-personas/featured.PNG" medium="image"/>
</item>
<item>
  <title>polars’ Rgonomic Patterns</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/py-rgo-polars/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/py-rgo-polars/featured.jpg" class="img-fluid figure-img"></p>
<figcaption>Photo credit to <a href="https://unsplash.com/@hansjurgen007">Hans-Jurgen Mager</a> on Unsplash</figcaption>
</figure>
</div>
<p>A few weeks ago, I shared some <a href="../..\post/py-rgo/">recommended modern python tools and libraries</a> that I believe have the most similar ergonomics for R (specifically <code>tidyverse</code>) converts. This post expands on that one with a focus on the <code>polars</code> library.</p>
<p>At the surface level, all data wrangling libraries have roughly the same functionality. Operations like selecting existing columns and making new ones, subsetting and ordering rows, and summarzing results is tablestakes.</p>
<p>However, no one falls in love with a specific library because it has the best <code>select()</code> or <code>filter()</code> function the world has ever seen. It’s the ability to easily do more complex transformations that differentiate a package expert versus novice, and the learning curve for everything that happens <em>after</em> the “Getting Started” guide ends is what can leave experts at one tool feeling so disempowered when working with another.</p>
<p>This deeper sense of intuition and fluency – when your technical brain knows intuitively how to translate in code what your analytical brain wants to see in the data – is what I aim to capture in the term “ergonomics”. In this post, I briefly discuss the surface-level comparison but spend most of the time exploring the deeper similarities in the functionality and workflows enabled by <code>polars</code> and <code>dplyr</code>.</p>
<section id="what-are-dplyrs-ergonomics" class="level2">
<h2 class="anchored" data-anchor-id="what-are-dplyrs-ergonomics">What are <code>dplyr</code>’s ergonomics?</h2>
<p>To claim <code>polars</code> has a similar aesthetic and user experience as <code>dplyr</code>, we first have to consider what the heart of <code>dplyr</code>‘s ergonomics actually is. The explicit design philosophy is described in the developers’ writings on <a href="https://design.tidyverse.org/unifying.html">tidy design principles</a>, but I’ll blend those official intended principles with my personal definitions based on the lived user experience.</p>
<ul>
<li>Consistent:
<ul>
<li>Function names are highly consistent (e.g.&nbsp;snake case verbs) with dependable inputs and outputs (mostly dataframe-in dataframe-out) to increase intuition, reduce mistakes, and eliminate surprises</li>
<li>Metaphors extend throughout the codebase. For example <code>group_by()</code> + <code>summarize()</code> or <code>group_by()</code> + <code>mutate()</code> do what one might expect (aggregation versus a window function) instead of requiring users to remember arbitrary command-specific syntax</li>
<li>Always returns a new dataframe versus modifying in-place so code is more idempotent<sup>1</sup> and less error prone</li>
</ul></li>
<li>Composable:
<ul>
<li>Functions exist at a “sweet spot” level of abstraction. We have the right primitive building blocks that users have full control to do anything they want to do with a dataframe but almost never have to write brute-force glue code. These building blocks can be layered however one choose to conduct</li>
<li>Conistency of return types leads to composability since dataframe-in dataframe-out allows for chaining</li>
</ul></li>
<li>Human-Centered:
<ul>
<li>Packages hit a comfortable level of abstraction somewhere between fully procedural (e.g.&nbsp;manually looping over array indexes without a dataframe abstraction) and fully declarative (e.g.&nbsp;SQL-style languages where you “request” the output but aspects like the order of operations may become unclear). Writing code is essentially articulating the steps of an analysis</li>
<li>This focus on code as recipe writing leads to the creation of useful optional functions and helpers (like my favorite – column selectors)</li>
<li>User’s rarely need to break the fourth wall of this abstraction-layer (versus thinking about things like indexes in <code>pandas</code>)</li>
</ul></li>
</ul>
<p>TLDR? We’ll say <code>dplyr</code>’s ergonomics allow users to express complex transformation precisely, concisely, and expressively.</p>
<p>So, with that, we will import <code>polars</code> and get started!</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> polars <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pl</span></code></pre></div>
</div>
<p>This document was made with <code>polars</code> version <code>0.20.4</code>.</p>
</section>
<section id="basic-functionality" class="level2">
<h2 class="anchored" data-anchor-id="basic-functionality">Basic Functionality</h2>
<p>The similarities between <code>polars</code> and <code>dplyr</code>’s top-level API are already well-explored in many posts, including those by <a href="https://blog.tidy-intelligence.com/posts/dplyr-vs-polars/">Tidy Intelligence</a> and <a href="https://robertmitchellv.com/blog/2022-07-r-python-side-by-side/r-python-side-by-side.html">Robert Mitchell</a>.</p>
<p>We will only do the briefest of recaps of the core data wrangling functions of each and how they can be composed in order to make the latter half of the piece make sense. We will meet these functions again in-context when discussing <code>dplyr</code> and <code>polar</code>’s more advanced workflows.</p>
<section id="main-verbs" class="level3">
<h3 class="anchored" data-anchor-id="main-verbs">Main Verbs</h3>
<p><code>dplyr</code> and <code>polars</code> offer the same foundational functionality for manipulating dataframes. Their APIs for these operations are substantially similar.</p>
<p>For a single dataset:</p>
<ul>
<li>Column selection: <code>select()</code> -&gt; <code>select()</code> + <code>drop()</code></li>
<li>Creating or altering columns: <code>mutate()</code> -&gt; <code>with_columns()</code></li>
<li>Subsetting rows: <code>filter()</code> -&gt; <code>filter()</code></li>
<li>Ordering rows: <code>arrange()</code> -&gt; <code>sort()</code></li>
<li>Computing group-level summary metrics: <code>group_by()</code> + <code>summarize()</code> -&gt; <code>group_by()</code> + <code>agg()</code></li>
</ul>
<p>For multiple datasets:</p>
<ul>
<li>Merging on a shared key: <code>*_join()</code> -&gt; <code>join(strategy = '*')</code></li>
<li>Stacking datasets of the same structure: <code>union()</code> -&gt; <code>concat()</code></li>
<li>Transforming rows and columns: <code>pivot_{longer/wider}()</code><sup>2</sup> -&gt; <code>pivot()</code></li>
</ul>
</section>
<section id="main-verb-design" class="level3">
<h3 class="anchored" data-anchor-id="main-verb-design">Main Verb Design</h3>
<p>Beyond the similarity in naming, <code>dplyr</code> and <code>polars</code> top-level functions are substantially similar in their deeper design choices which impact the ergonomics of use:</p>
<ul>
<li>Referencing columns: Both make it easy to concisely references columns in a dataset without the repeated and redundant references to said dataset (as sometimes occurs in base R or python’s <code>pandas</code>). dplyr does this through nonstandard evaluation wherein a dataframe’s coumns can be reference directly within a data transformation function as if they were top-level variables; in <code>polars</code>, column names are wrapped in <code>pl.col()</code></li>
<li>Optional argument: Both tend to have a wide array of nice-to-have optional arguments. For example the joining capabilities in both libraries offer optional join validation<sup>3</sup> and column renaming by appended suffix</li>
<li>Consistent dataframe-in -&gt; dataframe-out design: <code>dplyr</code> functions take a dataframe as their first argument and return a dataframe. Similarly, <code>polars</code> methods are called on a dataframe and return a dataframe which enables the chaining workflow discussed next</li>
</ul>
</section>
<section id="chaining-piping" class="level3">
<h3 class="anchored" data-anchor-id="chaining-piping">Chaining (Piping)</h3>
<p>These methods are applied to <code>polars</code> dataframes by <em>chaining</em> which should feel very familiar to R <code>dplyr</code> fans.</p>
<p>In <code>dplyr</code> and the broad <code>tidyverse</code>, most functions take a dataframe as their first argument and return a dataframe, enabling the piping of functions. This makes it easy to write more human-readable scripts where functions are written in the order of execution and whitespace can easily be added between lines. The following lines would all be equivalent.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">transformation2</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">transformation1</span>(df))</span>
<span id="cb2-2"></span>
<span id="cb2-3">df <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|&gt;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">transformation1</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|&gt;</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">transformation2</span>()</span>
<span id="cb2-4"></span>
<span id="cb2-5">df <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|&gt;</span></span>
<span id="cb2-6">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">transformation1</span>() <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|&gt;</span></span>
<span id="cb2-7">  <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">transformation2</span>()</span></code></pre></div>
</div>
<p>Similarly, <code>polars</code>’s main transfomration methods offer a consistent dataframe-in dataframe-out design which allows <em>method chaining</em>. Here, we similarly can write commands in order where the <code>.</code> beginning the next method call serves the same purpose as R’s pipe. And for python broadly, to achieve the same affordance for whitespace, we can wrap the entire command in parentheses.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">(</span>
<span id="cb3-2">  df</span>
<span id="cb3-3">  .transformation1()</span>
<span id="cb3-4">  .transformation2()</span>
<span id="cb3-5">)</span></code></pre></div>
</div>
<p>One could even say that <code>polars</code> dedication to chaining goes even deeper than <code>dplyr</code>. In <code>dplyr</code>, while core dataframe-level functions are piped, functions on specific columns are still often written in a nested fashion<sup>4</sup></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">df <span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%&gt;%</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">mutate</span>(<span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">z =</span> <span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">g</span>(<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">f</span>(a)))</span></code></pre></div>
</div>
<p>In contrast, most of <code>polars</code> column-level transformation methods also make it ergonomic to keep the same literate left-to-right chaining within column-level definitions with the same benefits to readability as for dataframe-level operations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">df.with_columns(z <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>).f().g())</span></code></pre></div>
</div>
</section>
</section>
<section id="advanced-wrangling" class="level2">
<h2 class="anchored" data-anchor-id="advanced-wrangling">Advanced Wrangling</h2>
<p>Beyond the surface-level similarity, <code>polars</code> supports some of the more complex ergonomics that <code>dplyr</code> users may enjoy. This includes functionality like:</p>
<ul>
<li>expressive and explicit syntax for transformations across multiple rows</li>
<li>concise helpers to identify subsets of columns and apply transformations</li>
<li>consistent syntax for window functions within data transformation operations</li>
<li>the ability to work with nested data structures</li>
</ul>
<p>Below, we will examine some of this functionality with a trusty fake dataframe.<sup>5</sup> As with <code>pandas</code>, you can make a quick dataframe in <code>polars</code> by passing a dictionary to <code>pl.DataFrame()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> polars <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pl </span>
<span id="cb6-2"></span>
<span id="cb6-3">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.DataFrame({<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>:[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>], </span>
<span id="cb6-4">                   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>:[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">5</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">6</span>], </span>
<span id="cb6-5">                   <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'c'</span>:[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">7</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">8</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">9</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>]})</span>
<span id="cb6-6">df.head()</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>3</td>
<td>7</td>
</tr>
<tr class="even">
<td>1</td>
<td>4</td>
<td>8</td>
</tr>
<tr class="odd">
<td>2</td>
<td>5</td>
<td>9</td>
</tr>
<tr class="even">
<td>2</td>
<td>6</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<section id="explicit-api-for-row-wise-operations" class="level3">
<h3 class="anchored" data-anchor-id="explicit-api-for-row-wise-operations">Explicit API for row-wise operations</h3>
<p>While row-wise operations are relatively easy to write ad-hoc, it can still be nice semantically to have readable and stylistically consistent code for such transformations.</p>
<p><code>dplyr</code>’s <a href="https://dplyr.tidyverse.org/articles/rowwise.html"><code>rowwise()</code></a> eliminates ambiguity in whether subsequent functions should be applied element-wise or collectively. Similiarly, <code>polars</code> has explicit <code>*_horizontal()</code> functions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1">df.with_columns(</span>
<span id="cb7-2">  b_plus_c <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.sum_horizontal(pl.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>), pl.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'c'</span>)) </span>
<span id="cb7-3">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 4)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
<th data-quarto-table-cell-role="th">b_plus_c</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>i64</th>
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>3</td>
<td>7</td>
<td>10</td>
</tr>
<tr class="even">
<td>1</td>
<td>4</td>
<td>8</td>
<td>12</td>
</tr>
<tr class="odd">
<td>2</td>
<td>5</td>
<td>9</td>
<td>14</td>
</tr>
<tr class="even">
<td>2</td>
<td>6</td>
<td>0</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="column-selectors" class="level3">
<h3 class="anchored" data-anchor-id="column-selectors">Column Selectors</h3>
<p><code>dplyr</code>’s <a href="https://dplyr.tidyverse.org/reference/select.html">column selectors</a> dynamically determine a set of columns based on pattern-matching their names (e.g.&nbsp;<code>starts_with()</code>, <code>ends_with()</code>), data types, or other features. I’ve previously <a href="../..\post/column-name-contracts/">written</a> and <a href="../..\talk/col-names-contract/">spoken</a> at length about how transformative this functionality can be when paired with</p>
<p><code>polars</code> has a similar set of <a href="https://docs.pola.rs/py-polars/html/reference/selectors.html">column selectors</a>. We’ll import them and see a few examples.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> polars.selectors <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> cs</span></code></pre></div>
</div>
<p>To make things more interesting, we’ll also turn one of our columns into a different data type.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.with_columns(pl.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>).cast(pl.Utf8))</span></code></pre></div>
</div>
<section id="in-select" class="level4">
<h4 class="anchored" data-anchor-id="in-select">In <code>select</code></h4>
<p>We can select columns based on name or data type and use one or more conditions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">df.select(cs.starts_with(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">|</span> cs.string())</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">a</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>str</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3</td>
<td>"1"</td>
</tr>
<tr class="even">
<td>4</td>
<td>"1"</td>
</tr>
<tr class="odd">
<td>5</td>
<td>"2"</td>
</tr>
<tr class="even">
<td>6</td>
<td>"2"</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>Negative conditions also work.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">df.select(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>cs.string())</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3</td>
<td>7</td>
</tr>
<tr class="even">
<td>4</td>
<td>8</td>
</tr>
<tr class="odd">
<td>5</td>
<td>9</td>
</tr>
<tr class="even">
<td>6</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="in-with_columns" class="level4">
<h4 class="anchored" data-anchor-id="in-with_columns">In <code>with_columns</code></h4>
<p>Column selectors can play multiple rows in the transformation context.</p>
<p>The same transformation can be applied to multiple columns. Below, we find all integer variables, call a method to add 1 to each, and use the <code>name.suffix()</code> method to dynamically generate descriptive column names.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">df.with_columns(</span>
<span id="cb12-2">  cs.integer().add(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>).name.suffix(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"_plus1"</span>)</span>
<span id="cb12-3">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 5)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
<th data-quarto-table-cell-role="th">b_plus1</th>
<th data-quarto-table-cell-role="th">c_plus1</th>
</tr>
<tr class="odd">
<th>str</th>
<th>i64</th>
<th>i64</th>
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"1"</td>
<td>3</td>
<td>7</td>
<td>4</td>
<td>8</td>
</tr>
<tr class="even">
<td>"1"</td>
<td>4</td>
<td>8</td>
<td>5</td>
<td>9</td>
</tr>
<tr class="odd">
<td>"2"</td>
<td>5</td>
<td>9</td>
<td>6</td>
<td>10</td>
</tr>
<tr class="even">
<td>"2"</td>
<td>6</td>
<td>0</td>
<td>7</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>We can also use selected variables within transformations, like the rowwise sums that we just saw earlier.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1">df.with_columns(</span>
<span id="cb13-2">  row_total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.sum_horizontal(cs.integer())</span>
<span id="cb13-3">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 4)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
<th data-quarto-table-cell-role="th">row_total</th>
</tr>
<tr class="odd">
<th>str</th>
<th>i64</th>
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"1"</td>
<td>3</td>
<td>7</td>
<td>10</td>
</tr>
<tr class="even">
<td>"1"</td>
<td>4</td>
<td>8</td>
<td>12</td>
</tr>
<tr class="odd">
<td>"2"</td>
<td>5</td>
<td>9</td>
<td>14</td>
</tr>
<tr class="even">
<td>"2"</td>
<td>6</td>
<td>0</td>
<td>6</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="in-group_by-and-agg" class="level4">
<h4 class="anchored" data-anchor-id="in-group_by-and-agg">In <code>group_by</code> and <code>agg</code></h4>
<p>Column selectors can also be passed as inputs anywhere else that one or more columns is accepted, as with data aggregation.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">df.group_by(cs.string()).agg(cs.integer().<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>())</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 3)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
</tr>
<tr class="odd">
<th>str</th>
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"1"</td>
<td>7</td>
<td>15</td>
</tr>
<tr class="even">
<td>"2"</td>
<td>11</td>
<td>9</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="consistent-api-for-window-functions" class="level3">
<h3 class="anchored" data-anchor-id="consistent-api-for-window-functions">Consistent API for Window Functions</h3>
<p>Window functions are another incredibly important tool in any data wrangling language but seem criminally undertaught in introductory analysis classes. Window functions allows you to apply aggregation <em>logic</em> over subgroups of data while preserving the original <em>grain</em> of the data (e.g.&nbsp;in a table of all customers and orders and a column for the max purchase account by customer).</p>
<p><code>dplyr</code> make window functions trivially easy with the <code>group_by()</code> + <code>mutate()</code> pattern, invoking users’ pre-existing understanding of how to write aggregation logic and how to invoke transformations that preserve a table’s grain.</p>
<p><code>polars</code> takes a slightly different but elegant approach. Similarly, it reuses the core <code>with_columns()</code> method for window functions. However, it uses a more SQL-reminiscent specification of the “window” in the column definition versus a separate grouping statement. This has the added advantage of allowing one to use multiple window functions with different windows in the same <code>with_columns()</code> call if you should so choose.</p>
<p>A simple window function tranformation can be done by calling <code>with_columns()</code>, chaining an aggregation method onto a column, and following with the <code>over()</code> method to define the window of interest.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1">df.with_columns(</span>
<span id="cb15-2">  min_b <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">min</span>().over(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>)</span>
<span id="cb15-3">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 4)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
<th data-quarto-table-cell-role="th">min_b</th>
</tr>
<tr class="odd">
<th>str</th>
<th>i64</th>
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"1"</td>
<td>3</td>
<td>7</td>
<td>3</td>
</tr>
<tr class="even">
<td>"1"</td>
<td>4</td>
<td>8</td>
<td>3</td>
</tr>
<tr class="odd">
<td>"2"</td>
<td>5</td>
<td>9</td>
<td>5</td>
</tr>
<tr class="even">
<td>"2"</td>
<td>6</td>
<td>0</td>
<td>5</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>The chaining over and aggregate and <code>over()</code> can follow any other arbitrarily complex logic. Here, it follows a basic “case when”-type statement that creates an indicator for whether column b is null.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">df.with_columns(</span>
<span id="cb16-2">  n_b_odd <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.when( (pl.col(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'b'</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb16-3">              .then(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb16-4">              .otherwise(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb16-5">              .<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">sum</span>().over(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>)</span>
<span id="cb16-6">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 4)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
<th data-quarto-table-cell-role="th">n_b_odd</th>
</tr>
<tr class="odd">
<th>str</th>
<th>i64</th>
<th>i64</th>
<th>i32</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"1"</td>
<td>3</td>
<td>7</td>
<td>1</td>
</tr>
<tr class="even">
<td>"1"</td>
<td>4</td>
<td>8</td>
<td>1</td>
</tr>
<tr class="odd">
<td>"2"</td>
<td>5</td>
<td>9</td>
<td>1</td>
</tr>
<tr class="even">
<td>"2"</td>
<td>6</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="list-columns-and-nested-frames" class="level3">
<h3 class="anchored" data-anchor-id="list-columns-and-nested-frames">List Columns and Nested Frames</h3>
<p>While the R <code>tidyverse</code>’s raison d’etre was originally around the design of heavily normalize <a href="https://vita.had.co.nz/papers/tidy-data.pdf">tidy data</a>, modern data and analysis sometimes benefits from more complex and hierarchical data structures. Sometimes data comes to us in nested forms, like from an API<sup>6</sup>, and other times nesting data can help us perform analysis more effectively<sup>7</sup> Recognizing these use cases, <code>tidyr</code> provides many capability for the creation and manipulation of <a href="https://tidyr.tidyverse.org/articles/nest.html">nested data</a> in which a single cell contains values from multiple columns or sometimes even a whoel miniature dataframe.</p>
<p><code>polars</code> makes these operations similarly easy with its own version of structs (list columns) and arrays (nested dataframes).</p>
<section id="list-columns-nested-frames" class="level4">
<h4 class="anchored" data-anchor-id="list-columns-nested-frames">List Columns &amp; Nested Frames</h4>
<p>List columns that contain multiple key-value pairs (e.g.&nbsp;column-value) in a single column can be created with <code>pl.struct()</code> similar to R’s <code>list()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">df.with_columns(list_col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.struct( cs.integer() ))</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 4)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
<th data-quarto-table-cell-role="th">list_col</th>
</tr>
<tr class="odd">
<th>str</th>
<th>i64</th>
<th>i64</th>
<th>struct[2]</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"1"</td>
<td>3</td>
<td>7</td>
<td>{3,7}</td>
</tr>
<tr class="even">
<td>"1"</td>
<td>4</td>
<td>8</td>
<td>{4,8}</td>
</tr>
<tr class="odd">
<td>"2"</td>
<td>5</td>
<td>9</td>
<td>{5,9}</td>
</tr>
<tr class="even">
<td>"2"</td>
<td>6</td>
<td>0</td>
<td>{6,0}</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>These structs can be further be aggregated across rows into miniature datasets.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1">df.group_by(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>).agg(list_col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.struct( cs.integer() ) )</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">list_col</th>
</tr>
<tr class="odd">
<th>str</th>
<th>list[struct[2]]</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"2"</td>
<td>[{5,9}, {6,0}]</td>
</tr>
<tr class="even">
<td>"1"</td>
<td>[{3,7}, {4,8}]</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>In fact, this could be a good use case for our column selectors! If we have many columns we want to keep unnested and many we want to next, it could be efficient to list out only the grouping variables and create our nested dataset by examining matches.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">cols <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>]</span>
<span id="cb19-2">(df</span>
<span id="cb19-3">  .group_by(cs.by_name(cols))</span>
<span id="cb19-4">  .agg(list_col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.struct(<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">~</span>cs.by_name(cols)))</span>
<span id="cb19-5">)</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (2, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">list_col</th>
</tr>
<tr class="odd">
<th>str</th>
<th>list[struct[2]]</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"2"</td>
<td>[{5,9}, {6,0}]</td>
</tr>
<tr class="even">
<td>"1"</td>
<td>[{3,7}, {4,8}]</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
</section>
<section id="undoing" class="level4">
<h4 class="anchored" data-anchor-id="undoing">Undoing</h4>
<p>Just as we constructed our nested data, we can denormalize it and return it to the original state in two steps. To see this, we can assign the nested structure above as <code>df_nested</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1">df_nested <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> df.group_by(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'a'</span>).agg(list_col <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pl.struct( cs.integer() ) )</span></code></pre></div>
</div>
<p>First <code>explode()</code> returns the table to the original grain, leaving use with a single struct in each row.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1">df_nested.explode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'list_col'</span>)</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 2)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">list_col</th>
</tr>
<tr class="odd">
<th>str</th>
<th>struct[2]</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"1"</td>
<td>{3,7}</td>
</tr>
<tr class="even">
<td>"1"</td>
<td>{4,8}</td>
</tr>
<tr class="odd">
<td>"2"</td>
<td>{5,9}</td>
</tr>
<tr class="even">
<td>"2"</td>
<td>{6,0}</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>
<p>Then, <code>unnest()</code> unpacks each struct and turns each element back into a column.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1">df_nested.explode(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'list_col'</span>).unnest(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'list_col'</span>)</span></code></pre></div>
<div class="cell-output-display">
<div>
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (4, 3)</small>
<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">a</th>
<th data-quarto-table-cell-role="th">b</th>
<th data-quarto-table-cell-role="th">c</th>
</tr>
<tr class="odd">
<th>str</th>
<th>i64</th>
<th>i64</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>"1"</td>
<td>3</td>
<td>7</td>
</tr>
<tr class="even">
<td>"1"</td>
<td>4</td>
<td>8</td>
</tr>
<tr class="odd">
<td>"2"</td>
<td>5</td>
<td>9</td>
</tr>
<tr class="even">
<td>"2"</td>
<td>6</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Meaning you can’t get the same result twice because if you rerun the same code the input has already been modified↩︎</p></li>
<li id="fn2"><p>Of the <code>tidyverse</code> funtions mentioned so far, this is the only one found in <code>tidyr</code> not <code>dplyr</code>↩︎</p></li>
<li id="fn3"><p>That is, validating an assumption that joins should have been one-to-one, one-to-many, etc.↩︎</p></li>
<li id="fn4"><p>However, this is more by convention. There’s not a strong reason why they would strictly need to be.↩︎</p></li>
<li id="fn5"><p>I recently ran a <a href="https://twitter.com/EmilyRiederer/status/1744707632886095998">Twitter poll</a> on whether people prefer real, canonical, or fake datasets for learning and teaching. Fake data wasn’t the winner, but a strategy I find personally fun and useful as the unit-test analog for learning.↩︎</p></li>
<li id="fn6"><p>For example, an API payload for a LinkedIn user might have nested data structures representing professional experience and educational experience↩︎</p></li>
<li id="fn7"><p>For example, training a model on different data subsets.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>rstats</category>
  <category>python</category>
  <category>tutorial</category>
  <guid>https://emilyriederer.com/post/py-rgo-polars/</guid>
  <pubDate>Sat, 13 Jan 2024 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/py-rgo-polars/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Crosspost: Why you’re closer to data documentation than you think</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/docs-closer-than-you-think/</link>
  <description><![CDATA[ 





<p>Documentation can be a make-or-break for the success of a data initiative, but it’s too often considered an optional nice-to-have. I’m a big believer that writing is thinking. Similarly, documenting is planning, executing, and validating.</p>
<p>Previously, I’ve explored how <a href="https://emilyriederer.netlify.app/post/latent-lasting-documentation/">we can create latent and lasting documentation</a> of data products and how <a href="https://emilyriederer.netlify.app/post/column-name-contracts/">column names can be self documenting</a>.</p>
<p>Recently, I had the opportunity to expand on these ideas in a <a href="https://www.selectstar.com/blog/why-youre-closer-to-data-documentation-than-you-think">cross-post with Select Star</a>. I argue that teams can produce high-quality and maintainable documentation with low overhead with a form of “documentation-driven development”. That is, smartly structuring and re-using artifacts from the development process into long-term documentation. For example:</p>
<ul>
<li>At the planning stage:
<ul>
<li>Structuring requirements docs in the form of data dictionaries</li>
<li>Creating early alignment on higher-order concepts like entity definitions (and <em>writing them down</em>)</li>
<li>Mentally beta testing data usability with an entity-relationship diagram</li>
</ul></li>
<li>At the development stage:
<ul>
<li>Ensuring relevant parts of internal “development documentation” (e.g.&nbsp;dbt column definitions, docstrings) are published to a format and location accessible to users</li>
<li>With different information but similar motivation to ER diagrams, sharing the full orchestration DAG to help users trace column-level lineage and internalize how each field maps to a real-world data generating process</li>
<li>Sharing data tests being executed (the “user contract”) and their results</li>
</ul></li>
<li>Throughout the lifecycle:
<ul>
<li>Answering questions “in public” (e.g.&nbsp;Slack versus email) to create a searchable collection of insights</li>
<li>Producing table usage statistics to help large, decentralized orgs capture the “wisdom of the crowds”</li>
</ul></li>
</ul>
<p>If you or your team works on data documentation, I’d love to hear what other patterns you’ve found to collect useful documentation assets during a data development process.</p>



 ]]></description>
  <category>data</category>
  <category>workflow</category>
  <category>elt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/docs-closer-than-you-think/</guid>
  <pubDate>Fri, 05 Jan 2024 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/docs-closer-than-you-think/featured.PNG" medium="image"/>
</item>
<item>
  <title>Python Rgonomics</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/py-rgo/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/py-rgo/featured.jpg" class="img-fluid figure-img"></p>
<figcaption>Photo credit to the inimitable <a href="https://allisonhorst.com/">Allison Horst</a></figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Some advice in this post has gone stale regarding IDEs, installers, and environment management tools. Please see me <a href="post/py-rgo-2025">2025 update</a> for more recent thoughts following the release of <code>uv</code> and <code>Positron</code></p>
</div>
</div>
<p>Interoperability was a key theme in open-source data languages in 2023. Ongoing innovations in <a href="https://arrow.apache.org/">Arrow</a> (a language-agnostic in-memory standard for data storage), growing adoption of <a href="https://quarto.org/">Quarto</a> (the language-agnostic heir apparent to R Markdown), and even pandas creator Wes McKinney <a href="https://posit.co/blog/welcome-wes/">joining Posit</a> (the language-agnostic rebranding of RStudio) all illustrate the ongoing investment in breaking down barriers between different programming languages and paradigms.</p>
<p>Despite these advances in <em>technical</em> interoperability, individual developers will always face more friction than state-of-the-art tools when moving between languages. Learning a new language is easily enough done; programming 101 concepts like truth tables and control flow translate seamlessly. But ergonomics of a language do not. The tips and tricks we learn to be hyper productive in a primary language are comfortable, familiar, elegant, and effective. They just <em>feel</em> good. Working in a new language, developers often face a choice between forcing their favored workflows into a new tool where they may not “fit”, writing technically correct yet plodding code to get the job done, or approaching a new language as a true beginner to learn it’s “feel” from the ground up.</p>
<p>Fortunately, some of these higher-level paradigms have begun to bleed across languages, enriching previously isolated tribes with the and enabling developers to take their advanced skillsets with them across languages. For any R users who aim to upskill in python in 2024, recent tools and versions of old favorites have made strides in converging the R and python data science stacks. In this post, I will overview some recommended tools that are both truly pythonic while capturing the comfort and familiarity of some favorite R packages of the <code>tidyverse</code> variety.<sup>1</sup></p>
<section id="what-this-post-is-not" class="level2">
<h2 class="anchored" data-anchor-id="what-this-post-is-not">What this post is not</h2>
<p>Just to be clear:</p>
<ul>
<li>This is not a post about why python is better than R so R users should switch all their work to python</li>
<li>This is not a post about why R is better than python so R semantics and conventions should be forced into python</li>
<li>This is not a post about why python <em>users</em> are better than R users so R users need coddling</li>
<li>This is not a post about why R <em>users</em> are better than python users and have superior tastes for their toolkit</li>
<li>This is not a post about why these python tools are the only good tools and others are bad tools</li>
</ul>
<p>If you told me you liked the New York’s Museum of Metropolitan Art, I might say that you might also like Chicago’s Art Institute. That doesn’t mean you should only go to the museum in Chicago or that you should never go to the Louvre in Paris. That’s not how recommendations (by human or recsys) work. This is an “opinionated” post in the sense that “I like this” and not opinionated in the sense that “you must do this”.</p>
</section>
<section id="on-picking-tools" class="level2">
<h2 class="anchored" data-anchor-id="on-picking-tools">On picking tools</h2>
<p>The tools I highlight below tend to have two competing features:</p>
<ul>
<li>They have aspects of their workflow and ergonomics that should feel very comfortable to users of favored R tools</li>
<li>They should be independently accepted, successful, and well-maintained python projects with the true pythonic spirit</li>
</ul>
<p>The former is important because otherwise there’s nothing tailored about these recommendations; the latter is important so users actually engage with the python language and community instead of dabbling around in its more peripheral edges. In short, these two principles <em>exclude</em> tools that are direct ports between languages with that as their sole or main benefit.<sup>2</sup></p>
<p>For example, <code>siuba</code> and <code>plotnine</code> were written with the direct intent of mirroring R syntax. They have seen some success and adoption, but more niche tools come with liabilities. With smaller user-bases, they tend to lack in the pace of development, community support, prior art, StackOverflow questions, blog posts, conference talks, discussions, others to collaborate with, cache in a portfolio, etc. Instead of enjoying the ergonomics of an old language or embracing the challenge of learning a new one, ports can sometimes force developers to invest energy into a “secret third thing” of learning tools that isolate them from both communities and facing inevitable snags by themselves.</p>
<p>When in Rome, do as the Romans do – but if you’re coming from the U.S. that doesn’t mean you can’t bring a universal adapter that can help charge your devices in European outlets.</p>
</section>
<section id="the-stack" class="level2">
<h2 class="anchored" data-anchor-id="the-stack">The stack</h2>
<p>WIth that preamble out of the way, below are a few recommendations for the most ergonomic tools for getting set up, conducting core data analysis, and communication results.</p>
<p>To preview these recommendations:</p>
<p><strong>Set Up</strong></p>
<ul>
<li>Installation: <a href="https://github.com/pyenv/pyenv"><code>pyenv</code></a></li>
<li>IDE: <a href="https://code.visualstudio.com/docs/languages/python">VS Code</a></li>
</ul>
<p><strong>Analysis</strong></p>
<ul>
<li>Wrangling: <a href="https://pola.rs/"><code>polars</code></a></li>
<li>Visualization: <a href="https://seaborn.pydata.org/"><code>seaborn</code></a></li>
</ul>
<p><strong>Communication</strong></p>
<ul>
<li>Tables: <a href="https://posit-dev.github.io/great-tables/articles/intro.html">Great Tables</a></li>
<li>Notebooks: <a href="https://quarto.org/">Quarto</a></li>
</ul>
<p><strong>Miscellaneous</strong></p>
<ul>
<li>Environment Management: <a href="https://pdm-project.org/latest/"><code>pdm</code></a></li>
<li>Code Quality: <a href="https://docs.astral.sh/ruff/"><code>ruff</code></a></li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>I don’t want this advice to set up users for a potential snag. If you are on Windows and install python with <code>pyenv-win</code>, Quarto (as of writing on v1.3) may struggle to find the correct executable. Better support for this is on the backlog, but if you run into this issue, checkout this <a href="https://github.com/quarto-dev/quarto-cli/issues/3500#issuecomment-1375334561">brilliant fix</a>.</p>
</div>
</div>
<section id="for-setting-up" class="level3">
<h3 class="anchored" data-anchor-id="for-setting-up">For setting up</h3>
<p>The first hurdle is often getting started – both in terms of installing the tools you’ll need and getting into a comfortable IDE to run them.</p>
<ul>
<li><strong>Installation</strong>: R keeps installation simple; there’s one way to do it* so you do and it’s done. But before python converts can <code>print("hello world")</code>, they face a range of options (system Python, Python installer UI, Anaconda, Miniconda, etc.) each with its own kinks. These decisions are made harder in Python since projects tend to have stronger dependencies of the language, requiring one to switch between versions. For both of these reasons, I favor the <a href="https://github.com/pyenv/pyenv"><code>pyenv</code></a> (or <code>pyenv-win</code> for those on Windows) for easily managing python installation(s) from the command line. While the installation process of <code>pyenv</code> may be <em>technically</em> different, it’s similar in that it “just works” with just a few commands. In fact, the workflow is <em>so slick</em> that things seem to have gone 180 degrees with <code>pyenv</code> inspiring <a href="https://github.com/r-lib/rig">similar project called <code>rig</code> to manage R installations</a>. This may sound intimidating, but the learning curve is actually quite shallow:
<ul>
<li><code>pyenv install --list</code>: To see what python versions are available to install</li>
<li><code>pyenv install &lt;version number&gt;</code>: To install a specific version</li>
<li><code>pyenv versions</code>: To see what python versions are installed on your system</li>
<li><code>pyenv global &lt;version number&gt;</code>: The set one python version as a global default</li>
<li><code>pyenv local &lt;version number&gt;</code>: The set a python version to be used within a specific directory/project</li>
</ul></li>
<li><strong>Integrated Development Environment</strong>: Once R is install, R users are typically off to the races with the intuitive RStudio IDE which helps them get immediately hands-on with the REPL. With the UI divided into quadrants, users can write an R script, run it to see results in the console, conceptualize what the program “knows” with the variable explorer, and navigate files through a file explorer. Once again, python is not lacking in IDE options, but users are confronted with yet another decision point before they even get started. Pycharm, Sublime, Spyder, Eclipse, Atom, Neovim, oh my! I find that <a href="https://code.visualstudio.com/docs/languages/python">VS Code</a> offers the best functionality. It’s rich extension ecosystem also means that most major tools (e.g.&nbsp;Quarto, git, linters and stylers, etc.) have nice add-ons so, like RStudio, you can customize your platform to perform many side-tasks in plaintext or with the support of extra UI components.<sup>3</sup></li>
</ul>
</section>
<section id="for-data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="for-data-analysis">For data analysis</h3>
<p>As data practitioners know, we’ll spend most of our time on cleaning and wrangling. As such, R users may struggle particularly to abandon their favorite tools for exploratory data analysis like <code>dplyr</code> and <code>ggplot2</code>. Fans of those packages often appreciate how their functional paradigm helps achieve a “flow state”. Precise syntax may differ, but new developments in the python wrangling stack provide increasingly close analogs to some of these beloved Rgonomics.</p>
<ul>
<li><strong>Data Wrangling</strong>: Although <code>pandas</code> is undoubtedly the best-known wrangling tool in the python space, I believe the growing <a href="https://pola.rs/"><code>polars</code></a> project offers the best experience for a transitioning developer (along with other nice-to-have benefits like being dependency free and blazingly fast). <code>polars</code> may feel more natural and less error-prone to R users for may reasons:
<ul>
<li>it has more internal consistent (and similar to <code>dplyr</code>) syntax such as <code>select</code>, <code>filter</code>, etc. and has demonstrated that the project values a clean API (e.g.&nbsp;recently renaming <code>groupby</code> to <code>group_by</code>)</li>
<li>it does not rely on the distinction between columns and indexes which can feel unintuitive and introduces a new set of concepts to learn</li>
<li>it consistently returns copies of dataframes (while <code>pandas</code> sometimes alters in-place) so code is more idempotent and avoids a whole class of failure modes for new users</li>
<li>it enables many of the same “advanced” wrangling workflows in <code>dplyr</code> with high-level, semantic code like making the transformation of multiple variables at once fast with <a href="https://docs.pola.rs/py-polars/html/reference/selectors.html">column selectors</a>, concisely expressing <a href="https://docs.pola.rs/user-guide/expressions/window/">window functions</a>, and working with nested data (or what <code>dplyr</code> calls “list columns”) with <a href="https://docs.pola.rs/user-guide/expressions/lists/">lists</a> and <a href="https://docs.pola.rs/user-guide/expressions/structs/">structs</a></li>
<li>supporting users working with increasingly large data. Similar to <code>dplyr</code>’s many backends (e.g.&nbsp;<code>dbplyr</code>), <code>polars</code> can be used to write lazily-evaluated, optimized transformations and it’s syntax is reminiscent of <code>pyspark</code> should users ever need to switch between</li>
</ul></li>
<li><strong>Visualization</strong>: Even some of R’s critics will acknowledge the strength of <code>ggplot2</code> for visualization, both in terms of it’s intuitive and incremental API and the stunning graphics it can produce. <a href="https://seaborn.pydata.org/tutorial/objects_interface"><code>seaborn</code>’s object interface</a> seems to strike a great balance between offering a similar workflow (which <a href="https://seaborn.pydata.org/whatsnew/v0.12.0.html">cites <code>ggplot2</code> as an inspiration</a>) while bringing all the benefits of using an industry-standard tool</li>
</ul>
</section>
<section id="for-communication" class="level3">
<h3 class="anchored" data-anchor-id="for-communication">For communication</h3>
<p>Historically, one possible dividing line between R and python has been framed as “python is good at working with computers, R is good at working with people”. While that is partially inspired by reductive takes that R is not production-grade, it is not without truth that the R’s academic roots spurred it to overinvest in a rich “communication stack” and translating analytical outputs into human-readable, publishable outputs. Here, too, the gaps have begun to close.</p>
<ul>
<li><strong>Tables</strong>: R has no shortage of packages for creating nicely formatted tables, an area that has historically lacked a bit in python both in workflow and outcomes. Barring strong competition from the native python space, the one “port” I am bullish about is the recently announced <a href="https://posit-dev.github.io/great-tables/articles/intro.html">Great Tables</a> package. This is a pythonic clone of R’s <code>gt</code> package. I’m more comfortable recommending this since it’s maintained by the same developer as the R version (to support long-term feature parity), backed by an institution not just an individual (to ensure it’s not a short-lived hobby project), and the design feels like it does a good job balancing R inspiration with pythonic practices</li>
<li><strong>Computational notebooks</strong>: Jupyter Notebooks are widely used, widely critiqued parts of many python workflows. While the ability to mix markdown and code chunks. However, notebooks can introduce new types of bugs for the uninitiated; for example, they are hard to version control and easy to execute in the wrong environment. For those coming from the world of R Markdown, plaintext computational notebooks like <a href="https://quarto.org/">Quarto</a> may provide a more transparent development experience. While Quarto allows users to write in <code>.qmd</code> files which are more like their <code>.rmd</code> predecessors, its renderer can also handle Jupyter notebooks to enable collaboration across team members with different preferences</li>
</ul>
</section>
<section id="miscellaneous" class="level3">
<h3 class="anchored" data-anchor-id="miscellaneous">Miscellaneous</h3>
<p>A few more tools may be helpful and familiar to <em>some</em> R users who tend towards the more “developer” versus “analyst” side of the spectrum. These, in my mind, have even more varied pros and cons, but I’ll leave them for consideration:</p>
<ul>
<li><strong>Environment Management</strong>: Joining the python world means never having to settle on an environment management tool for installing packages. There’s a truly overwhelming number of ways to manage project-level dependencies (<code>virtualenv</code>, <code>conda</code>, <code>piptools</code>, <code>pipenv</code>, <code>poetry</code>, and that doesn’t even scratch the surface) with different pros and cons and phenomenal amount of ink/pixels have been spilled over litigating these trade-offs. Putting all that aside, lately, I’ve been favoring <a href="https://pdm-project.org/latest/"><code>pdm</code></a> because it prioritizes features I care most about (auto-updating <code>pyproject.toml</code>, isolating dependencies from dependencies-of-dependencies, active development and error handling, mostly just works pretty undramatically)</li>
<li><strong>Developer Tools</strong>: <a href="https://docs.astral.sh/ruff/"><code>ruff</code></a> provides a range of linting and styling options (think R’s <code>lintr</code> and <code>styler</code>) and provides a one-stop-shop over what can be an overwhelming number of atomic tools in this space (<code>isort</code>, <code>black</code>, <code>flake8</code>, etc.). <code>ruff</code> is super fast, has a nice VS Code extension, and, while this class of tools is generally considered more advanced, I think linters can be a fantastic “coach” for new users about best practices</li>
</ul>
</section>
</section>
<section id="more-to-come" class="level2">
<h2 class="anchored" data-anchor-id="more-to-come">More to come!</h2>
<p>Each recommendation here itself could be its own tutorial or post. In particular, I hope to showcase the Rgonomics of <code>polars</code>, <code>seaborn</code>, and <code>great_tables</code> in future posts.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Of course, languages have their own subcultures too. The <code>tidyverse</code> and <code>data.table</code> parts of the R world tend to favor different semantics and ergonomics. This post caters more to the former.↩︎</p></li>
<li id="fn2"><p>There is no doubt a place for language ports, especially for earlier stage project where no native language-specific standard exists. For example, I like Karandeep Singh’s lab work on <a href="https://github.com/TidierOrg/Tidier.jl">a tidyverse for Julia</a> and maintain my own <a href="https://github.com/emilyriederer/dbtplyr"><code>dbtplyr</code></a> package to port <code>dplyr</code>’s select helpers to <code>dbt</code>↩︎</p></li>
<li id="fn3"><p> If anything, the one challenge of VS Code is the sheer number of set up options, but to start out, you can see these excellent tutorials from Rami Krispin on recommended <a href="https://github.com/RamiKrispin/vscode-python">python</a> and <a href="https://github.com/RamiKrispin/vscode-r">R</a> configurations ↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>rstats</category>
  <category>python</category>
  <guid>https://emilyriederer.com/post/py-rgo/</guid>
  <pubDate>Sat, 30 Dec 2023 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/py-rgo/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Big ideas from the 2023 Causal Data Science Meeting</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/recap-causal-2023/</link>
  <description><![CDATA[ 





<p>Last week, I enjoyed attending parts of the annual virtual <a href="https://www.causalscience.org/">Causal Data Science Meeting</a> organized by researchers from Maastricht University, Netherlands, and Copenhagen Business School, Denmark. This has been one of my favorite virtual events since the first iteration in 2020, and I find it consistently highlights the best of the causal research community: brining together industry and academia with concise talks that are at once thought-provoking, theoretically well-grounded, yet thoroughly pragmatic.</p>
<p>While I could not join the entire event (running in CET time, some sessions fit snuggly between my first cup of coffee and first work meeting of the day in CST), this year’s conference did not disappoint! Below, I share a sampling with five “big ideas” from the sessions.</p>
<ol type="1">
<li><p><strong>What’s the current “gold standard” of causal ML methods in industry?</strong> <a href="https://www.linkedin.com/in/dimgold/https://www.linkedin.com/in/dimgold/">Dima Goldenberg</a> presented a great case study on heterogeneous uplift modeling at Booking.com. (While I couldn’t find the exact slides or paper, you can get a flavor of Booking’s work in experimentation and causal inference from their excellent <a href="https://blog.booking.com/#datascience">tech blog</a> )</p></li>
<li><p><strong>How does causal evidence add value?</strong> <a href="https://www.linkedin.com/in/robert-kubinec-9191a9a/">Robert Kubinec</a> conceptualized a measurable spectrum of descriptive to causal studies based on entropy. This framework broadens the aperture to think about how both quantitative and qualitative evidence can come together to form causal conclusions. (<a href="https://osf.io/preprints/socarxiv/a492b/">Preprint</a>)</p></li>
<li><p><strong>But how do we know the methods work?</strong> Causal methods are notoriously hard to validate since, by definition, we lack a ground truth against which to compare our estimate. To validate new methods, Lingjie Shen and coauthors presented one approach with their new [<code>RCTrep</code> R package] (https://github.com/duolajiang/RCTrep) which can be used to compare outcomes between real-world data (RWD) and randomized control trial data (RCT).</p></li>
<li><p><strong>And what do we do when they can’t get all the way there?</strong> <a href="https://www.linkedin.com/in/ferlocar/">Carlos Fernández-Loría</a> and <a href="https://www.linkedin.com/in/jorge-lor%C3%ADa/">Jorge Loría</a> talk on “Causal Scoring” explores how we can accept and make use of “causal ranking” or “causal classification” even when we do not believe we can generate fully credible, calibrated causal estimates. By defining which type of estimand is really necessary for a specific use case, they show how one can tailor their modeling approach and broaden the range of applications. (<a href="https://arxiv.org/abs/2206.12532">Preprint</a>)</p></li>
<li><p><strong>Finally, do the best methods that correctly accrue causal evidence and validate <em>matter</em>?</strong> <a href="https://www.linkedin.com/in/ronberman/">Ron Berman</a> and Anya Shchetkina tackled this question in their paper about when correctly modeling uplift heterogeneity does and doesn’t matter. They decomposed potential causes using real-world marketing and public health examples and presented a methodology for identifying when uplift-based personalization makes a business impact (I couldn’t find pre-print, but they also presented at MIT’s CODE this week, so hopefully there will be a video soon!)</p></li>
</ol>
<p>One of the joys of the causal DS community’s mindset is the inherent focus on impact and pragmatism, and this year’s conference continued to deliver in that vein. I’m marking my calendar (and setting my 4AM alarm!) for next year already.</p>



 ]]></description>
  <category>causal</category>
  <guid>https://emilyriederer.com/post/recap-causal-2023/</guid>
  <pubDate>Sat, 18 Nov 2023 06:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/recap-causal-2023/featured.png" medium="image" type="image/png" height="83" width="144"/>
</item>
<item>
  <title>Data Downtime Horror Stories Panel</title>
  <link>https://emilyriederer.com/talk/data-downtime/</link>
  <description><![CDATA[ 




<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>In October, I joined a Halloween-themed panel along with Chad Sanderson and Joe Reis to discuss our horror stories of data quality gone wrong and how to build successful data quality strategies in large organizations. Key takeaways are summarized on <a href="https://www.montecarlodata.com/blog-scary-data-quality-stories-7-tips-for-preventing-your-own-data-downtime-nightmare/">Monte Carlo’s blog</a>.</p>


</section>

 ]]></description>
  <category>data</category>
  <guid>https://emilyriederer.com/talk/data-downtime/</guid>
  <pubDate>Mon, 23 Oct 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/data-downtime/featured.PNG" medium="image"/>
</item>
<item>
  <title>Operationalizing Column-Name Contracts with dbtplyr</title>
  <link>https://emilyriederer.com/talk/dbtplyr/</link>
  <description><![CDATA[ 




<p>url_video: “”</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" aria-controls="tabset-1-4" aria-selected="false">Video</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p>At Coalesce for dbt user audience:</p>
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides.pdf">Slides</a> </span><br>
<span><i class="bi bi-play"></i> <a href="https://www.getdbt.com/coalesce-2021/operationalizing-columnname-contracts-with-dbtplyr/">Video</a> </span></p>
<p>At posit::conf for R user audience:</p>
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides-posit.pdf">Slides</a> </span><br>
<span><i class="bi bi-play"></i> Video - posit::conf for R User Audience <em>coming soon!</em> </span></p>
<p><span><i class="bi bi-pencil"></i> <a href="../..\post/column-name-contracts/">Post - Column Name Contracts</a> </span><br>
<span><i class="bi bi-pencil"></i> <a href="../..\post/convo-dbt/">Post - Column Name Contracts in dbt</a> </span><br>
<span><i class="bi bi-pencil"></i> <a href="../..\post/convo-dbt-update/">Post - Column Name Contracts with dbtplyr</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>Complex software systems make performance guarantees through documentation and unit tests, and they communicate these to users with conscientious interface design.</p>
<p>However, published data tables exist in a gray area; they are static enough not to be considered a “service” or “software”, yet too raw to earn attentive user interface design. This ambiguity creates a disconnect between data producers and consumers and poses a risk for analytical correctness and reproducibility.</p>
<p>In this talk, I will explain how controlled vocabularies can be used to form contracts between data producers and data consumers. Explicitly embedding meaning in each component of variable names is a low-tech and low-friction approach which builds a shared understanding of how each field in the dataset is intended to work.</p>
<p>Doing so can offload the burden of data producers by facilitating automated data validation and metadata management. At the same time, data consumers benefit by a reduction in the cognitive load to remember names, a deeper understanding of variable encoding, and opportunities to more efficiently analyze the resulting dataset. After discussing the theory of controlled vocabulary column-naming and related workflows, I will illustrate these ideas with a demonstration of the {dbtplyr} dbt package which helps analytics engineers get the most value from controlled vocabularies by making it easier to effectively exploit column naming structures while coding.</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div id="slides" style="width:100%; aspect-ratio:16/11;">
<embed src="slides.pdf#zoom=Fit" width="100%" height="100%">
</div>
</div>
<div id="tabset-1-4" class="tab-pane" aria-labelledby="tabset-1-4-tab">
<p>Coming Soon!</p>
</div>
</div>
</div>



 ]]></description>
  <category>workflow</category>
  <category>rmarkdown</category>
  <category>rstats</category>
  <guid>https://emilyriederer.com/talk/dbtplyr/</guid>
  <pubDate>Thu, 21 Sep 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/dbtplyr/featured.png" medium="image" type="image/png" height="82" width="144"/>
</item>
<item>
  <title>Scaling Personalized Volunteer Emails</title>
  <link>https://emilyriederer.com/talk/midterm-email/</link>
  <description><![CDATA[ 




<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" aria-controls="tabset-1-4" aria-selected="false">Video</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides.pdf">Slides</a> </span><br>
<span><i class="bi bi-play"></i> <a href="https://youtu.be/5UGUcgxTWTM">Video</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>In this four-minute lightning talk, I explain how Two Million Texans used components of our existing data stack to provide personalized success metrics and action recommendations to over 5,000 volunteers in the lead up to the 2022 midterm elections. I briefly describe our pipeline and how we frontloaded key computational steps in BigQuery to circumvent limitations of downstream tools.</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div id="slides" style="width:100%; aspect-ratio:16/11;">
<embed src="slides.pdf#zoom=Fit" width="100%" height="100%">
</div>
</div>
<div id="tabset-1-4" class="tab-pane" aria-labelledby="tabset-1-4-tab">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/5UGUcgxTWTM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>



 ]]></description>
  <category>data</category>
  <category>elt</category>
  <guid>https://emilyriederer.com/talk/midterm-email/</guid>
  <pubDate>Wed, 21 Jun 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/midterm-email/featured.png" medium="image" type="image/png" height="80" width="144"/>
</item>
<item>
  <title>Causal Design Patterns</title>
  <link>https://emilyriederer.com/talk/causal-design-patterns/</link>
  <description><![CDATA[ 




<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" aria-controls="tabset-1-4" aria-selected="false">Video</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides.pdf">Slides</a> </span><br>
<span><i class="bi bi-play"></i> <a href="https://www.youtube.com/watch?v=0kDjkTB2yZ0">Video</a> </span><br>
<span><i class="bi bi-play"></i> <a href="https://www.youtube.com/watch?v=VP3BBZ7poc0&amp;t=191s">Video - Discussion</a> </span><br>
<span><i class="bi bi-pencil"></i> <a href="../..\post/causal-design-patterns/">Post - Causal Design Patterns</a> </span><br>
<span><i class="bi bi-pencil"></i> <a href="../..\post/causal-data/">Post - Causal Data Management</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>Experimentation is a pillar of product data science and machine learning. But what can you do when experimentation is impractical, costly, risky to customer experience, or too slow to read the desired long-term results?</p>
<p>While industry is often spoiled by their ability to AB test, the question of how to draw valid causal measurements from non-randomized data has long been a focus of many fields from epidemiology to public policy. This talk will review four common ‘design pattern’ for observational causal inference and how they can apply to industry. Exploring the assumptions, limitations, and applications of these methods will help practicing data scientists recognize opportunities to use this methods to tackle seemingly unanswerable questions they face.</p>
<p>Moving beyond the basics, we will see how these building-block patterns are fueling an explosion in modern causal machine learning and discuss how to seed your organization for success with enterprise knowledge and data management.</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div id="slides" style="width:100%; aspect-ratio:16/11;">
<embed src="slides.pdf#zoom=Fit" width="100%" height="100%">
</div>
</div>
<div id="tabset-1-4" class="tab-pane" aria-labelledby="tabset-1-4-tab">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0kDjkTB2yZ0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>



 ]]></description>
  <category>causal</category>
  <guid>https://emilyriederer.com/talk/causal-design-patterns/</guid>
  <pubDate>Wed, 07 Jun 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/causal-design-patterns/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Industry information management for causal inference</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/causal-data/</link>
  <description><![CDATA[ 





<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://emilyriederer.com/post/causal-data/featured.png" class="img-fluid figure-img"></p>
<figcaption>Data strategy motivated by causal methods</figcaption>
</figure>
</div>
<p><em>This post summarizes the final third of my talk at Data Science Salon NYC in June 2023. Please see the <a href="../..\talk/causal-design-patterns">talk details</a> for more content.</em></p>
<p>Techniques of observational causal inference are becoming increasingly popular in industry as a complement to experimentation. Causal methods offer the promise of accelerating measurement agendas and facilitating the estimation of previously un-measurable targets by allowing analysts to extract causal insights from “found” data (e.g.&nbsp;observational data collected without specific intent). However, if executed without careful attention to their assumptions and limitations, they can lead to spurious conclusions.</p>
<p>Both experimental and observational methods attempt to address the <strong>fundamental problem of causal inference</strong>: that is, the fact that for a given treatment of interest, we can never “see” the <em>individual-level outcome</em> both for the case when an individual received a treatment and a counterfactual scenario in which <em>for the same individual in the exact same context</em> that treatment was withheld. Some literature casts this as a “missing data” problem.<sup>1</sup> Counterfactual data is uncollectable; however, this fundamental missingness can be partially mitigated by diligent collection of <em>other</em> types of quantitative and qualitative information to control for confounding<sup>2</sup> and interrogate assumptions.</p>
<p>In this post, I argue that industry has unique advantages when using causal techniques over the social science disciplines that originated many foundational methods due to industry’s (theoretically) superior ability to observe and capture relevant supplemental data and context. Examining the implicit assumptions in common <a href="../..\post/causal-design-patterns">causal design patterns</a> motivates the types of proactive enterprise information management – including data, metadata, and knowledge management – that will help preserve the raw inputs that future data scientists will need to effectively deploy causal techniques on historical data and answer questions that our organizations cannot even anticipate today. By casting an intentionally wide net on what information we observationally collect, we increase the likelihood that the future “found” data will have what those analysts need to succeed.</p>
<section id="why-industry-needs-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="why-industry-needs-causal-inference">Why industry needs causal inference</h2>
<p><img src="https://emilyriederer.com/post/causal-data/why-not-experiment.png" class="img-fluid"></p>
<p>Industry data science tends to highly value the role of A/B testing and experimentation. However, there are many situations where experimentation is not an optimal approach to learning. Experiments can be infeasible if we worry about the ethics or reputational risk of offering disparate customer treatments; they may be impractical in situations that are hard to randomize or avoid spillover effects; they can be costly to run and configure either in direct or opportunity costs; and, finally, they can just be <em>slow</em> if we wish to measure complex and long-term impacts on customer behaviors (e.g.&nbsp;retention, lifetime value).</p>
</section>
<section id="what-causal-methods-require" class="level2">
<h2 class="anchored" data-anchor-id="what-causal-methods-require">What causal methods require</h2>
<p><img src="https://emilyriederer.com/post/causal-data/patterns-and-variation.png" class="img-fluid"></p>
<p>These limitations are one of the reasons why observational causal inference is gaining increasing popularity in industry. Methods of observational causal inference allows us to estimate treatment effects without randomized controlled experimentation by using existing historical data. At the highest level, these methods work by replacing <em>randomization</em> with strategies to exploit other forms of <em>semi-random variation</em> in historical exposures of a population to a treatment. Since this semi-random <em>variation</em> could be susceptible to confounding, observational methods supplement variation with <em>additional data</em> to control for other observable sources of bias in our estimates and <em>contextual assumptions</em> about the data generating process.</p>
<p>My previous post on <a href="../..\post/causal-design-patterns">causal design patterns</a> outlines a number of foundational causal methods, but I’ll briefly recap to emphasize the different ways that sources of variation, data, and context are used:</p>
<ul>
<li><strong>Stratification and Inverse Propensity Score Weighting</strong>:
<ul>
<li>Exploits “similar” populations of treated and untreated individuals</li>
<li>Assumes we can observe and control for common causes of the treatment and the outcome</li>
</ul></li>
<li><strong>Regression Discontinuity</strong>:
<ul>
<li>Exploits a sharp, semi-arbitrary cut-off between treated and untreated individuals</li>
<li>Assumes that the outcome is continuous with respect to the assigment variable and the assignment mechanism is unknown to individuals (to avoid self-selection)</li>
</ul></li>
<li><strong>Difference in Differences</strong>:
<ul>
<li>Exploits variation between <em>behavior over time</em> of treated and untreated <em>groups</em></li>
<li>Assumes that the treatment assignment is unrelated to expected future outcomes and that the treatment is well-isolated to the treatment group</li>
</ul></li>
</ul>
<p>Notably, the assumptions mentioned above are largely untestable statistically (e.g.&nbsp;not like testing for normality or multicolinearity) but rely on knowledge of past strategies and policies that guided differential treatment in historical data.<sup>3</sup></p>
</section>
<section id="industrys-unique-advantages-deploying-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="industrys-unique-advantages-deploying-causal-inference">Industry’s unique advantages deploying causal inference</h2>
<p><img src="https://emilyriederer.com/post/causal-data/industry-advantages.png" class="img-fluid"></p>
<p>Many causal methods originated in fields like epidemiology, economics, political science, and other social sciences. In such fields, direct experimentation is often impossible and even first-hand data collection is less common. Often, researchers may have to rely on pre-existing data sources like censuses, surveys, and administrative data (e.g.&nbsp;electronic health records).</p>
<p>Despite the lineage of these methods, industry has many advantages over traditional research fields in using them because each company controls the entire “universe” in which its customers exist. This should in theory provide a distinct advantage when collecting each of the three “ingredients” that causal methods use to replace randomization:</p>
<ul>
<li><strong>Variation</strong>: We control customer engagement strategies through methods like customer segmentation or models. Subsequent customer treatments are completely known to us but inherently have some arbitrary, judgmental component to exploit</li>
<li><strong>Data</strong>: We tend to be able to collect more measurements of our customers both as a snapshot (more variety in fields) and longitudinally (more observations over time) that can be brought into our analyses to control for confounders<sup>4</sup>, reduce other sources of variation in our estimate, and have additional ‘out of time’ data left over to conduct forms of validation like placebo tests</li>
<li><strong>Context</strong>: We tend to know how past strategies were set-up, how they looked to individuals involved, and <em>why</em> those decisions were made. This can be critical in reasoning whether our assumptions hold</li>
</ul>
<p>However, to convert this theoretical benefit to a practical one requires information management.</p>
</section>
<section id="data-management-for-causal-inference" class="level2">
<h2 class="anchored" data-anchor-id="data-management-for-causal-inference">Data management for causal inference</h2>
<p><img src="https://emilyriederer.com/post/causal-data/featured.png" class="img-fluid"></p>
<p>While all causal methods will be enhanced with better enterprise information management, it’s easiest to see the motivation by thinking back to specific examples. Causal inference can benefit from better data, metadata, and knowledge management. These are illustrated by propensity score weighting, regression discontinuity, and diff-in-diff respectively.</p>
<p><strong>Integrated Data Management</strong></p>
<p>Earlier, we posited that one advantage that industry has over academia for causal inference is access to richer historical data sources as a higher level of resolution (more measures per individual at more time points). A rich set of customer measures is critical for stratification and propensity score weighting where we attempt to control for selection on observables by balancing populations along dimensions that might be common causes of treatment assignment and outcome. (And, we may also wish to control for other unrelated sources of variation that effect only the outcome to develop more precise estimates.)</p>
<p>However, this is only true if customer data is proactively <em>collected, cleaned, and harmonized</em> across sources in the true spirit of a customer 360 view. Enterprises may collect data about customers from many different operational systems – for example, demographic information provided at registration, digital data on their logins and web activity, campaign data on attempted customer touchpoints and engagement, behavioral or fulfillment data on purchases / subscription renewals / etc. Any of these sources could be useful “observables” that help close confounding pathways in our analyses.</p>
<p>To make this data useful and accessible for analysis, it must be <em>proactively integrated</em> into a common source like a data warehouse, <em>well-documented</em> to help future users understand the nuances of each system, <em>harmonized</em> so fields have standard definitions (e.g.&nbsp;common definitions of an “account” and a “customer”), and <em>unified</em> by using techniques like entity resolution to ensure all sources share common identifiers so that they can be merged for analysis.</p>
<p><strong>Metadata Management</strong></p>
<p>Beyond those “typical” sources of customer data, our past customer strategies create data beyond the data directly generated by our customers. Metadata about past campaigns such as precise business logic on the different treatments offered (e.g.&nbsp;if sending customers a discount, what algorithmically determined the amount?), the campaign targeting and segmentation (e.g.&nbsp;What historical behaviors were used to segments customers? Was treatment determined by a predictive model?), and launch timing can all be critical to clearly identifying those sources of variation that we wish to exploit. For example, we might know that we once ran an re-engagement campaign to attempt the nudge interaction from customers who didn’t log-in to a website for some amount of time, but knowing whether that campaign was targeting customers &gt;30 days inactive or &gt;45 days inactive impacts our ability to analyze it with a regression discontinuity.</p>
<p>This means that we need to <em>treat metadata as first-class data</em> and ensure that it is extracted from operational source systems (or intent docs, config files, etc.), structured in a machine-readable format, and preserved in analytical data stores along with our customer data.</p>
<p>The importance of “metadata as data” extends beyond business-as-usual organization strategies. We can also fuel future causal inference with better metadata management of past formal experiments and execution errors.</p>
<p>As discussed above, formal experiments may represent a substantial <em>investment</em> in company resources so the data collected from them should be regarded as an <em>asset</em>. Beyond their utility for one-time reads and decisions, experiment designs and results should be carefully catalogued along with the assigned treatment group and the randomization criteria (such as fields characterizing <a href="https://www.census.gov/programs-surveys/acs/technical-documentation/user-notes/2022-07.html">sampling weights</a> as provided in US Census data). This can support future <em>observational</em> analysis of past experiments, including generalizing and transporting results to different populations.</p>
<p>Furthermore, even <em>mistakes</em> in executing past strategies may become “natural experiments” to help businesses understand scenarios that they might never have prioritized for testing. So, machine-readable incident logs and impacted populations can be useful as well.</p>
<p><strong>Knowledge Management</strong></p>
<p>Of course, not <em>all</em> information can be condensed into a nice, machine-readable spreadsheet. Methods like difference-in-differences illustrate how conceptual context can also help us battle-test assumptions like whether the decision-to-treat could have spilled over into the control population or been influenced by an anticipated change in the future outcome. This is the one area where industry may sometimes <em>lag</em> social sciences in information since some population-level treatments like a state law or local ordinance often have documented histories through the legislative process, news coverage, and historical knowledge about their implementation.</p>
<p>Industry can catch up on knowledge management by documenting and preserving in a centralized knowledge repository key information about strategic decisions undertaken, the motivating factors, and the anticipated customer experience. Such documents are inevitably created when working on new projects through memos ad decks intended to communicate the business case, intent, and expected customer experience. However, proactively figuring out how to <em>organize and index</em> this information through a classification system and <em>democratize access</em> through centralized knowledge repositories is critical to giving future users entree to this tribal knowledge. Projects like Airbnb’s <a href="https://github.com/airbnb/knowledge-repo">Knowledge Repository</a> suggest what such a system might look like in practice.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>For example, see https://arxiv.org/abs/1710.10251↩︎</p></li>
<li id="fn2"><p>If you’ve heard of ‘selection on observables’ in causal literature, richer data means observables!↩︎</p></li>
<li id="fn3"><p>There are some exceptions to this like placebo tests, bunching checks, etc.↩︎</p></li>
<li id="fn4"><p>Notable, the availability of more data absolutely does <em>not</em> mean that we should simply “dump in” all the data we have. Controlling for certain variables like colliders is counterproductive.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>causal</category>
  <category>data</category>
  <guid>https://emilyriederer.com/post/causal-data/</guid>
  <pubDate>Tue, 30 May 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/causal-data/featured.png" medium="image" type="image/png" height="60" width="144"/>
</item>
<item>
  <title>DataFold Data Quality Meet Up</title>
  <link>https://emilyriederer.com/talk/meetup-datafold/</link>
  <description><![CDATA[ 




<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" aria-controls="tabset-1-4" aria-selected="false">Video</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-file-bar-graph"></i> <a href="slides.pdf">Slides</a> </span><br>
<span><i class="bi bi-play"></i> <a href="https://www.youtube.com/watch?v=uAe74zdLHbM">Video</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>This is the full recording from Datafold’s 9th Data Quality Meetup on Thursday, May 11th, 2023, which was focused on ‘Running dbt at scale’.</p>
<p>Following our usual structure, each of our speakers present a lightning talk and then we transition into a panel discussion moderated by Gleb Mezhanskiy - who pulls in the audiences’ questions.</p>
<p>We had 6 guest speakers &amp; panelists: 1. Emily Riederer @ Capital One - “Operationalizing Column Name Contracts” 2. Felix Kreitschmann and Jorrit Posor @ FINN Auto - “Supercharging Analytics Engineers: How to save time and prevent technical debt by automating CI checks” 3. Alexandra Gronemeyer @ Airbyte - “adopting and running dbt within a small data team at Airbyte” 4. Jason Jones @ Virgin Media O2 - “Zero to 200: scaling analytics engineering within an enterprise” 5. Sung Won Chung @ dbt Labs - “Experiences implementing dbt at scale”</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<div id="slides" style="width:100%; aspect-ratio:16/11;">
<embed src="slides.pdf#zoom=Fit" width="100%" height="100%">
</div>
</div>
<div id="tabset-1-4" class="tab-pane" aria-labelledby="tabset-1-4-tab">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/uAe74zdLHbM" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>



 ]]></description>
  <category>elt</category>
  <category>data</category>
  <guid>https://emilyriederer.com/talk/meetup-datafold/</guid>
  <pubDate>Fri, 12 May 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/meetup-datafold/featured.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Crosspost: The Art of Abstraction in ETL</title>
  <dc:creator>Emily Riederer</dc:creator>
  <link>https://emilyriederer.com/post/abstraction-airbyte/</link>
  <description><![CDATA[ 





<p><img src="https://emilyriederer.com/post/abstraction-airbyte/featured.PNG" class="img-fluid"></p>
<p>I previously shared the first in my three-part series of guest posts on Airbyte’s developer blog about ETL. The first focused on errors in data extraction. The next two focused on the countless, small decisions one makes when loading data, and finally the DataOps burden to keep things up-and-running.</p>
<p>This post serves only to serve as a quick reference to those posts:</p>
<ul>
<li><a href="https://airbyte.com/blog/dodging-data-extraction-errors">Dodging extraction errors</a></li>
<li><a href="https://airbyte.com/blog/loading-data-in-etl">Making sound loading decisions</a></li>
<li><a href="https://airbyte.com/blog/etl-good-practices">Keeping the good things going</a></li>
</ul>



 ]]></description>
  <category>data</category>
  <category>workflow</category>
  <category>elt</category>
  <category>crosspost</category>
  <guid>https://emilyriederer.com/post/abstraction-airbyte/</guid>
  <pubDate>Wed, 03 May 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/post/abstraction-airbyte/featured.PNG" medium="image"/>
</item>
<item>
  <title>Posit Data Science Hangout</title>
  <link>https://emilyriederer.com/talk/meetup-posit/</link>
  <description><![CDATA[ 




<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs"><li class="nav-item"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" aria-controls="tabset-1-1" aria-selected="true">Quick Links</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" aria-controls="tabset-1-2" aria-selected="false">Abstract</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" aria-controls="tabset-1-3" aria-selected="false">Slides</a></li><li class="nav-item"><a class="nav-link" id="tabset-1-4-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-4" aria-controls="tabset-1-4" aria-selected="false">Video</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" aria-labelledby="tabset-1-1-tab">
<p><span><i class="bi bi-play"></i> <a href="https://www.youtube.com/watch?v=VUgoaNhE4u0">Video</a> </span></p>
</div>
<div id="tabset-1-2" class="tab-pane" aria-labelledby="tabset-1-2-tab">
<p>We were recently joined by Emily Riederer, Senior Manager - Customer Management Data Science &amp; Analytics at Capital One. We discussed how a strong foundation in high-quality data infrastructure and reproducible tools sets the stage for innovation in modeling, causal inference, and analytics, and so much more.</p>
</div>
<div id="tabset-1-3" class="tab-pane" aria-labelledby="tabset-1-3-tab">
<p>Not applicable - live conversation</p>
</div>
<div id="tabset-1-4" class="tab-pane" aria-labelledby="tabset-1-4-tab">
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/VUgoaNhE4u0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</div>
</div>
</div>



 ]]></description>
  <category>rstats</category>
  <category>workflow</category>
  <guid>https://emilyriederer.com/talk/meetup-posit/</guid>
  <pubDate>Thu, 13 Apr 2023 05:00:00 GMT</pubDate>
  <media:content url="https://emilyriederer.com/talk/meetup-posit/featured.png" medium="image" type="image/png" height="74" width="144"/>
</item>
</channel>
</rss>
