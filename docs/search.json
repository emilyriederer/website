[
  {
    "objectID": "talk/tidycf/index.html",
    "href": "talk/tidycf/index.html",
    "title": "tidycf: Turning analysis on its head by turning cashflows on their side",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n\n\nStatistical computing has revolutionized predictive modeling, but financial modeling lags in innovation. At Capital One, valuations analysis required legacy SAS platforms, obscure data lineage, and cumbersome Excel cashflow statements. This talk describes development of the tidycf R package to reinvent this process as a seamless, end-to-end workflow.\nReimagining cashflow statements as tidy data facilitates a simple, efficient, and transparent workflow while incorporating more statistically rigorous methods. tidycf leverage the full power of R and RStudio – building on top of the tidyverse; reducing complex crunching, wrangling, and visualization to pipeable functions; guiding analysis and documentation with RMarkdown templates; and incorporating features of the latest development version IDE. Altogether, this delivers a good user experience without the overheard of maintaining a custom GUI.\nThe resulting package goes beyond “getting stuff done”. tidycf also increases quality, reproducibility, and creativity of analysis; ensures consistency and knowledge transfer; reduces the burdens of documentation and regulation; and speeds innovation and time-to-market – all while guiding less-technical analysts through an immersive crash course to R and the tidyverse."
  },
  {
    "objectID": "talk/rmarkdown-driven-development/index.html",
    "href": "talk/rmarkdown-driven-development/index.html",
    "title": "RMarkdown Driven Development",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n Post - RMarkown Driven Development \n Post - RmdDD Technical Appendix \n\n\nRMarkdown enables analysts to engage with code interactively, embrace literate programming, and rapidly produce a wide variety of high-quality data products such as documents, emails, dashboards, and websites. However, RMarkdown is less commonly explored and celebrated for the important role it can play in helping R users grow into developers. In this talk, I will provide an overview of RMarkdown Driven Development, a workflow for converting one-off analysis into a well-engineered and well-designed R package with deep empathy for user needs. We will explore how the methodical incorporation of good coding practices such as modularization and testing naturally evolves a single-file RMarkdown into an R project or package. Along the way, we will discuss big-picture questions like “optimal stopping” (why some data products are better left as single files or projects) and concrete details such as the {here} and {testthat} packages which can provide step-change improvements to project sustainability."
  },
  {
    "objectID": "talk/projmgr/index.html",
    "href": "talk/projmgr/index.html",
    "title": "projmgr: Managing the human dependencies of your project",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n Post - A beginner’s guide to Shiny modules \n\n\nMany tools (e.g. git, make, Docker) and R packages (e.g. packrat, renv) aim to eliminate pain and uncertainty from technical project management, resulting in well-engineered software and reproducible research. However, there is no analogous gold standard tool for managing the most time-consuming and unpredictable dependencies in our work: our fellow humans.\nCommunication with our collaborators and customers is often spread across email, Slack, GitHub, and sometimes third-party project management tools like Jira or Trello. Switching between these different software tools and frames of mind knocks analysts out of their flow and detracts from getting work done.\nThe projmgr R package offers a solution: an opinionated interface for conducting end-to-end project management through GitHub.Key features of this package include bulk generation of GitHub issues and milestones from a YAML project plan and automated creation of status updates with user-friendly text summaries and plots.\nIn this lightning talk, I demonstrate the key features of projmgr motivated by a range of use cases including updating stakeholders, monitoring KPIs, managing an analytics team, and organizing a hackathon."
  },
  {
    "objectID": "talk/organization/index.html",
    "href": "talk/organization/index.html",
    "title": "oRganization: Design patterns for internal packages",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n Post - A Team of Packages \n\n\nMany case studies demonstrate the benefits of organizations developing internal R packages. But how do you move your organization from individual internal packages to a coherent internal ecosystem?\nIn this talk, I analyze how internal packages differ from open-source, publicly available packages in their ideal level of abstraction and their understanding of specific user needs. By applying the jobs-to-be-done framework, I envision internal packages as teammates annd consider the many roles they can play, from unblocking IT challenges to democratizing tribal knowledge.\nTo help our team of packages succeed, we explore a wide variety of concrete design choices that developers of internal packages may make – spanning functions, documentation, testing, and more."
  },
  {
    "objectID": "talk/midterm-email/index.html",
    "href": "talk/midterm-email/index.html",
    "title": "Scaling Personalized Volunteer Emails",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n\n\nIn this four-minute lightning talk, I explain how Two Million Texans used components of our existing data stack to provide personalized success metrics and action recommendations to over 5,000 volunteers in the lead up to the 2022 midterm elections. I briefly describe our pipeline and how we frontloaded key computational steps in BigQuery to circumvent limitations of downstream tools."
  },
  {
    "objectID": "talk/meetup-posit/index.html",
    "href": "talk/meetup-posit/index.html",
    "title": "Posit Data Science Hangout",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Video \n\n\nWe were recently joined by Emily Riederer, Senior Manager - Customer Management Data Science & Analytics at Capital One. We discussed how a strong foundation in high-quality data infrastructure and reproducible tools sets the stage for innovation in modeling, causal inference, and analytics, and so much more.\n\n\nNot applicable - live conversation"
  },
  {
    "objectID": "talk/index.html",
    "href": "talk/index.html",
    "title": "Talks",
    "section": "",
    "text": "I enjoy sharing ideas, tools, and methods that excite me. Slides and videos from past conference talks are available and embedded in almost all of the posts below. Additional resources may be available upon request if not included online – please don’t hesitate to reach out!\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCasual Inference Pod - Optimizing Data Workflows with Emily Riederer (Season 6, Episode 8)\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\n\nCasual Inference is a podcast on all things epidemiology, statistics, data science, causal inference, and public health. Sponsored by the American Journal of Epidemiology. As a guest on this episode, I discuss data science communication, the different challenges of causal analysis in industry versus academia, and much more.\n\n\n\n\n\nJun 25, 2025\n\n\nCasual Inferencence Podcast\n\n\n\n\n\n\n\n\n\n\n\n\nA different type of DAG - data pipelines for epidemiology\n\n\n\n\n\n\nworkflow\n\n\ndata\n\n\n\nA tour of data pipeline techniques and tools for use in academia\n\n\n\n\n\nJun 11, 2025\n\n\nSociety of Epidemiologic Research\n\n\n\n\n\n\n\n\n\n\n\n\nPython Rgonomics\n\n\n\n\n\n\nworkflow\n\n\npython\n\n\nrstats\n\n\n\nA survey of modern python tooling that “feels good” to R users\n\n\n\n\n\nAug 15, 2024\n\n\nposit::conf(2024)\n\n\n\n\n\n\n\n\n\n\n\n\nData Downtime Horror Stories Panel\n\n\n\n\n\n\ndata\n\n\n\nPanel discussion with Chad Sanderson and Joe Reis, hosted by Monte Carlo Data, on our thorniest brushes with data downtime, leading data teams to tackle data quality at scale with testing, contracts, observability and monitoring, and more.\n\n\n\n\n\nOct 23, 2023\n\n\nMonte Carlo Data Panel\n\n\n\n\n\n\n\n\n\n\n\n\nOperationalizing Column-Name Contracts with dbtplyr\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nAn exploration of how data producers and consumers can use column names as interfaces, configuations, and code to improve data quality and discoverability. The second half of the talk demonstrates how to implement these ideas with my dbtplyr dbt package.\n\n\n\n\n\nSep 21, 2023\n\n\ndbt Labs Coalesce (2021) | NYR (2023) | posit::conf(2023)\n\n\n\n\n\n\n\n\n\n\n\n\nScaling Personalized Volunteer Emails\n\n\n\n\n\n\ndata\n\n\nelt\n\n\n\nAn overview of the data stack used to automate over 50,000 personalized emails to voter turnout volunteers using BigQuery, dbt, Census, and MailChimp\n\n\n\n\n\nJun 21, 2023\n\n\nData for Progress Data Engineering Open Mic (Jun 2023)\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Design Patterns\n\n\n\n\n\n\ncausal\n\n\n\nAn overview of basic research design patterns in causal inference, modern extensions, and data management strategies to set up a causal inference initiative for success\n\n\n\n\n\nJun 7, 2023\n\n\nData Science Salon NYC (2023) | Pivigo Data Meet-Up (2021) | Toronto Data Workshop (2021)\n\n\n\n\n\n\n\n\n\n\n\n\nDataFold Data Quality Meet Up\n\n\n\n\n\n\nelt\n\n\ndata\n\n\n\nJoined a panel of speakers to discuss tips and tricks for running dbt at scale\n\n\n\n\n\nMay 12, 2023\n\n\nDataFold Data Quality Meetup (2023)\n\n\n\n\n\n\n\n\n\n\n\n\nPosit Data Science Hangout\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\n\nEach week, host Rachael Dempsey invites an accomplished data science leader to talk about their experience and answer questions from the audience. The discussion focuses mainly on the human elements of data science leadership. There’s no sales or marketing fluff, just great insights from inspiring professionals.\n\n\n\n\n\nApr 13, 2023\n\n\nPosit Data Science Hangout (2023)\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation without Experimentation\n\n\n\n\n\n\ncausal\n\n\n\nAn introduction to inverse propensity of treatment weighting for program evaluation with applications to Two Million Texans’ relational organizing campaign during the 2022 midterms\n\n\n\n\n\nMar 22, 2023\n\n\nData for Progress Data Engineering Open Mic (Mar 2023)\n\n\n\n\n\n\n\n\n\n\n\n\nTaking Flight with Shiny: a Modules-First Approach\n\n\n\n\n\n\nworkflow\n\n\nrstats\n\n\nshiny\n\n\n\nAn argument for the individual and organization-wide benefits of teaching new developers Shiny with a modules-first paradigm.\n\n\n\n\n\nMar 15, 2023\n\n\nAppsilon ShinyConf 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Data (error) Generating Process\n\n\n\n\n\n\ndata\n\n\n\nInterrogating the data generating process to devise better data quality tests.\n\n\n\n\n\nNov 12, 2022\n\n\nAirbyte move(data) (2022) | CANSSI Ontario (2022)\n\n\n\n\n\n\n\n\n\n\n\n\nThe Data Engineering Podcast: Column Names as Contracts\n\n\n\n\n\n\ndata\n\n\n\nDiscussing how column names can serve as a light-weight alternative to data catalogs and contracts and how to implement this approach with dbtplyr\n\n\n\n\n\nJan 12, 2022\n\n\nThe Data Engineering Podcast\n\n\n\n\n\n\n\n\n\n\n\n\nUIUC STAT447 (Data Science Programming) Guest Lecture\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\n\nDiscussing how to move from scripting to tool development, designing tools in enterprise, and navigating diverse data career paths\n\n\n\n\n\nNov 17, 2021\n\n\nUniversity of Illinois Urbana Champaign (2021)\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Names as Contracts\n\n\n\n\n\n\ndata\n\n\n\nExploring the benefits of using controlled vocabularies to encode metadata in column names, and demonstrations of implementing this approach with the convo R package or dbt extensions of SQL.\n\n\n\n\n\nFeb 26, 2021\n\n\nToronto Data Workshop on Reproducibility (2021) | Good Tech Fest (2021) | LA R User Group (2022)\n\n\n\n\n\n\n\n\n\n\n\n\noRganization: Design patterns for internal packages\n\n\n\n\n\n\nworkflow\n\n\npkgdev\n\n\nrstats\n\n\n\nAn overview of the unique design challenges and opportunities when building R packages for use inside of a single organization versus open-source. By using the jobs-to-be-done framework, this talk explores how internal packages can be better teammates by following specific design patterns for API design, testing, documentaiton, and more.\n\n\n\n\n\nJan 21, 2021\n\n\nrstudio::conf(2021) | csv,conf,v6 | EARL Boston 2021\n\n\n\n\n\n\n\n\n\n\n\n\nprojmgr: Managing the human dependencies of your project\n\n\n\n\n\n\nworkflow\n\n\nrstats\n\n\npkgdev\n\n\n\nA lightning talk on key features of the projmgr package which brings enables code-based planning and reporting workflows grounded in GitHub issues and milestones\n\n\n\n\n\nJul 6, 2020\n\n\nUseR!2020\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown Driven Development\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nHow and why to refactor one time analyses in RMarkdown into sustainable data products\n\n\n\n\n\nJan 30, 2020\n\n\nrstudio::conf(2020) | csv,conf,v5\n\n\n\n\n\n\n\n\n\n\n\n\nAssorted talks on designing analytical tools and communities for enterprise\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nA variety of related talks to creating innersource culture with R packages and related tools\n\n\n\n\n\nNov 1, 2017\n\n\nO’Reilly Strata NYC (2018) | data.world Afternoon of Data (2019) | IDEASS Chicago (2018) | Domino Data Pop-Up (2017)\n\n\n\n\n\n\n\n\n\n\n\n\ntidycf: Turning analysis on its head by turning cashflows on their side\n\n\n\n\n\n\nworkflow\n\n\npkgdev\n\n\nrstats\n\n\n\nA case study on building an internal R package for customer lifetime value modeling at Capital One and leading broader analyst adoption of open-source tooling and reproducible workflows through a community of practice.\n\n\n\n\n\nNov 1, 2017\n\n\nEARL Boston (2017) | RLadies Chicago (2017) | rstudio::conf(2018)\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talk/epi-pipes/index.html",
    "href": "talk/epi-pipes/index.html",
    "title": "A different type of DAG - data pipelines for epidemiology",
    "section": "",
    "text": "Quick LinksAbstractSlides\n\n\n Slides \n\n\nThis talk was part of a symposium on data science tools and opportunities for adoption in epidemiology. The full session description is provided below:\nMost applied research and education in epidemiology does not yet benefit from modern data science. Fledgling epidemiologists may receive cutting-edge education on the theory of epidemiologic methods, but remain largely untrained in how to collect data effectively, how to apply modern analytical methods to real data sets, how to reproducibly document code and results, and how to effectively work in teams in a digital workplace. Despite their own nagging concerns, they may rely on Dr. Google as their training on algorithms, document study procedures in e-mail chains, store data in spreadsheets, copy-paste analytical code, hard-code observations per person into separate variables, and manually type out estimates into results tables – only to discover that they are requested to do it all over when three study participants turn out to be ineligible for an analysis.\nThis symposium will illustrate success stories on how to efficiently practice data science in epidemiology and how to teach it along the way. There will be no exhortations how Excel is bad and that good people practice code sharing. Instead, the symposium will discuss cutting-edge approaches and real-life use cases of how modern data science has made research and teaching more efficient. The goal is for attendees to bring home a sparkling, vetted toolkit of new ideas and tools for research and teaching."
  },
  {
    "objectID": "talk/dbtplyr/index.html",
    "href": "talk/dbtplyr/index.html",
    "title": "Operationalizing Column-Name Contracts with dbtplyr",
    "section": "",
    "text": "url_video: “”\n\nQuick LinksAbstractSlidesVideo\n\n\nAt Coalesce for dbt user audience:\n Slides \n Video \nAt posit::conf for R user audience:\n Slides \n Video - posit::conf for R User Audience coming soon! \n Post - Column Name Contracts \n Post - Column Name Contracts in dbt \n Post - Column Name Contracts with dbtplyr \n\n\nComplex software systems make performance guarantees through documentation and unit tests, and they communicate these to users with conscientious interface design.\nHowever, published data tables exist in a gray area; they are static enough not to be considered a “service” or “software”, yet too raw to earn attentive user interface design. This ambiguity creates a disconnect between data producers and consumers and poses a risk for analytical correctness and reproducibility.\nIn this talk, I will explain how controlled vocabularies can be used to form contracts between data producers and data consumers. Explicitly embedding meaning in each component of variable names is a low-tech and low-friction approach which builds a shared understanding of how each field in the dataset is intended to work.\nDoing so can offload the burden of data producers by facilitating automated data validation and metadata management. At the same time, data consumers benefit by a reduction in the cognitive load to remember names, a deeper understanding of variable encoding, and opportunities to more efficiently analyze the resulting dataset. After discussing the theory of controlled vocabulary column-naming and related workflows, I will illustrate these ideas with a demonstration of the {dbtplyr} dbt package which helps analytics engineers get the most value from controlled vocabularies by making it easier to effectively exploit column naming structures while coding.\n\n\n\n\n\n\n\nComing Soon!"
  },
  {
    "objectID": "talk/data-downtime/index.html",
    "href": "talk/data-downtime/index.html",
    "title": "Data Downtime Horror Stories Panel",
    "section": "",
    "text": "In October, I joined a Halloween-themed panel along with Chad Sanderson and Joe Reis to discuss our horror stories of data quality gone wrong and how to build successful data quality strategies in large organizations. Key takeaways are summarized on Monte Carlo’s blog."
  },
  {
    "objectID": "talk/data-downtime/index.html#abstract",
    "href": "talk/data-downtime/index.html#abstract",
    "title": "Data Downtime Horror Stories Panel",
    "section": "",
    "text": "In October, I joined a Halloween-themed panel along with Chad Sanderson and Joe Reis to discuss our horror stories of data quality gone wrong and how to build successful data quality strategies in large organizations. Key takeaways are summarized on Monte Carlo’s blog."
  },
  {
    "objectID": "talk/causal-design-patterns/index.html",
    "href": "talk/causal-design-patterns/index.html",
    "title": "Causal Design Patterns",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n Video - Discussion \n Post - Causal Design Patterns \n Post - Causal Data Management \n\n\nExperimentation is a pillar of product data science and machine learning. But what can you do when experimentation is impractical, costly, risky to customer experience, or too slow to read the desired long-term results?\nWhile industry is often spoiled by their ability to AB test, the question of how to draw valid causal measurements from non-randomized data has long been a focus of many fields from epidemiology to public policy. This talk will review four common ‘design pattern’ for observational causal inference and how they can apply to industry. Exploring the assumptions, limitations, and applications of these methods will help practicing data scientists recognize opportunities to use this methods to tackle seemingly unanswerable questions they face.\nMoving beyond the basics, we will see how these building-block patterns are fueling an explosion in modern causal machine learning and discuss how to seed your organization for success with enterprise knowledge and data management."
  },
  {
    "objectID": "publication/data-eng-97-things/index.html",
    "href": "publication/data-eng-97-things/index.html",
    "title": "97 Things Every Data Engineer Should Know: Collective Wisdom from the Experts",
    "section": "",
    "text": "I contributed six chapters to the book:\n\nDevelop communities - not just code: On building developing communities along with code bases and empowering versus patronizing your data product’s customers\nGive data products a front-end with latent documentation: On low effort practices for improving data documentation and usability\nThere’s no such thing as data quality: On the value of data “fit for purpose”\nThe many meanings of missingness: On causes and consequences of null field encoding\nColumn names as contracts: On embedding metadata and performance “contracts” in column names\nData validation needs more than summary statistics: On the importance of context-informed data validation"
  },
  {
    "objectID": "project/rtistic/index.html",
    "href": "project/rtistic/index.html",
    "title": "Rtistic",
    "section": "",
    "text": "GitHub Repo\nRelated Post"
  },
  {
    "objectID": "project/rtistic/index.html#quick-links",
    "href": "project/rtistic/index.html#quick-links",
    "title": "Rtistic",
    "section": "",
    "text": "GitHub Repo\nRelated Post"
  },
  {
    "objectID": "project/rtistic/index.html#description",
    "href": "project/rtistic/index.html#description",
    "title": "Rtistic",
    "section": "Description",
    "text": "Description\nRtistic is a GitHub project template which provides a minimal wireframe R package. It is not intended to be installed but rather to serve as a starting point for individuals or groups (e.g. meet-ups, classrooms, workplaces) to collaboatively build out their own set of ggplot2 themes and palettes and various RMarkown styles.\nThe repository contains many templates, scratchpads, and step-by-step instructions for a variety of styling options including customizing:\n\nggplot2 with:\n\ncolor palettes\nthemes\n\nRmarkdown with:\n\nCSS styles\nfavicons\nfooters with logos"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Projects",
    "section": "",
    "text": "dbtplyr\n\n\n\n\n\n\npackage\n\n\n\ndbt package bringing dplyr semantics to SQL\n\n\n\n\n\nFeb 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nRtistic\n\n\n\n\n\n\npackage\n\n\n\nHackathon-in-a-box templates for custom Rmd and ggplot2 themes\n\n\n\n\n\nMay 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nprojmgr\n\n\n\n\n\n\npackage\n\n\n\nR package providing project management interface to GitHub\n\n\n\n\n\nJan 15, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/writing-a-tech-talk/index.html",
    "href": "post/writing-a-tech-talk/index.html",
    "title": "Notes on preparing a tech talk",
    "section": "",
    "text": "As part of co-organizing satRday Chicago, we wanted to offer a mentorship program for first-time speakers. To avoid totally putting the onus on our volunteer mentors, it seemed like it might be useful to break down the steps of writing and preparing for a tech talk into more modular stages where one can seek help. To that end, I wrote up the following advice to share with our speakers.\nI don’t pretend this is the only way to write a talk. I don’t pretend its the best way. I also don’t pretend I’m such an experienced speaker that I’m qualified to write a textbook on this topic. However, this is an approach I’ve found helpful and a structure that has helped me keep organized and stay productive through writer’s block or malaise. Just my two cents, and I’d love to hear yours, too!\nThanks to Patrick Miller for providing some typos and edits in the original version of this piece which was shared via GitHub."
  },
  {
    "objectID": "post/writing-a-tech-talk/index.html#preparing-a-tech-talk",
    "href": "post/writing-a-tech-talk/index.html#preparing-a-tech-talk",
    "title": "Notes on preparing a tech talk",
    "section": "Preparing a Tech Talk",
    "text": "Preparing a Tech Talk\nThere are countless ways to prepare a good tech talk.\nOne particularly popular approach is to procrastinate, board a plane to your conference without a deck, and crank your deck out in a caffeine-induced rampage at the hotel.\nOne incredibly reasonable alternative is to only apply to speak at a conference after you already have a full-fledged presentation ready and waiting. In theory, this approach likely decreases stress and preserves sanity. However, that’s all theoretical because, as far as I know, no one has ever tried it.\nThis guide is for the people that fall in between – neither the black swan of preparation nor the majority of speakers that end up giving talks in a state of sheer panic and exhaustion. In particular, if you are giving your first tech talk or helping someone else give their, this guide will hopefully add some structure to the process. (Disclaimer: lack of panic and exhaustion not guaranteed even if you follow these steps.)\nGenerally, I try to approach talk preparation in 5 main steps:\n\nDecide your call to action\nShape the story\nTest the talk\nCreate the slides\nPractice\nBonus: Don’t Overthink\n\n\nStep 0: Find a conference, find a topic, and apply!\nNaturally, a prerequisite to giving a tech talk is to find a conference and write a proposal to speak on a certain topic. That’s an interesting topic in and of itself but out of the scope of this post. You can find many good resources for thinking of talk ideas and writing abstracts online.\nFrom here on, we will assume you’ve been accepted to speak at a conference – congratulations!\n\n\nStep 1: Decide the call to action\nIt’s shockingly easy to write a compelling abstract without fully understanding your own story. (It’s unfortunately easy to give a conference talk without ever understanding this.) Naturally, the first step is to figure out what your “call to action” should be.\nDon’t think you have a call to action? Every talk does! By “action”, I don’t mean you have to be asking your audience to do anything. But inherently, you are trying to get them to think about something differently, wonder about something, or want to try something. As a speaker, you are asking for their attention, and we all know that the value of attention is at an all-time high. What is it you want to do with this valuable resource?\nRemember that your audience will be listening to a lot of presentations over the course of the conference. Whether you give a 5 minute tech talk or an hour long keynote, imagine your goal to be making one single impression so compelling that they can still recall it when they wake up the next morning. Anything else is details that they can get by referring to your slides, searching online, or reaching out to you the presenter.\nHere are some possible calls to action for common types of R talks:\n\npackage demo: you should consider using package x to solve problems like y\ntutorial: this is how you can use package x to solve problem y like y\ncase study (e.g. “how my company uses R”): when teaching a / organizing b / hiring c / facing barrier d / etc., try tips x, y, and z\ndata analysis: you should consider using method x to solve problems like y OR when thinking about problem z, you should bear in mind the interesting insight q that I found\n\nThis is not to say you cannot make more than one point. I’ve heard many successful talks along the lines of “five things to consider when designing an A/B test” or “three common mistakes when building random forests”. Your aim is still to encourage your audience to think about a problem differently whenever they next encounter it.\n\n\nStep 2: Shape the story\nOnce you know the main point you want to make, your temptation may be to start working on your slides. Please don’t. This temptation may be exacerbated by conference organizers asking for your slides ahead. Why not make the slides first and then you can always figure out what you want to say later, right? Please don’t.\nI say this piece rather passionately from personal experience. Slide making is one of the stickiest time investments in writing your presentation. Making slides too soon can result in many bad outcomes. If you decide to change the story later on, you might have to scrap hours of hard work on beautiful slides. Even worse, having slides may lock in your thinking so you resist change even as a better idea emerges.\nInstead of jumping straight into deck making, it is easier to plan a talk and seek feedback by first making a minimum viable product outline. Move from your call-to-action into a story by thinking of your talk piece-by-piece. I first tend to write “topic sentences” (thanks, SAT essay writing!) for each subpoint I want to make to tell my story. You should be able to read through these clearly and they alone should tell a logical story which ultimately emphasizes your call to action.\nAfter that, fill in sub-points or ideas you want to expand on to support each claim. As a general rule of thumb, I like to try to make no more than two or three supporting points for each topic sentence, but you mileage may vary. This depends quite a lot on the type of talk you’re giving.\nFor example, here’s the beginning of an outline I wrote a few months ago about analytical communities of practice. Not everything below was kept in the final presentation. Some of its redundant, some of its tacky, some of it I’d be embarrassed to stand up and say in front of an audience, but regardless it was a way for me to organize my ideas:\n# What do we mean when we talk about community?\n\nWe have the raw ingredients of a thriving community: a lot of people working to solve similar problems with a common set of constraints (e.g. governance) and opportunities\n\nHowever, we often don't act like a community and solve problems in silos\n\nToo often, we are missing the catalyst that brings a community to life\n\n# High-functioning communities drive outsized values \n\nThe whole is greater than the sum of the parts\n\nOn the technical side, open source software is a clear example, with phenomenal tools and resources like R and python coming from open-source\n\nAlternatively / more rudimentary: looks at ants and bee communities; each work modularly and autonomously but ultimately create a whole greater than the sum of the parts\n\n# High-functioning community overcome barriers\n\nSpecifically, {{Project Q}} is posing countless challenges as analysts are having to quickly learn new tools and systems just to do their BAU work.\n\nWe also have finite FTE devoted to solving these problems\n\nWhile we are here to help and doing as much as we can, we can also be more successful if we help each other\n\nExamples:\n- {{Team X}} has more better infrastructure rating and is sharing methods / practices\n- {{Team Y}} will have advanced SME on {{System Z}} before {{Team X}}'s migration \n\n# Our team seeks to deploy technology-driven solutions to solve human problems\n\nWe want to make it easier to share:\n- Methods \n- Code\n- Problem-Solving\n\nWe are doing this with:\n- Packages (+ contribution; methodologies & implementation on package websites)\n- Training / consulting (office hours)\n- Proactive knowledge capture (GitHub, Slack)\n\n# Rich communities require engagement\n\nThe richness of communities comes from user engagement and community ownership. \nThis can mean many different things and require a range of levels of effort and investment. \nWe want to take some time today to thank some early engagers who exemplify how to get involved.\n\n\nStep 3: Test the talk\nYou have a minimum-viable product talk, and what do we do with MVPs? You test them!\nPractice talking through your story at this stage. Assess (alone or with a practice audience member) if its functional. At minimum, it should be:\n\nroughly right length: does your talk take up roughly the right amount of time? If anything, err on the short side. It’s much worse etiquette to run over (and risk taking up another speaker’s time) than under, and this leaves time for questions\nmotivated by your call to action: does the story consistency emphasize your key point? No matter how interesting some of your tangents may feel, don’t risk confusing your audience. Ruthlessly cut out things that aren’t relevant\ncoherent: do the different pieces of your story fit together?\ncomfortable: do you feel good talking through it? Does it feel natural?\n\nIterate until you feel you have a good outline that meets all of the above criteria. As you practices at this stage, you’ll probably find yourself adjusting many of the subpoints. Ideally, as you get additional reps here, you can begin to incorporate more personal voice into your presentation. In subsequent iterations, consider whether your presentation:\n\nis interesting: are you stating a series of facts or telling a story?\nhas voice: do you sound like you’re talking to a friend (ideally) or giving a closing argument in a trial?\nfeels authentic: do you feel like you’re playing a role or acting like yourself? You’ll have enough on your mind when presenting without learning how to act.\n\n\n\nStep 4: Create your slides\nNow, finally, move towards creating your slides. There are many great books and resources already and creating impactful presentations, and I don’t pretend to have better (or as good of) advice as any of these. However, there are a few specific things I’ll call out about tech talks.\nSome things to consider at this stage:\n\nlet your story determine the number of slides you need: personally, I hate guidelines on “try to get through x slides per minute”. With some tech talks, you could easily spend a great deal of time walking through a single flowchart or example piece of code; other times you might simply want to flash up a bullet list, an impactful image, or a quote. I find it easiest to let my topic sentences guide when I should create a new slide instead of making arbitrary rules for myself.\npaint a picture; don’t write a novel: in academic or professional settings, you may have created report-like presentations which can be read and comprehended in your absence. However, for tech talks don’t feel like you have to cram your whole story onto your slides. Slides are more of an illustration that help you speak to your points or provide concrete examples of concepts you’re discussion.\nput branding at the beginning: don’t feel embarrassed with a little shameless self-promotion! Add social media handles (especially Twitter) to your title slide so conference members can opt to follow your are tweet about your talk. If you handle is hard to spell or remember, you can even consider putting them as footers on all of the slides.\nconvey your key point(s): returning to the ideas of a “call to action” and to the potential for your slides to end up on social media, remember that clean, simple, and visually interesting slides can help emphasize a key point and potentially disseminate it to a wider audience.\ndon’t fall in love with your slides: similar to jokes and ideas, you may make some truly beautiful, awesome, epic slides that absolutely do not belong in your story. It’s hard but resist the urge to keep things because they look cool or because you worked hard on them\n\n\n\nStep 5: Practice\nPractice, practice, practice! Different people clearly prefer different amounts of practice, so if you feel confident in your story, don’t practice until you are sick of hearing yourself talk and sick of thinking about your topic. That serves no one. But do practice until you feel like the story flows, like you know your transitions, and like you aren’t distracted while talking by wondering what’s coming next.\nIt may feel weird, but it’s probably courteous to practice your first few iterations by yourself. On these reps, you will still be figuring out how you want to tell the story, so an audience won’t be able to give you much constructive feedback. It’s not the best use of their time and may just make you feel more stressed.\nIn these early rounds, you might also consider recording yourself so you can play it back. This can help you perfect your phrasing and also notice things you don’t like in your tone, phrasing, etc. Some jokes may not sound as funny listening to them as they feel when saying them; sometimes things feel more fun to say than they are to hear.\nAfter a few of these practice rounds, practice in front of an audience. Everything will feel different. You might get self-conscious or start talking fast. Get your listener’s candid, honest feedback. Some of the best feedback I ever got was that I needed to stop “presenting” and just start talking as I would to another person. This made my presentation tons easier to give and far more palatable to listen to.\nPractice until you feel confident you know your story. Or until you’re bored or tired or other things come up in your life. You’ve got this thing.\n\n\nStep 6: Don’t Overthink!\nI cannot emphasize enough the extent to which all of these steps are in no way necessary to give a successful talk. Everyone has their own process, so don’t let any of this advice add make-work to your process. Also don’t stress out. Conference talks are not theatre; they are, at best, a genuine coming together of people eager to share ideas. The fact that you applied to speak because you have a story you want to tell means you’re already a long way there. My intent is only to give process and structure to anyone looking for some traction and a good way to get started."
  },
  {
    "objectID": "post/supporting-speakers/index.html",
    "href": "post/supporting-speakers/index.html",
    "title": "Notes on supporting conference speakers",
    "section": "",
    "text": "Photo by Glenn Carstens-Peters on Unsplash\nWhen I joined the team of co-organizers for satRday Chicago, Chicago supeRlady Angela Li tasked me with managing all-things speaker. This set me reflecting on the best and worst of my own speaker experiences and I began to list out highs and lows I’d experienced.\nWhen I solicited feedback and more ideas on Twitter, the topic seemed to strike a chord:\nThe following list is based on a combination of my own speaking experiences, additional feedback and ideas that were shared with me, and new lessons learned from satRday. Hopefully it will be of use to other organizers as a checklist of information worth sharing or to other speakers to inspire questions they should be asking.\nAs I certainly learned from satRday, conference organizing is hard. The point of this post is not to imply that organizers must successfully do all of these things! This was just a helpful list for me to have in mind to try to hit as many as possible.\nAdvice is broken down by time-to-conference:"
  },
  {
    "objectID": "post/supporting-speakers/index.html#tldr",
    "href": "post/supporting-speakers/index.html#tldr",
    "title": "Notes on supporting conference speakers",
    "section": "TLDR",
    "text": "TLDR\nBelow, I share a lot of specific details, but in the end it is all about communication.\nCommunicate early. Only getting information upon arrival or at the beginning of one’s session is more stressful for both hosts and speakers. Share information early and build in opportunities for speakers to engage (e.g. day before or morning of system checks versus before session or at start of speaking slot.) Also if you want anything from speakers (e.g. a certain slide template, slide submission date, etc.) tell them ASAP.\nCommunicate frequently. At the risk of being annoying, incessant, and spammy, overcommunicate. Communicate early, often and, for important things, redundantly.\nCommunicate in detail. Don’t leave things uncertain or up to the imagination. Cater to the level of detail desired by the most inquisitive or nervous participant. Others won’t mind skimming (and probably will anyway).\nCommunicate two ways. Be sure speakers have the opportunity to tell you stuff as well, ask for what they need, or decline things they don’t want (e.g. social media).\nCommunicate concisely. Angela graciously edited my speaker emails and helped me remember that speakers have lives, jobs, and responsibilities beyond their generously donated time. Share information at the right time in the right increments. Don’t overwhelm speakers with content or assume they’ve retained everything you’ve told them. Require an affirmation for anything truly critical.\nCommunicate correctly. Bad information is worse than no information. If you don’t have all the details, feel free to tell speakers that, but don’t set expectations that may not be met and cause confusion during the session."
  },
  {
    "objectID": "post/supporting-speakers/index.html#before",
    "href": "post/supporting-speakers/index.html#before",
    "title": "Notes on supporting conference speakers",
    "section": "Before Conference",
    "text": "Before Conference\nBefore the conference, make sure you have everything you need from speakers and they have everything they need from you. The goal is for no one to be at all surprised on the big day.\nInformation Collection\n\nWhen speakers are confirming participation, ask that they share additional metadata to be used in introductions. Specifically, be sure to know how to pronounce their name, their preferred pronouns, and any biographical information they wish to have mentioned\nMake sure than infrastructure (e.g. bio/photo collection, publicity, feedback, etc.) allows for co-presenters if conference allows this\nCollect consent to be included in photos / videos\n\nInformation to Share with Speakers\n\nClarify topic and time slot and whether stated session time includes questions\nSet clear expectation of speakers staying within allotted time or being cut-off, for the sake of other speakers and attendees who need to stay on track\nConfirm conference speakers as soon as possible to give them sufficient time to prepare their talk, book travel, and get permission for any sensitive material they might be covering\nEstablish and communicate a single speaker point-of-contact and provide both phone and email options to reach. This person should be the touch stone for any questions or concerns before or during the conference\nRequest that speakers upload their slides to internet, email to organizers, or bring on physical drive as a back-up in the case of computer problems\nOffer the optional opportunity for speakers to practice their talk or show their slides to a conference planner or someone else before the conference\nIn case of any changes (like a speaker time slot changing after being announced), be sure to notify affected speakers or, better yet, ask them if its alright\n\nPublicity\n\nMake speakers social media handles available on website or conference collateral so it is easy for people to live-tweet their talk or otherwise engage with their content.1\nWhen possible, offer option to record (and/or stream?) each speaker’s talk. Even if you don’t plan to mass distribute, it will be useful for them to have a copy to watch or share as they see fit."
  },
  {
    "objectID": "post/supporting-speakers/index.html#dayof",
    "href": "post/supporting-speakers/index.html#dayof",
    "title": "Notes on supporting conference speakers",
    "section": "On the Day Of the Conference",
    "text": "On the Day Of the Conference\nThroughout the conference, speakers should feel comfortable, have opportunities to relax, and get good visibility for their efforts and contributions.\nAt Check-In\n\nHave a separate check-in table for speakers so they can rest assured\nIf possible, have the single speaker point-of-contact at check-in to great speakers. This may not be possible if that person has other responsibilities, but regardless make sure that person is available via the contact methods given to speakers (e.g. text)\n\nIn Opening Remarks\n\nFoster a constructive environment by defining what does and does not constitute a question. (For example, rstudio::conf does a nice job of this, defining questions up front as a single sentence ending in a question mark.)\nBe sure to recognize speakers for the tremendous time investment that they have made\n\nThroughout the Day\n\nProvide a “speaker lounge” for speakers to step away from conference, practice presentation, or get “in the zone” as desired\nMake it easy to identify speakers with a different color of lanyards or name-tags versus attendees. This will help speakers network more effectively and be more available to interested audience members after their session"
  },
  {
    "objectID": "post/supporting-speakers/index.html#slot",
    "href": "post/supporting-speakers/index.html#slot",
    "title": "Notes on supporting conference speakers",
    "section": "During Speaking Slot",
    "text": "During Speaking Slot\nDuring the speaking slot, things should stay predictable and under control.\nShare Details about the Room Ahead of Time\nClearly communicate the following about their speaking slot to speakers via email:\n\nType of laptop connectors (e.g. HDMI, VGA) available\nType of microphones available (e.g. wearable, podium)\nSlide progression options (e.g. from laptop, clicker)\nRoom set-up (e.g. stage or flat)\nA time they can come ahead of their session for an equipment check (e.g. computer projection, microphone, etc.)\nRoom number and recommended time to arrive before session\nHow timing will be communicated during sessions (e.g. at what intervals will there be warnings? Who will give them that signal?)\n\nHelp Speakers Be Comfortable in the Environment\n\nProvide an opportunity (as communicated above) for speakers to see the room and test their equipment before their session\nEncourage session facilitators to step in and call-out “non-questions” in a positive way that keeps the session on track\nEnsure room will have water available to speakers\n\nMaintain Control of the Conference\nWhile it is good to care about your speakers, recognize that you as an organizer are also responsible for the attendee experience. Don’t be afraid to do what you need to keep things on track.\n\nHelp speakers stay on track by an active session moderator in an easy to see location giving time warnings\nEmpower session facilitators to cut-off speakers at time. This is not to be rude to one speaker but rather to be fair to all of the others\nEven better, consider purchasing a large clock or timer to hang in the room. This is easier to read and less distracting than a speaker waving their hand or a sign around"
  },
  {
    "objectID": "post/supporting-speakers/index.html#after",
    "href": "post/supporting-speakers/index.html#after",
    "title": "Notes on supporting conference speakers",
    "section": "After the Conference",
    "text": "After the Conference\nSpeakers donate quite a lot of time and effort preparing for their presentations. After the conference, your main responsibility is to continue to make sure that the effort they put towards making your conference great is recognized, appreciated, and mutually beneficial to them.\n\nEncourage speakers to write up their talk as a blog post and help them publicize\nShare their materials (decks, videos, GitHubs, etc.) to whatever extent you are able and they are comfortable\nWhen the budget allows, provide a speaker gift. This does not need to be economically valuable, but a momento and token of gratitude is a nice touch"
  },
  {
    "objectID": "post/supporting-speakers/index.html#footnotes",
    "href": "post/supporting-speakers/index.html#footnotes",
    "title": "Notes on supporting conference speakers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor satRday Chicago, I made a Twitter list of all the speakers, and Maëlle Salmon shared a way to follow them all↩︎"
  },
  {
    "objectID": "post/states-scraping-automation/index.html",
    "href": "post/states-scraping-automation/index.html",
    "title": "A Tale of Six States: Flexible data extraction with scraping and browser automation",
    "section": "",
    "text": "Like many Americans, last fall I was captivated by Professor Michael McDonald’s US Elections Project and, in particular, his daily reporting of early vote totals across all 50 states. Not only did this reporting provide fuel for anxious speculation and satiate an innate desire for reading the tea leaves, but it was also a quite a feat in data management.\nIn many discussions of modern data management, the extract-load-transform (ELT) process is offered as a solution to data silos. However, these tools largely focus on abstracting away the different REST APIs of common sources (e.g. Salesforce, Google Analytics, Facebook Ads) and destinations (e.g. BigQuery, Redshift, Snowflake). At the same time, many data science resources introduce static web page scraping as a tool in one’s toolkit but discuss less scraping websites rendered with client side JavaScript (as quite a bit of the internet is) or navigating arbitrary applications with browser automation.\nFor more “creative”, persay, sources, we need to build our own solutions. And this is particularly true when we’re attempting to access, standardize, and analyze 50 separate data sets published at different cadences, provided with different levels of granularity, partitioned along different dimensions, controlled by different levels of access and permission, and embedded in systems with vastly different underlying architecture. (Oh, and in Dr. McDonald’s case, with a good number of Twitter users starting to tweet at you if you haven’t refreshed your website by 8:02AM!)\nTo give an example of some of the diverse data sources used in the Elections Project:\nThe disparities in these data publishing formats and access patterns seemed like an interesting opportunity to compare different tools for data extraction. This posts starts out with a few trivial examples of using requests for HTTP requests and BeautifulSoup for static web scraping in order to understand the strengths and limitations for each tool and the unmet needs that browser automation helps address. We’ll then switch over to Playwright (with a touch of pytesseract) for a sampling of the types of challenges that browser automation can tackle."
  },
  {
    "objectID": "post/states-scraping-automation/index.html#nc-direct-download",
    "href": "post/states-scraping-automation/index.html#nc-direct-download",
    "title": "A Tale of Six States: Flexible data extraction with scraping and browser automation",
    "section": "NC: Direct Download",
    "text": "NC: Direct Download\n\nNorth Carolina provides one of the simplest paths to accessing paths to downloading its data. Data files live in a public S3 bucket with static file names based on the election date. Granular voter-level data can be downloaded easily with the urllib.request library.\n\nimport urllib.request\n\nurl = 'https://s3.amazonaws.com/dl.ncsbe.gov/ENRS/2020_11_03/absentee_counts_county_20201103.csv'\nurllib.request.urlretrieve(url, 'nc.csv')"
  },
  {
    "objectID": "post/states-scraping-automation/index.html#wi-direct-download-with-dynamic-url",
    "href": "post/states-scraping-automation/index.html#wi-direct-download-with-dynamic-url",
    "title": "A Tale of Six States: Flexible data extraction with scraping and browser automation",
    "section": "WI: Direct Download with Dynamic URL",
    "text": "WI: Direct Download with Dynamic URL\n\nWisconsin has similarly accessible data files available for download. However, when they update files during an election cycle, each new file is named by publishing date. So, unlike North Carolina, the URL of interest varies and it’s not altogether obvious what the most current one is.\nWe can still use requests to download this data, but it requires more caution in constructing the URL. The retrieve_date() function requires an ISO 8601 date to be passed in and attempts to construct a URL from it. Our GET request returns a status code of 500 if no such path exists, at which point we can throw an exception. Some calling program could decrement the date and try again.\n\nimport requests\nimport datetime\n\ndef retrieve_date(date): \n\n  # format dates as needed\n  dt = datetime.datetime.strptime(date, '%Y-%m-%d')\n  yyyy_mm = dt.strftime('%Y-%m')\n  mm_d_yyyy = dt.strftime('%m-%#d-%Y')\n  \n  # download csv\n  url = f'https://elections.wi.gov/sites/elections.wi.gov/files/{yyyy_mm}/AbsenteeCounts_County%20{mm_d_yyyy}.csv'\n  req = requests.get(url)\n  if req.status_code == 500:\n    raise Exception('Resource not found')\n  content = req.content\n  \n  # write to file\n  csv = open('wi.csv', 'wb')\n  csv.write(content)\n  csv.close()"
  },
  {
    "objectID": "post/states-scraping-automation/index.html#ak-web-scraping-with-beautifulsoup",
    "href": "post/states-scraping-automation/index.html#ak-web-scraping-with-beautifulsoup",
    "title": "A Tale of Six States: Flexible data extraction with scraping and browser automation",
    "section": "AK: Web Scraping with BeautifulSoup",
    "text": "AK: Web Scraping with BeautifulSoup\n\nInstead of provided a direct download, Arkansas publishes data to its election website by rendering a static HTML table with server-side PHP. As with before, we can retrieve this content with requests, but now we need to parse the output ourselves. Specifically, we want to iterate over table rows such as this:\n&lt;tr&gt;\n  &lt;td&gt;Online Delivery&lt;/td&gt;\n  &lt;td&gt;16,446&lt;/td&gt;\n  &lt;td&gt;12,026&lt;/td&gt;\n&lt;/tr&gt;\nFor this, we can use BeautifulSoup to iterate through the table and save the results to a CSV file with pandas.\n\nfrom bs4 import BeautifulSoup\nimport requests\nimport datetime\nimport pandas as pd\n\nurl = \"https://www.elections.alaska.gov/doc/info/statstable.php\"\nhtml_content = requests.get(url).text\nsoup = BeautifulSoup(html_content, \"lxml\")\ntable = soup.find(\"table\", attrs = {\"class\":\"lctbl\"})\n\ndata = []\nrows = table.tbody.find_all(\"tr\")\n\n# iterate over rows excluding first (header) and last (total)\nfor r in range(1, len(rows) - 1): \n\n  row = rows[r]\n  vals = [d.get_text() for d in row.find_all(\"td\")]\n  # convert count columns to numeric\n  vals[1] = int(vals[1].replace(',',''))\n  vals[2] = int(vals[2].replace(',',''))\n    \n  data.append(vals)\n  \n# save resulting data\ndf = pd.DataFrame(data, columns = ['channel','n_issued','n_returned'])\ndf['dt_updated'] = dt_fmt\ndf.to_csv('ak.csv', index = False)"
  },
  {
    "objectID": "post/states-scraping-automation/index.html#va-web-scraping-with-playwright",
    "href": "post/states-scraping-automation/index.html#va-web-scraping-with-playwright",
    "title": "A Tale of Six States: Flexible data extraction with scraping and browser automation",
    "section": "VA: Web Scraping with Playwright",
    "text": "VA: Web Scraping with Playwright\n\nVirginia seems superficially similar to Arkansas insomuch as data is provided in an in-browser display. However, since this display is constructed with browser-side JavaScript, it won’t appear in the content that results from a call to requests.get().\nMany different approaches exist to force JavaScript to update the DOM before accessing the source3. Arguably Microsoft’s Playwright is overly complicated for this situation, but I use it for this example since it is a flexible tool for browser automation, and the next three examples help demonstrate the variety of features it offers.\nThe following script navigates to Virginia’s website with a headless Firefox browser and then extracts vote counts stored as attributes in the bar chart tooltip. Each bar is create with script like this:\n&lt;rect x=\"95.333\" y=\"101\" width=\"7.333\" height=\"104\" \n      data-toggle=\"popever\" data-placement=\"top\" title \n      data-content=\"In-Person: 140&lt;br /&gt;Mail: 94&lt;br /&gt;Total: 234\" \n      data-original-title=\"Sep 30\"&gt;\n&lt;/rect&gt;\nAs before, results are then coerced into a pandas dataframe and written to a csv.\nThe following script defines a retrieve_county() function to parse out this information for a single county.\n\nfrom playwright.sync_api import sync_playwright\nimport datetime\nimport re\nimport pandas as pd\n\ndef retrieve_county(county, page):\n\n  # navigate to county-specific page\n  county_url = county.lower().replace(' ','-')\n  page.goto(f'https://www.vpap.org/elections/early-voting/year-2020/{county_url}-va')\n  \n  county_records = []\n  \n  for n in range(1,100):\n  \n    selector = f'#timeline g.popovers rect:nth-of-type({n})'\n    try:\n      date = page.get_attribute(selector, 'data-original-title')\n      vals = page.get_attribute(selector, 'data-content')\n    except:\n      break\n\n    # process data into tabular structure\n    vals_method = re.search('In-Person: (\\d+)&lt;br /&gt;Mail: (\\d+)&lt;br /&gt;Total: (\\d+)', vals.replace(',',''))\n    date_parse = datetime.datetime.strptime(date + ' 2020', '%b %d %Y').strftime('%Y-%m-%d')\n    county_records.append([county, date_parse, 'In-Person', vals_method.group(1)])\n    county_records.append([county, date_parse, 'Mail', vals_method.group(2)])\n    \n  return county_records\n\nAs a low fidelity but transparent example of it in operation, I show it looping over a few counties. In reality, we’d add more exception handling or save interim results separately so failures in any one county did not take down the whole process.\n\nwith sync_playwright() as p:\n\n  # set up\n  browser = p.firefox.launch()\n  context = browser.new_context(accept_downloads = True)\n  page = context.new_page()\n  \n  # iterate over counties\n  county = ['Accomack County', 'Albemarle County', 'Alexandria City']\n  records = []\n  for c in county:\n    records += retrieve_county(c, page)\n  \n  # save resulting data\n  df = pd.DataFrame(records, columns = ['county', 'date', 'channel', 'n'])\n  df.to_csv('va.csv', index = False)\n\n  # cleanup\n  page.close()\n  context.close()\n  browser.close()\n\nThis creates the following data structure:\n\n\n# A tibble: 6 x 4\n  county          date       channel       n\n  &lt;chr&gt;           &lt;date&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 Accomack County 2020-09-17 In-Person     0\n2 Accomack County 2020-09-17 Mail          0\n3 Accomack County 2020-09-18 In-Person   212\n4 Accomack County 2020-09-18 Mail          0\n5 Accomack County 2020-09-19 In-Person     0\n6 Accomack County 2020-09-19 Mail          0"
  },
  {
    "objectID": "post/states-scraping-automation/index.html#tx-browser-automation-with-playwright",
    "href": "post/states-scraping-automation/index.html#tx-browser-automation-with-playwright",
    "title": "A Tale of Six States: Flexible data extraction with scraping and browser automation",
    "section": "TX: Browser Automation with Playwright",
    "text": "TX: Browser Automation with Playwright\n\nNext up, we come to Texas. Texas, somewhat paradoxically, generously provides rich data (by day, by voting method, by county, and even by person) and yet does so in a way that is particularly tedious to access. Navigating to the data download requires selecting an election and then a voting date out of a UI before clicking a button in a Java serverlet4 that triggers the creation and downloading of a report as shown above.\nThis is where Playwright really shines. As with Virginia, it loads the Texas’ data in a headless5 browser. But beyond just opening a browser, Playwright can interact with it in the same way as a user: selecting options from menus, clicking buttons, and more.\nIn the retrieve_date() function below, I tell my browser exactly what I want it to do: go to the website, pick an election, click submit, pick a date, click submit, and then finally click a button to download data.\n\nfrom playwright.sync_api import sync_playwright\nimport datetime\n\ndef retrieve_date(date, page):\n\n  # navigate to date-specific page \n  target_date = datetime.datetime.strptime(date, '%Y%m%d')\n  target_date_str = target_date.strftime('%Y-%m-%d 00:00:00.0')\n  target_file = 'tx-' + target_date.strftime('%Y%m%d') + '.csv'\n  \n  # pick election\n  page.goto('https://earlyvoting.texas-election.com/Elections/getElectionDetails.do')\n  page.select_option('#idElection', label = \"2020 NOVEMBER 3RD GENERAL ELECTION\")\n  page.click('#electionsInfoForm button')\n  page.wait_for_selector('#selectedDate')\n  \n  # pick day\n  page.select_option('#selectedDate', value = target_date_str)\n  page.click('#electionsInfoForm button:nth-child(2)')\n  page.wait_for_selector('\"Generate Statewide Report\"')\n\n  # download report  \n  with page.expect_download() as download_info:\n    page.click('\"Generate Statewide Report\"')\n  download = download_info.value\n  download.save_as(target_file)\n\nThis function could then be called for one or more dates of interest:\n\nwith sync_playwright() as p:\n\n  browser = p.firefox.launch()\n  context = browser.new_context(accept_downloads = True)\n  page = context.new_page()\n  \n  dates = ['20201020','20201021','20201022']\n  for d in dates:\n    retrieve_date(d, page)\n\n  # cleanup\n  page.close()\n  context.close()\n  browser.close()"
  },
  {
    "objectID": "post/states-scraping-automation/index.html#ri-arbitrary-uis-with-ocr",
    "href": "post/states-scraping-automation/index.html#ri-arbitrary-uis-with-ocr",
    "title": "A Tale of Six States: Flexible data extraction with scraping and browser automation",
    "section": "RI: Arbitrary UIs with OCR",
    "text": "RI: Arbitrary UIs with OCR\n\nThe ability to navigate around a UI starts to blend the capabilities of Playwright’s browser automation with the more full-fledged concept of robotic process automation (RPA). RPA tools can similarly navigate arbitrary non-browser-based UIs to perform manual tasks with great speed. Abritrary UIs lack many of the features we’ve been using so far such as Xpaths and CSS IDs and classes to tell our tools where to do what. Instead, their often have built-in optical character recognition (OCR) to recognize buttons or input boxes “on sight”.\nPlaywright doesn’t quite have these capabilities built in natively, but it does offer users the ability to screenshot their browser. This allows us to pass the screenshot to pytesseract for OCR in a similar manner.\nThis techniques comes in handy for Rhode Island whose data is hosted in an embedded PowerBI app. The following script navigates to and screenshots the app, converts the resulting image to text, extracts the total vote count, and writes the results to PDFs.\n\nimport cv2\nimport pytesseract\nfrom playwright.sync_api import sync_playwright\nimport time\nimport re\nimport pandas as pd\npytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n\nwith sync_playwright() as p:\n\n  # set up\n  browser = p.firefox.launch()\n  context = browser.new_context(accept_downloads = True)\n  page = context.new_page()\n  page.goto('https://app.powerbigov.us/view?r=eyJrIjoiMGEwN2E0MzUtOTA0OC00ZDA3LThjMTItZDZhYTBjYjU5ZjhjIiwidCI6IjJkMGYxZGI2LWRkNTktNDc3Mi04NjVmLTE5MTQxNzVkMDdjMiJ9')\n  page.wait_for_load_state(state = 'networkidle')\n  time.sleep(30)\n  page.screenshot(path = 'ri.png')\n  \n  # cleanup\n  page.close()\n  context.close()\n  browser.close()\n\n# extract text\nimg = cv2.imread('ri.png')\ntext = pytesseract.image_to_string(img)\nn_tot = re.search('Turnout\\n\\n(\\d+)', text.replace(',','')).group(1)\nn_mail = re.search('Mail Ballots Received by BOE\\n\\n(\\d+)', text.replace(',','')).group(1)\n\n# write output\ndf = pd.DataFrame([[n_tot, n_mail]], columns = ['n_tot','n_mail'])\ndf.to_csv('ri.csv', index = False)\n\nThis creates the following data structure:\n\n\n# A tibble: 1 x 2\n   n_tot n_mail\n   &lt;dbl&gt;  &lt;dbl&gt;\n1 305724 156178"
  },
  {
    "objectID": "post/states-scraping-automation/index.html#what-next",
    "href": "post/states-scraping-automation/index.html#what-next",
    "title": "A Tale of Six States: Flexible data extraction with scraping and browser automation",
    "section": "What Next?",
    "text": "What Next?\nSimply accessing data is only the first in many steps towards unifying and analyzing it. The full scope of the US Elections Project requires far more numerous and challenging steps including:\n\nUnderstanding the exact schema and variables of each data set\nUnderstanding when historical data may be modified or corrected and adjusting accordingly\nAccounting for sudden changes to reporting formats, cadences, or locations\nObtaining data from states where the robots.txt prevents scraping (I’m looking at you, IL)\nBuilding relationships with Secretary of States’ offices where data is not publicly available\n\nRegardless, surveying what different states choose to publish and how they choose to share it provides an interesting opportunity to think about data access, usability, and available technologies for data retrieval."
  },
  {
    "objectID": "post/states-scraping-automation/index.html#footnotes",
    "href": "post/states-scraping-automation/index.html#footnotes",
    "title": "A Tale of Six States: Flexible data extraction with scraping and browser automation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTraditional tools like python’s BeautifulSoup or R’s rvest don’t play nicely with dynamic websites where client-side JavaScript is modifying the DOM↩︎\nOr so this StackExchange would suggest based on the .do extension↩︎\nSelenium and PhantomJS are popular related tools↩︎\nThe significance of this is that this button is not like clicking a link with a specific web URL.↩︎\nHeadless basically just means the browser doesn’t physically open on our computer so we don’t see it. Time and compute resources aren’t wasted on “painting” the browser to show us what is happening. However, if you enjoy watching your computer work in a “Look Mom, no hands!” sort of way, you can use the headless = False option when launching the browser.↩︎"
  },
  {
    "objectID": "post/sql-generation/index.html",
    "href": "post/sql-generation/index.html",
    "title": "Generating SQL with {dbplyr} and sqlfluff",
    "section": "",
    "text": "Declarative programming languages such as HTML, CSS, and SQL are popular because they allow users to focus more on the desired outcome than the exact computational steps required to achieve that outcome. This can increase efficiency and code readability since programmers describe what they want – whether that be how their website is laid out (without worrying about how the browser computes this layout) or how a dataset is structured (regardless of how the database goes about obtaining and aggregating this data).\nHowever, sometimes this additional layer of abstraction can introduce problems of its own. Most notably, the lack of common control flow can introduce a lot of redundancy. This is part of the motivation for pre-processing tools which use more imperative programming concepts such as local variables and for-loops to automatically generate declarative code. Common examples in the world of web development are Sass for CSS and Haml for HTML. Of course, such tools naturally come at a cost of their own by requiring developers to learn yet another tool.1\nFor R (or, specifically tidyverse) users who need to generate SQL code, recent advances in dplyr v1.0.0 and dbplyr v2.0.0 pose an interesting alternative. By using efficient, readable, and most important familiar syntax, users can generate accurate SQL queries that could otherwise be error-prone to write. For example, computing sums and means for a large number of variables. Coupled with the power of sqlfluff, an innovative SQL styler which was announced at DBT’s recent coalesce conference, these queries can be made not only accurate but also imminently readable."
  },
  {
    "objectID": "post/sql-generation/index.html#the-basic-approach",
    "href": "post/sql-generation/index.html#the-basic-approach",
    "title": "Generating SQL with {dbplyr} and sqlfluff",
    "section": "The basic approach",
    "text": "The basic approach\nIn the following example, I’ll briefly walk through the process of generating readable, well-styled SQL using dbplyr and sqlfluff.\n\nlibrary(dbplyr)\nlibrary(dplyr)\nlibrary(DBI)\n\nFirst, we would connect to our database using the DBI package. For the sake of example, I simply connect to an “in-memory” database, but a wide range of database connectors are available depending on where your data lives.\n\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = \":memory:\")\n\nAgain, for the sake of this tutorial only, I will write the palmerpenguins::penguins data to our database. Typically, data would already exist in the database of interest.\n\ncopy_to(con, palmerpenguins::penguins, \"penguins\")\n\nFor reference, the data looks like this:\n\nhead(palmerpenguins::penguins)\n\n# A tibble: 6 x 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# i 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nNow, we’re done with set-up. Suppose we want to write a SQL query to calculate the sum, mean, and variance for all of the measures in our dataset measured in milimeters (and ending in “mm”). We can accomplish this by using the tbl() function to connect to our database’s data and describing the results we want with dplyr’s elegant syntax. This is now made especially concise with select helpers (e.g. ends_with()) and the across() function.\n\npenguins &lt;- tbl(con, \"penguins\")\n\n\npenguins_aggr &lt;-\n  penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(\n    N = n(),\n    across(ends_with(\"mm\"), sum, .names = \"TOT_{.col}\"),\n    across(ends_with(\"mm\"), var, .names = \"VAR_{.col}\"),\n    across(ends_with(\"mm\"), mean, .names = \"AVG_{.col}\"),\n  )\npenguins_aggr\n\n# Source:   lazy query [?? x 11]\n# Database: sqlite 3.33.0 [:memory:]\n  species       N TOT_bill_length_mm TOT_bill_depth_mm TOT_flipper_length_mm\n  &lt;chr&gt;     &lt;int&gt;              &lt;dbl&gt;             &lt;dbl&gt;                 &lt;int&gt;\n1 Adelie      152              5858.             2770.                 28683\n2 Chinstrap    68              3321.             1253.                 13316\n3 Gentoo      124              5843.             1843.                 26714\n# i 6 more variables: VAR_bill_length_mm &lt;dbl&gt;, VAR_bill_depth_mm &lt;dbl&gt;,\n#   VAR_flipper_length_mm &lt;dbl&gt;, AVG_bill_length_mm &lt;dbl&gt;,\n#   AVG_bill_depth_mm &lt;dbl&gt;, AVG_flipper_length_mm &lt;dbl&gt;\n\n\nHowever, since we are using a remote backend, the penguins_aggr object does not contain the resulting data that we see when it is printed (forcing its execution). Instead, it contains a reference to the database’s table and an accumulation of commands than need to be run on the table in the future. We can access this underlying SQL translation with the dbplyr::show_query() and use capture.output() to convert that query (otherwise printed to the R console) to a character vector.\n\npenguins_query &lt;- capture.output(show_query(penguins_aggr))\npenguins_query\n\n[1] \"&lt;SQL&gt;\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n[2] \"SELECT `species`, COUNT(*) AS `N`, SUM(`bill_length_mm`) AS `TOT_bill_length_mm`, SUM(`bill_depth_mm`) AS `TOT_bill_depth_mm`, SUM(`flipper_length_mm`) AS `TOT_flipper_length_mm`, VARIANCE(`bill_length_mm`) AS `VAR_bill_length_mm`, VARIANCE(`bill_depth_mm`) AS `VAR_bill_depth_mm`, VARIANCE(`flipper_length_mm`) AS `VAR_flipper_length_mm`, AVG(`bill_length_mm`) AS `AVG_bill_length_mm`, AVG(`bill_depth_mm`) AS `AVG_bill_depth_mm`, AVG(`flipper_length_mm`) AS `AVG_flipper_length_mm`\"\n[3] \"FROM `penguins`\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n[4] \"GROUP BY `species`\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n\n\nAt this point, we already have a function SQL query and have saved ourselves the hassle of writing nine typo-free aggregation functions. However, since dbplyr was not written to generate “pretty” queries, this is not the most readable or well-formatted code. To clean it up, we can apply the sqlfluff linter and styler.\nAs a prerequisite, we slightly reformat the query to remove anything that isn’t native to common SQL and will confuse the linter, such as the first line of the query vector: &lt;SQL&gt;.\n\npenguins_query &lt;- penguins_query[2:length(penguins_query)]\npenguins_query &lt;- gsub(\"`\", \"\", penguins_query)\npenguins_query\n\n[1] \"SELECT species, COUNT(*) AS N, SUM(bill_length_mm) AS TOT_bill_length_mm, SUM(bill_depth_mm) AS TOT_bill_depth_mm, SUM(flipper_length_mm) AS TOT_flipper_length_mm, VARIANCE(bill_length_mm) AS VAR_bill_length_mm, VARIANCE(bill_depth_mm) AS VAR_bill_depth_mm, VARIANCE(flipper_length_mm) AS VAR_flipper_length_mm, AVG(bill_length_mm) AS AVG_bill_length_mm, AVG(bill_depth_mm) AS AVG_bill_depth_mm, AVG(flipper_length_mm) AS AVG_flipper_length_mm\"\n[2] \"FROM penguins\"                                                                                                                                                                                                                                                                                                                                                                                                                                              \n[3] \"GROUP BY species\"                                                                                                                                                                                                                                                                                                                                                                                                                                           \n\n\nAfter cleaning, we can write the results to a temp file.\n\ntmp &lt;- tempfile()\nwriteLines(penguins_query, tmp)\n\nThe current state of our file looks like this:\n\nSELECT species, COUNT(*) AS N, SUM(bill_length_mm) AS TOT_bill_length_mm, SUM(bill_depth_mm) AS TOT_bill_depth_mm, SUM(flipper_length_mm) AS TOT_flipper_length_mm, VARIANCE(bill_length_mm) AS VAR_bill_length_mm, VARIANCE(bill_depth_mm) AS VAR_bill_depth_mm, VARIANCE(flipper_length_mm) AS VAR_flipper_length_mm, AVG(bill_length_mm) AS AVG_bill_length_mm, AVG(bill_depth_mm) AS AVG_bill_depth_mm, AVG(flipper_length_mm) AS AVG_flipper_length_mm\nFROM penguins\nGROUP BY species\n\nFinally, we are ready to use sqlfluff. The lint command highlights errors in our script, and the fix command automatically fixes them (with flags --no-safety and -f requesting that it apply all rules and does not ask for permission to overwrite the file, respectively). However, note that if your stylistic preferences differ from the defaults, sqlfluff is imminently customizable via YAML.\n\nsystem(paste(\"sqlfluff lint\", tmp), intern = TRUE) \n\nWarning in system(paste(\"sqlfluff lint\", tmp), intern = TRUE): running command\n'sqlfluff lint C:\\Users\\emily\\AppData\\Local\\Temp\\RtmpOyDJYM\\file51f4757357ce'\nhad status 1\n\n\ncharacter(0)\nattr(,\"status\")\n[1] 1\n\n# intern = TRUE is only useful for the sake of showing linter results for this blog post\n# it is not needed for interactive use\n\n\nsystem(paste(\"sqlfluff fix --no-safety -f\", tmp))\n\n[1] 1\n\n\nThe results of these commands are a well-formatted and readable query.\n\nSELECT species, COUNT(*) AS N, SUM(bill_length_mm) AS TOT_bill_length_mm, SUM(bill_depth_mm) AS TOT_bill_depth_mm, SUM(flipper_length_mm) AS TOT_flipper_length_mm, VARIANCE(bill_length_mm) AS VAR_bill_length_mm, VARIANCE(bill_depth_mm) AS VAR_bill_depth_mm, VARIANCE(flipper_length_mm) AS VAR_flipper_length_mm, AVG(bill_length_mm) AS AVG_bill_length_mm, AVG(bill_depth_mm) AS AVG_bill_depth_mm, AVG(flipper_length_mm) AS AVG_flipper_length_mm\nFROM penguins\nGROUP BY species"
  },
  {
    "objectID": "post/sql-generation/index.html#a-slightly-more-realistic-example",
    "href": "post/sql-generation/index.html#a-slightly-more-realistic-example",
    "title": "Generating SQL with {dbplyr} and sqlfluff",
    "section": "A (slightly) more realistic example",
    "text": "A (slightly) more realistic example\nOne situation in which this approach is useful is when engineering features that might include many subgroups or lags. Some flavors of SQL have PIVOT functions which help to aggregate and reshape data by group; however, this can vary by engine and even those that do (such as Snowflake) require manually specifying the names of each field. Instead, our dbplyr and sqlfluff can help generate an accurate query to accomplsh this more concisely.\nNow assume we want to find the mean for each measurement separately for years 2007 through 2009. Ultimately, we want these measures organized in a table with one row per species. We can concisely describe this goal with dplyr instead of writing out the definition of each of 9 variables (three metrics for three years) separately.\n\npenguins_pivot &lt;-\n  penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize_at(vars(ends_with(\"mm\")), \n               list(in09 = ~mean(if_else(year == 2009L, ., 0)),\n                    in08 = ~mean(if_else(year == 2008L, ., 0)),\n                    in07 = ~mean(if_else(year == 2007L, ., 0)))\n               ) \npenguins_pivot\n\n# Source:   lazy query [?? x 10]\n# Database: sqlite 3.33.0 [:memory:]\n  species   bill_length_mm_in09 bill_depth_mm_in09 flipper_length_mm_in09\n  &lt;chr&gt;                   &lt;dbl&gt;              &lt;dbl&gt;                  &lt;dbl&gt;\n1 Adelie                   13.3               6.19                   65.7\n2 Chinstrap                17.3               6.47                   69.9\n3 Gentoo                   17.0               5.34                   76.4\n# i 6 more variables: bill_length_mm_in08 &lt;dbl&gt;, bill_depth_mm_in08 &lt;dbl&gt;,\n#   flipper_length_mm_in08 &lt;dbl&gt;, bill_length_mm_in07 &lt;dbl&gt;,\n#   bill_depth_mm_in07 &lt;dbl&gt;, flipper_length_mm_in07 &lt;dbl&gt;\n\n\nFollowing the same process as before, we can convert this to a SQL query.\n\nquery &lt;- capture.output(show_query(penguins_pivot))\nquery &lt;- query[2:length(query)]\nquery &lt;- gsub(\"`\", \"\", query)\ntmp &lt;- tempfile()\nwriteLines(query, tmp)\nsystem(paste(\"sqlfluff fix --no-safety -f\", tmp))\n\n[1] 1\n\n\nThe following query shows the basic results. In this case, the sqlfluff default is significantly more aggressive with identations for the CASE WHEN statements than I personally prefer. If I were to use this in practice, I could refer back to the customizable sqlfluff rules and either change their configuration or restrict rules I perceived as unaesthetic or overzealous from running.\n\nSELECT species, AVG(CASE WHEN (year = 2009) THEN (bill_length_mm) WHEN NOT(year = 2009) THEN (0.0) END) AS bill_length_mm_in09, AVG(CASE WHEN (year = 2009) THEN (bill_depth_mm) WHEN NOT(year = 2009) THEN (0.0) END) AS bill_depth_mm_in09, AVG(CASE WHEN (year = 2009) THEN (flipper_length_mm) WHEN NOT(year = 2009) THEN (0.0) END) AS flipper_length_mm_in09, AVG(CASE WHEN (year = 2008) THEN (bill_length_mm) WHEN NOT(year = 2008) THEN (0.0) END) AS bill_length_mm_in08, AVG(CASE WHEN (year = 2008) THEN (bill_depth_mm) WHEN NOT(year = 2008) THEN (0.0) END) AS bill_depth_mm_in08, AVG(CASE WHEN (year = 2008) THEN (flipper_length_mm) WHEN NOT(year = 2008) THEN (0.0) END) AS flipper_length_mm_in08, AVG(CASE WHEN (year = 2007) THEN (bill_length_mm) WHEN NOT(year = 2007) THEN (0.0) END) AS bill_length_mm_in07, AVG(CASE WHEN (year = 2007) THEN (bill_depth_mm) WHEN NOT(year = 2007) THEN (0.0) END) AS bill_depth_mm_in07, AVG(CASE WHEN (year = 2007) THEN (flipper_length_mm) WHEN NOT(year = 2007) THEN (0.0) END) AS flipper_length_mm_in07\nFROM penguins\nGROUP BY species"
  },
  {
    "objectID": "post/sql-generation/index.html#when-you-cant-connect-to-you-data",
    "href": "post/sql-generation/index.html#when-you-cant-connect-to-you-data",
    "title": "Generating SQL with {dbplyr} and sqlfluff",
    "section": "When you can’t connect to you data",
    "text": "When you can’t connect to you data\nEven if, for some reason, you cannot connect to R with your specific dataset, you may still use this approach.\nFor example, suppose we cannot connect to the penguins dataset directly, but with the help of a data dictionary we can obtain a list of all of the fields in the dataset.\n\npenguins_cols &lt;- names(palmerpenguins::penguins)\n\nIn this case, we can simple mock a fake dataset using the column names, write it to an in-memory database, generate SQL, and style the output as before.\n\n# make fake dataset ----\npenguins_mat &lt;- matrix(rep(1, length(penguins_cols)), nrow = 1)\npenguins_dat &lt;- setNames(data.frame(penguins_mat), penguins_cols)\npenguins_dat\n\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex\n1       1      1              1             1                 1           1   1\n  year\n1    1\n\n# copy to database ----\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = \":memory:\")\ncopy_to(con, penguins_dat, \"penguins_mock\")\npenguins_mock &lt;- tbl(con, \"penguins_mock\")\n\n# generate sql ----\npenguins_aggr &lt;-\n  penguins_mock %&gt;%\n  group_by(species) %&gt;%\n  summarize(\n    N = n(),\n    across(ends_with(\"mm\"), sum, .names = \"TOT_{.col}\"),\n    across(ends_with(\"mm\"), var, .names = \"VAR_{.col}\"),\n    across(ends_with(\"mm\"), mean, .names = \"AVG_{.col}\"),\n  )\n\nshow_query(penguins_aggr)\n\n&lt;SQL&gt;\nSELECT `species`, COUNT(*) AS `N`, SUM(`bill_length_mm`) AS `TOT_bill_length_mm`, SUM(`bill_depth_mm`) AS `TOT_bill_depth_mm`, SUM(`flipper_length_mm`) AS `TOT_flipper_length_mm`, VARIANCE(`bill_length_mm`) AS `VAR_bill_length_mm`, VARIANCE(`bill_depth_mm`) AS `VAR_bill_depth_mm`, VARIANCE(`flipper_length_mm`) AS `VAR_flipper_length_mm`, AVG(`bill_length_mm`) AS `AVG_bill_length_mm`, AVG(`bill_depth_mm`) AS `AVG_bill_depth_mm`, AVG(`flipper_length_mm`) AS `AVG_flipper_length_mm`\nFROM `penguins_mock`\nGROUP BY `species`\n\n\nThe only caution with this approach is that one should not use type-driven select helpers such summarize_if(is.numeric, ...) because our mock data has some erroneous types (e.g. species, island, and sex are erroneously numeric). Thus, we could generate SQL that would throw errors when applied to actual data. For example, the following SQL code attempts to sum up islands. This is perfectly reasonably given our dummy dataset but would be illogical and problematic when applied in production.\n\npenguins_mock %&gt;%\n  group_by(species) %&gt;%\n  summarize_if(is.numeric, sum) %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT `species`, SUM(`island`) AS `island`, SUM(`bill_length_mm`) AS `bill_length_mm`, SUM(`bill_depth_mm`) AS `bill_depth_mm`, SUM(`flipper_length_mm`) AS `flipper_length_mm`, SUM(`body_mass_g`) AS `body_mass_g`, SUM(`sex`) AS `sex`, SUM(`year`) AS `year`\nFROM `penguins_mock`\nGROUP BY `species`"
  },
  {
    "objectID": "post/sql-generation/index.html#caveats",
    "href": "post/sql-generation/index.html#caveats",
    "title": "Generating SQL with {dbplyr} and sqlfluff",
    "section": "Caveats",
    "text": "Caveats\nI have found this combination of tools to be useful for generating readable, typo-free queries when doing a large number of queries. However, I will end by highlighting when this may not be the best approach.\ndbplyr is not intended to generate SQL. There’s always a risk when using tools for something other than their primary intent. dbplyr is no exception. Overall, it does an excellent job translating SQL and being aware of the unique flavor of various SQL backends. However, translating between languages is a challenging problem, and sometimes the SQL translation may not be the most computationally efficient (e.g. requiring more subqueries) or semantic approach. For multistep or multitable problems, you may wish to use this approach simple for generating a few painful SQL chunks instead of your whole script.\ndbplyr is intended for you to not look at the SQL. One major benefit of dbplyr for R users is distinctly to not change languages and to benefit from a database’s compute power while staying in R. Not only is this use case not the intended purpose, you could go as far as to argue it is almost antithetical. Nevertheless, I do think there are many cases where one should preserve SQL independently; for example, you might need to do data tranformations in a production pipeline that does not run R, not wish to take on additional code dependencies, not be able to connect to your database with R, or be collaborating with non-R users.\nsqlfluff is still experimental. As the developers emphasized in their DBT talk, sqlfluff is still in its early changes and subject to change. While I’m optimistic that this only means this tool will only keep getting better, it’s possible the exact rules, configuration, flags, syntax, etc. may change. Check out the docs for the latest documentation there."
  },
  {
    "objectID": "post/sql-generation/index.html#footnotes",
    "href": "post/sql-generation/index.html#footnotes",
    "title": "Generating SQL with {dbplyr} and sqlfluff",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThat being said, for SQL dbt with Jinja templating support is an intriguing option↩︎"
  },
  {
    "objectID": "post/shiny-modules/index.html",
    "href": "post/shiny-modules/index.html",
    "title": "A beginner’s guide to Shiny modules",
    "section": "",
    "text": "Recently, I argued the case on Twitter that Shiny modules are not an advanced topic and can actually be a great way for novice Shiny developers to start building more complex applications.\nThe good people of #rstats Twitter helped me refine this thesis a bit with a few key caveats. If you already are an R user who likes to think and write functions and understand Shiny basics (i.e. the basics of reactivity come first no matter what), then modules for certain types of tasks (discussed at the end of this post) are an excellent way to up your game.\nIn fact, when I first tried to learn Shiny, the monolithic scripts and lack of function-based thinking in introductory materials was something that really tripped me up because it felt so unlike normal R programming. So, not only is it possible to learn modules early, it may actually be decidedly easier than the alternative depending on your frame of mind.\nIn this post, I walk through a toy example of building a reporting app from the flights data in the nycflights13 package to demonstrate how modules help scale basic Shiny skills. The recurring theme we will discuss are that modules help novice developers:\nIn effect, I aim to demonstrate a workflow and encourage use of modules for newer Shiny users. I do not aim to teach Shiny or module development itself, persay. For that, I recommend the Shiny documentation and Hadley Wickham’s Mastering Shiny book."
  },
  {
    "objectID": "post/shiny-modules/index.html#why-modules",
    "href": "post/shiny-modules/index.html#why-modules",
    "title": "A beginner’s guide to Shiny modules",
    "section": "Why Modules?",
    "text": "Why Modules?\nIn effect, you can think of modules as the “function-ization” of a set of Shiny UI and server elements. You may wonder why you cannot just accomplish this with the normal functions as you use in other R programming. The reason for this is a bit more technical. If you are interested, it is explained well in Mastering Shiny. However, I believe that it’s only the “why functions won’t work” part of Shiny modules that make them appear to be an advanced topic. If you see the value of writing functions, you are more than ready to take advantage of modules in app development."
  },
  {
    "objectID": "post/shiny-modules/index.html#motivating-example",
    "href": "post/shiny-modules/index.html#motivating-example",
    "title": "A beginner’s guide to Shiny modules",
    "section": "Motivating Example",
    "text": "Motivating Example\nFor the sake of argument, let’s pretend that we work for an airline and are tasked with building a basic dashboard to track various measures of travel delays against preset thresholds. We have the following requirements:\n\nLet users pick a month of interest to visualize\nFor each1 metric of interest, users should:\n\nSee a time-series plot of the average daily value of the metric\nClick a download button to download a PNG of the plot\nRead a text summary that reports the number of days the value breached the threshold\n\nThe metrics of interest are:\n\nAverage departure delay\nAverage arrival delay\nProportion of daily flights with an arrival delay exceeding 5 minutes\n\n\nThe completed application is hosted here, on shinyapps.io,2 and the underlying code can be read on GitHub.\nBelow is a preview of the final application. It isn’t going to win any beauty contests; I kept the layout and styling to a minimum so we could focus on modules in the code."
  },
  {
    "objectID": "post/shiny-modules/index.html#set-up",
    "href": "post/shiny-modules/index.html#set-up",
    "title": "A beginner’s guide to Shiny modules",
    "section": "Set-Up",
    "text": "Set-Up\n\nlibrary(shiny)\nlibrary(nycflights13)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nFor any of the explanation below to make sense, it will help to familiarize yourself with the data. We filter the flights data down to a single airline and aggregate the results by day.\n\nua_data &lt;-\n  nycflights13::flights %&gt;%\n  filter(carrier == \"UA\") %&gt;%\n  mutate(ind_arr_delay = (arr_delay &gt; 5)) %&gt;%\n  group_by(year, month, day) %&gt;%\n  summarize(\n    n = n(),\n    across(ends_with(\"delay\"), mean, na.rm = TRUE)\n    ) %&gt;%\n  ungroup()\n\nhead(ua_data)\n\n# A tibble: 6 x 7\n   year month   day     n dep_delay arr_delay ind_arr_delay\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1  2013     1     1   165      7.65     6.27          0.476\n2  2013     1     2   170     12.8      7.04          0.458\n3  2013     1     3   159      8.66    -2.76          0.357\n4  2013     1     4   161      6.84    -9.86          0.180\n5  2013     1     5   117      9.66     0.786         0.274\n6  2013     1     6   137      9.79     3.53          0.409\n\n\nI’ve also defined the plotting function that we will want to visualize a month-long timeseries of data for each metric.\n\nviz_monthly &lt;- function(df, y_var, threshhold = NULL) {\n  \n  ggplot(df) +\n    aes(\n      x = .data[[\"day\"]],\n      y = .data[[y_var]]\n    ) +\n    geom_line() +\n    geom_hline(yintercept = threshhold, color = \"red\", linetype = 2) +\n    scale_x_continuous(breaks = seq(1, 29, by = 7)) +\n    theme_minimal()\n}\n\nFor example, to visualize the average arrival delay by day for all of March and compare it to a threshhold of 10 minutes, we can write:\n\nua_data %&gt;%\n  filter(month == 3) %&gt;%\n  viz_monthly(\"arr_delay\", threshhold = 10)"
  },
  {
    "objectID": "post/shiny-modules/index.html#one-module-at-a-time",
    "href": "post/shiny-modules/index.html#one-module-at-a-time",
    "title": "A beginner’s guide to Shiny modules",
    "section": "One Module at a Time",
    "text": "One Module at a Time\nModules don’t just help organize your code; they help you organize your thinking. Given our list of requirements, it might feel overwhelming where to start. Filtering the data? Making the plots? Wiring up buttons? Inevitably, when juggling 10+ components (the reactive dataset plus a plot, button, and text summary for each of three metrics), you’re likely to introduce a bug by copy-pasting a line with the wrong id or getting nested parentheses out of whack.\nInstead, modules essentially allow your to write many simple Shiny apps and compose them together.\nFor example, we might decide that first we just want to focus on a very simple app: given a monhthly subset of the data, a metric, and a threshold of interest. Let’s write a simple text summary of the flights performance. This task seems relatively straightforward. We now know we just need to define a UI (text_ui) with a single call to textOutput(), a server (text_server) that does a single calculation and calls renderText(). Best of all, we can immediately see whether or not our “app” works by writing a minimalistic testing function (text_demo) which renders the text for a small, fake dataset.3\nI saved the follow in a file called mod-test.R:\n\n# text module ----\ntext_ui &lt;- function(id) {\n  \n  fluidRow(\n    textOutput(NS(id, \"text\"))\n  )\n  \n}\n\ntext_server &lt;- function(id, df, vbl, threshhold) {\n  \n  moduleServer(id, function(input, output, session) {\n    \n    n &lt;- reactive({sum(df()[[vbl]] &gt; threshhold)})\n    output$text &lt;- renderText({\n      paste(\"In this month\", \n            vbl, \n            \"exceeded the average daily threshhold of\",\n            threshhold,\n            \"a total of\", \n            n(), \n            \"days\")\n    })\n    \n  })\n  \n}\n\ntext_demo &lt;- function() {\n  \n  df &lt;- data.frame(day = 1:30, arr_delay = 1:30)\n  ui &lt;- fluidPage(text_ui(\"x\"))\n  server &lt;- function(input, output, session) {\n    text_server(\"x\", reactive({df}), \"arr_delay\", 15)\n  }\n  shinyApp(ui, server)\n  \n}\n\nWe can follow the same pattern to create a module for the plot itself consisting of a UI (plot_ui), a server (plot_server), and a testing function (plot_demo). This module is responsible for showing the plot of a single metric and enabling users to download it. You can see this code on GitHub in the file mod-plot.R:\nOnce again, we can run that self-contained file and then execute plot_demo() to run our mini-application. This time, it is more interactive. We can click the “Download” button and ensure that our download feature is working.\nWe don’t have an application yet, but we have two components that build pretty easily off of our basic Shiny knowledge and that we can see working before our eyes4.\n\nComposing Modules\nWe now have a text module and a plot module. However, recall for each metric of interest, we want to produce both. We could do this by simply calling these two modules one after the other in our final app, but if we want to, we can create another module that wraps our first two modules so that we can produce in single commands everything that we need for a given metric.\nWith all of the underlying plot and text module logic abstracted, our metric module definition is very clean and simple. I define it in the mod-metr.R file:\n\n# metric module ----\nmetric_ui &lt;- function(id) {\n  \n  fluidRow(\n    text_ui(NS(id, \"metric\")),\n    plot_ui(NS(id, \"metric\"))\n  )\n  \n}\n\nmetric_server &lt;- function(id, df, vbl, threshhold) {\n  \n  moduleServer(id, function(input, output, session) {\n    \n    text_server(\"metric\", df, vbl, threshhold)\n    plot_server(\"metric\", df, vbl, threshhold)\n    \n  })\n  \n}\n\nmetric_demo &lt;- function() {\n  \n  df &lt;- data.frame(day = 1:30, arr_delay = 1:30)\n  ui &lt;- fluidPage(metric_ui(\"x\"))\n  server &lt;- function(input, output, session) {\n    metric_server(\"x\", reactive({df}), \"arr_delay\", 15)\n  }\n  shinyApp(ui, server)\n  \n}\n\nAgain, we can test that these components went together as we intended by running the metric_demo() function. We will see the text from our text module on top of the plot and button from our plot module. Here’s what our module looks like so far:\n\nFor this app, this might seem like overkill, but I wanted to illustrate the ability to compose modules because it’s very useful as your application grows in complexity. In essence, everything you bundle into a module gives you a license to forget about how the next layer lower is implemented and frees up your mind to take on the next challenge.\n\n\nPutting it all together\nFinally, we are ready to write our complete application in a file called flights-app.R:\n\n# load libraries ----\nlibrary(nycflights13)\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# load resources ----\nsource(\"viz-mtly.R\")\nsource(\"mod-plot.R\")\nsource(\"mod-text.R\")\nsource(\"mod-metr.R\")\n\n# data prep ----\nua_data &lt;-\n  nycflights13::flights %&gt;%\n  filter(carrier == \"UA\") %&gt;%\n  mutate(ind_arr_delay = (arr_delay &gt; 5)) %&gt;%\n  group_by(year, month, day) %&gt;%\n  summarize(\n    n = n(),\n    across(ends_with(\"delay\"), mean, na.rm = TRUE)\n    ) %&gt;%\n  ungroup()\n\n# full application ----\nui &lt;- fluidPage(\n  \n  titlePanel(\"Flight Delay Report\"),\n  \n  sidebarLayout(\n  sidebarPanel = sidebarPanel(\n    selectInput(\"month\", \"Month\", \n                choices = setNames(1:12, month.abb),\n                selected = 1\n    )\n  ),\n  mainPanel = mainPanel(\n    h2(textOutput(\"title\")),\n    h3(\"Average Departure Delay\"),\n    metric_ui(\"dep_delay\"),\n    h3(\"Average Arrival Delay\"),\n    metric_ui(\"arr_delay\"),\n    h3(\"Proportion Flights with &gt;5 Min Arrival Delay\"),\n    metric_ui(\"ind_arr_delay\")\n  )\n)\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$title &lt;- renderText({paste(month.abb[as.integer(input$month)], \"Report\")})\n  df_month &lt;- reactive({filter(ua_data, month == input$month)})\n  metric_server(\"dep_delay\", df_month, vbl = \"dep_delay\", threshhold = 10)\n  metric_server(\"arr_delay\", df_month, vbl = \"arr_delay\", threshhold = 10)\n  metric_server(\"ind_arr_delay\", df_month, vbl = \"ind_arr_delay\", threshhold = 0.5)\n  \n}\n\nshinyApp(ui, server)\n\nNotice how few lines of code this file requires to create all of our various components and interactions! We’ve eliminated much of the dense code, nesting, and potential duplication we might have encountered if trying to write our application without modules. Whether you were trying to maintain this app or reading someone else’s code, the top-level code is accessible, semantic, and declarative. We can fairly easily infer the intent of each line and note which pieces of UI and server logic are responsible for which components.\nAdditionally, we also have all of the typical benefits afforded by functions. For example, if we were asked to change the plot download feature to download an .svg file instead of a .png, we could make a single change to the plot_server() function instead of having to change many pieces of our code for each metric.\n\n\nCaveats\nNot all modules are made alike, and in this walk-through I intentionally chose relatively easy pieces of logic to demonstrate. Note that our modules consume a reactive variable (data) from the global environment, but they themselves do not attempt to alter the global environment or exchange information between them. Both of those things are also very possible to do with modules, but they may feel a bit harder or more error prone at first. In my opinion, modules that simply consume reactives are the easiest way to start out.\nI also intentionally did not discuss more advanced features of modules or more formal and automated testing of them. These topics are covered in both Mastering Shiny and Engineering Production Grade Shiny Apps. These books also introduce ways to share modules via R packages and to organize them in Shiny app projects built with the excellent golem package (the usethis of Shiny apps)."
  },
  {
    "objectID": "post/shiny-modules/index.html#footnotes",
    "href": "post/shiny-modules/index.html#footnotes",
    "title": "A beginner’s guide to Shiny modules",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the spirit of “don’t repeat yourself”, any time you have the phrase “for each” in your requirements, it’s a strong signal that modules might make your work a good bit easier.↩︎\nIf I stop hosting the app in the future and this link does not work, the easiest way to run the app is by going to the repo, copying the code in the file flights-app-single-file.R and running it locally.↩︎\nIn Mastering Shiny, Hadley points out its a good practice to always include such a demo function for testing: https://mastering-shiny.org/scaling-modules.html#updated-app↩︎\nOf course, “working for one use case” is not a substitute for real testing, but that’s out of scope for this post↩︎"
  },
  {
    "objectID": "post/rtistic/index.html",
    "href": "post/rtistic/index.html",
    "title": "Rtistic: A package-by-numbers repo",
    "section": "",
    "text": "Last winter, I attended a holiday party at a “paint-and-sip” venue. For those unfamiliar, “paint-and-sip” is a semi-trendy cottage industry offering evenings of music, wine, and a guided painting activity. For example, my group painted sasquatch on a snowy winter’s eve:\nAs often happens, this completely unrelated thing set me thinking about R. What made painting fun when we lacked the talent and experience to excel, when it almost surely wouldn’t turn out very well, and when “failure” would be very visibly and undeniably on display? Many of these same dimensions (novelty, risk, and visibility) often intimidate new coders, yet somehow they were core to the paint-and-sip business model.\n(Admittedly, the “sipping” part may be a major confounder here, but this isn’t a post on causal inference!)\nOf course, this was nothing more than a tongue-in-cheek thought experiment, but it did bring a few observations to the front of my mind.\nIt did pique my curiosity if these principles could help introduce concepts like GitHub or package development in a friendlier and more engaging way. Over the next few months, life carried on and took me in a variety of directions, including to the Chicago R unconf where I marvelled at the passion and productivity of teams coming together for a two day hackathon. Somewhere in there, a project idea began to form."
  },
  {
    "objectID": "post/rtistic/index.html#introducing-rtistic",
    "href": "post/rtistic/index.html#introducing-rtistic",
    "title": "Rtistic: A package-by-numbers repo",
    "section": "Introducing Rtistic",
    "text": "Introducing Rtistic\nNow that you’ve indulged my musings, let me introduce my Rtistic repo on GitHub.\n\n\n\nRtistic hex logo\n\n\nStructurally, Rtistic is a package in the sense that you could install it with devtools::install_github(). However, it is not an R package in the sense that you would be sorely disappointed if you did that. Rtistic is incomplete, by intent, and always will be. It is a skeleton of an R package and intended to be used as a “hackathon-in-a-box” or an “R package cookbook”. Small groups at a meetup, office, or classroom can come together and collectively build out a package containing palettes and themes for ggplot2 and RMarkdown. Much like the paint-and-sip, it strives to be highly structured and low stakes.\n\nCaveat lector!: Rtistic continues to evolve. For example, since first writing the below, I have added support for xaringan, utilized the new GitHub template repository functionality, and further standardized file names. The description below remains accurate in spirit, but the repo README is the best resource for the most up-to-date information for actual use.\n\n\nStructure\nMuch of the boilerplate code already exists, so partipants are less likely to get caught up on some cryptic error message. Off-the-shelf, the project’s file structure looks similar to what’s shown below (additional files may be added after I post this). Files prefixed with my- are open to edits by each team.\n\n-- DESCRIPTION\n-- footer-helpers\n   |__generate-footer-logo.R\n   |__my-footer-template.html\n   |__my-logo.png\n-- inst\n   |__rmarkdown\n      |__resources\n         |__my-footer.html\n         |__my-styles.css\n-- LICENSE.md\n-- man\n   |__figures\n      |__logo.png\n   |__my_html_format.Rd\n   |__my_theme.Rd\n   |__scale_custom.Rd\n   |__test_pal.Rd\n-- NAMESPACE\n-- R\n   |__my-gg-palette.R\n   |__my-gg-theme.R\n   |__my-html-format.R\n   |__palette-infrastructure.R\n-- README.md\n-- Rtistic.Rproj\n-- scratchpad\n   |__gg-theme-demo.Rmd\n   |__rmd-theme-demo.Rmd\n-- vignettes\n   |__my-gg-theme-vignette.Rmd\n\n\n\nWhimsical…\nWith this much structure, participants can focus on the fun task of picking colors, fonts, and other theme components. Truly, a team contribution could be as minimal as defining four palettes in a file similar to the provided my-gg-palette.R file:\n\ntest_pal &lt;- c(\"green\", \"yellow\", \"orange\", \"red\") # discrete colors\ntest_pal_op &lt;- c(\"green\", \"grey50\", \"red\")        # discrete colors mapping to good/neutral/bad\ntest_pal_cont &lt;- c(\"green\", \"yellow\")             # endpoints for a continous scale\ntest_pal_div &lt;- c(\"green\", \"yellow\", \"red\")       # reference points for diverging scale\n\n\n\nFull example file below the fold\n\n#' Test palette\n#'\n#' This is a test palette inspired by stop-light colors\n#'\n#' @references https://en.wikipedia.org/wiki/Traffic_light\n#' @name test_pal\nNULL\n\n#' @name test_pal\n#' @export\n# Define disrete palette\ntest_pal &lt;- c(\n  \"#00A850\", # green\n  \"#FEEF01\", # yellow\n  \"#F58222\", # orange\n  \"#E13C29\"  # red\n)\n\n#' @name test_pal\n#' @export\n# Define opinionated discrete palette (good, neutral, bad)\ntest_pal_op &lt;- c(test_pal[1], \"grey50\", test_pal[4])\n\n#' @name test_pal\n#' @export\n# Define two colors for endpoints of continuous palette\ntest_pal_cont &lt;- c(test_pal[1], test_pal[2])\n\n#' @name test_pal\n#' @export\n# Define three colors for endpoints of diverging continuous pallete (high, middle, low)\ntest_pal_div  &lt;- c(test_pal[1], test_pal[2], test_pal[4])\n\nThese palettes can then be used in a number of functions (thanks to the R/palette-infrastructure.R file):\n\nscale_(color/colour/fill)_discrete_rtistic(palette = \"test\"): Discrete palette with optional extend parameter to interpolate more values\nscale_(color/colour/fill)_opinionated_rtistic(palette = \"test\"): Discrete palette to map to subjectively coded “good”/“bad”/“neutral” column2\nscale_(color/colour/fill)_continuous_rtistic(palette = \"test\"): Standard continous palette\nscale_(color/colour/fill)_diverging_rtistic(palette = \"test\"): Diverging continuous palette\n\nFor example, that contribution leads to the following styles:\n\n\n\n\n\n\n\n\n\nTo “level up”, participants can also use roxygen2 syntax to add documentation about their theme and showcase it by editing the my-gg-theme-vignette.Rmd template.\nThe get_rtistic_palettes() function scans the package’s namespace for anything ending in _pal to help users learn about all the available options3:\n\nRtistic::get_rtistic_palettes()\n\n[1] \"test_pal\"\n\n\nAdding actual ggplot2 and RMarkdown themes is slightly more advanced, but the core design of participants altering templates still holds. Helper functions and instructions are also provided for some of the most esoteric tasks, like encoding a logo image as a URI to be included as a custom footer of a self-contained RMarkdown.\n\n\n…but Valuable\nDespite the low barriers to entry, my hope is that there is a lot to learn with Rtistic. For example, participants might get exposure to:\n\nPackage structure: By filling in the missing pieces of an existing package, teams will navigate through an R package file structure. Whether or not teams have package building aspirations, being able to read package source code is a useful skill.4 It’s also a chance to practice good documentation with commenting and vignette writing.5\nCollaboration on GitHub: I’m convinced no one will ever understand git or GitHub by reading. The easiest way to learn forks, branches, and pull requests is to use them. Rtistic attempts to make this as easy as possible. Multiple teams should never need to edit the same file, so the likelihood of a big merge conflict problem is low.6\nggplot2 styling: ggplot2 is intuitive; however, compared to most of the tidyverse, it is intuitive in the sense of having a rich philosophical basis - not in the sense of “barely needed to read the docs because I can infer it all”. Unfortunately, this difference can make it feel unintuitive to new users. Exposure to ggplot2 theme options pays dividends in getting the most from the package.\nHTML / CSS: R users more typically come from math and stats backgrounds (versus tech), knowledge of front-end web development can be limited. However, users of any of the *down packages7 can benefit a lot by having even a cursory understanding of these tools to take advantage of RMarkdown’s customization capabilities.\n\nIf this seems like too much at once, the modular nature of the project makes it easy to strip down. If GitHub is out of scope, participants can locally change a small number of files and share them with an organizer for compilation. If ggplot2 and HTML/CSS are too much to tackle at once, either the plot or RMarkdown theme pieces can be ignored.\n\n\nA Level Playing Field\nFor the more complex tasks of ggplot2 and RMarkdown themes, the scratchpad/ directory provides additional context. This directory contains two RMarkdown files with working examples of:\n\nRMarkdown styling with CSS for participants to edit and re-knit. This can help build intuition over how RMarkdown is translated into different HTML tags and how those respond to CSS\nAs many ggplot2 theme options as I could possibly fit in one plot with the goal of exposing participants to all of the possible options\n\nThe plot, in particular, is hopefully illuminating since the theme is in no way intended to be coherent. Each design choice uses wildly different fonts, colors, and alignments to make it very clear what line of code corresponds to each element:\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\ni Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\n\n\nCode here\n\n\nlibrary(ggplot2)\n# sample data for plot ----\npoints &lt;- \n  data.frame(\n    x = rep(1:10,3), \n    y = rep(1:10,3), \n    z = sort(rep(letters[1:2], 15)),\n    w = rep(letters[3:4], 15)\n    )\n# ggplot using many theme options ----\nggplot(data = points, \n       mapping = aes(x = x, y = y, col = factor(x))) + \n  geom_point(size = 5) + \n  facet_grid(w ~ z, switch = \"y\") +\n  theme(\n    \n    plot.background = element_rect(fill = \"lightyellow\"),\n    plot.title = element_text(size = 30, hjust = 0.25),\n    plot.subtitle = element_text(size = 20, hjust = 0.75, color = \"mediumvioletred\", family = \"serif\"),\n    plot.caption = element_text(size = 10, face = \"italic\", angle = 25),\n    \n    panel.background = element_rect(fill = 'lightblue', colour = 'darkred', size = 4),\n    panel.border = element_rect(fill = NA, color = \"green\", size = 2),\n    panel.grid.major.x = element_line(color = \"purple\", linetype = 2),\n    panel.grid.minor.x = element_line(color = \"orange\", linetype = 3),\n    panel.grid.minor.y = element_blank(),\n    \n    axis.title.x = element_text(face = \"bold.italic\", color = \"blue\"),\n    axis.title.y = element_text(family = \"mono\", face = \"bold\", size = 20, hjust = 0.25),\n    axis.text = element_text(face = \"italic\", size = 15),\n    axis.text.x.bottom = element_text(angle = 180), # note that axis.text options from above are inherited\n    \n    strip.background = element_rect(fill = \"magenta\"),\n    strip.text.y = element_text(color = \"white\"),\n    strip.placement = \"outside\",\n    \n    legend.background = element_rect(fill = \"orangered4\"), # generally will want to match w plot background\n    legend.key = element_rect(fill = \"orange\"),\n    legend.direction = \"horizontal\",\n    legend.position = \"bottom\",\n    legend.justification = \"left\",\n    legend.title = element_text(family = \"serif\", color = \"white\"),\n    legend.text = element_text(family = \"mono\", face = \"italic\", color = \"limegreen\")\n    \n  ) +\n  labs(title = \"test title\",\n       subtitle = \"test subtitle\",\n       x = \"my x axis\",\n       y = \"my y axis\",\n       caption = \"this is a caption\",\n       col = \"Renamed Legend\") \n\n\n\n\nTake it for a Spin!\nComplete information and more tactical step-by-step instructions for both organizers and participants can be found on the Rtistic GitHub repo. Please let me know if you check it out. I welcome any and all feedback!"
  },
  {
    "objectID": "post/rtistic/index.html#footnotes",
    "href": "post/rtistic/index.html#footnotes",
    "title": "Rtistic: A package-by-numbers repo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs in the ever popular “draw an owl” meme↩︎\nI will admit that the opinionated scale likely is not useful or relevant in many cases. However, since corporate color palettes are one place I can imagine Rtistic being useful, I decided to include it since having a predefined mapping of “good” and “bad” colors (e.g. for profits and losses). I also think it’s a nice discrete parallel to the more typical diverging scale for continuous-valued variables.↩︎\nThis allows discoverability without having to maintain any centralized list of palettes since, for logistical reasons, Rtistic is designed to be completely modular.↩︎\nFor example, I recently started using a package with very little documentation but an excellent test suite. Once I figured this out, I was able to use it a lot more efficiently by learning from the test examples.↩︎\nAnother conviction of mine is that these skills should be used for most R projects – not just packages. R’s strong conventions and culture for documentation are one of its hallmarks.↩︎\nObviously, merge conflicts are an important concept to understand, but as a day-one intro, “how this ideally works” is easier to grasp.↩︎\nOr Shiny, flexdashboard, etc.↩︎"
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html",
    "href": "post/rmarkdown-driven-development/index.html",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "",
    "text": "RMarkdown is an excellent platform for capturing narrative analysis and code to create reproducible reports, blogs, slides, books, and more. One benefit of RMarkdown is its abilities to keep an analyst in the “flow” of their work and to capture their thought process along the way. However, thought processes are rarely linear; as a result, first-draft RMarkdown scripts rarely are either. This is fine for some individual analysis and preliminary exploration but can significantly decrease how understandable and resilient an RMarkdown will be in the future.\nAs a proponent of the concept of “analysis engineering”1, I tend to think of each RMarkdown as having a “data product” (an analytical engine calibrated to answer some specific question) nestled within it. Surfacing this tool just requires a touch of forethought before beginning an analysis and a bit of clean-up afterwards.\nIn this post, I describe RMarkdown Driven Development (RmdDD?): a progression of stages between a single ad-hoc RMarkdown script and more advanced and reusable data products like R projects and packages. This approach has numerous benefits.\nFirst, the promise of re-use and reproducibility helps justify any incremental time expenditure on constructing a single analysis with “good practices”. Second, for newer project users or package developers, it hopefully helps emphasize that the learning gap between being a useR and developeR is much small than it may seem2. Finally, I have found that this approach to package development leads to very intuitive, user-friendly packages. Essentially, you, a humble user, have conducted rigorous UX research before you, the tool developer, ever shows up!\nIn the broadest terms, I see an evolution of the following steps:\n\nRemoving Troublesome Elements\nRearrange the Chunks\nReduce Duplication with Functions\nMigrate from RMarkdown to R Project (Modularize your files!)\nConvert your R Project to a Package\n\nEach of these is discussed in more detail below. I also want to emphasize that this sequence is not a recommendation that every RMarkdown needs to become a project or a package. Clearly, that is just silly and in many cases causes unneccesary fragmentation and overhead. However, I believe imagining a spectrum between a single-file RMarkdown to a full-functioning R package is helpful to conscientiously make the decision where to draw the line. “Optimal stopping” at different stages may be the subject of a future post."
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#introduction",
    "href": "post/rmarkdown-driven-development/index.html#introduction",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "",
    "text": "RMarkdown is an excellent platform for capturing narrative analysis and code to create reproducible reports, blogs, slides, books, and more. One benefit of RMarkdown is its abilities to keep an analyst in the “flow” of their work and to capture their thought process along the way. However, thought processes are rarely linear; as a result, first-draft RMarkdown scripts rarely are either. This is fine for some individual analysis and preliminary exploration but can significantly decrease how understandable and resilient an RMarkdown will be in the future.\nAs a proponent of the concept of “analysis engineering”1, I tend to think of each RMarkdown as having a “data product” (an analytical engine calibrated to answer some specific question) nestled within it. Surfacing this tool just requires a touch of forethought before beginning an analysis and a bit of clean-up afterwards.\nIn this post, I describe RMarkdown Driven Development (RmdDD?): a progression of stages between a single ad-hoc RMarkdown script and more advanced and reusable data products like R projects and packages. This approach has numerous benefits.\nFirst, the promise of re-use and reproducibility helps justify any incremental time expenditure on constructing a single analysis with “good practices”. Second, for newer project users or package developers, it hopefully helps emphasize that the learning gap between being a useR and developeR is much small than it may seem2. Finally, I have found that this approach to package development leads to very intuitive, user-friendly packages. Essentially, you, a humble user, have conducted rigorous UX research before you, the tool developer, ever shows up!\nIn the broadest terms, I see an evolution of the following steps:\n\nRemoving Troublesome Elements\nRearrange the Chunks\nReduce Duplication with Functions\nMigrate from RMarkdown to R Project (Modularize your files!)\nConvert your R Project to a Package\n\nEach of these is discussed in more detail below. I also want to emphasize that this sequence is not a recommendation that every RMarkdown needs to become a project or a package. Clearly, that is just silly and in many cases causes unneccesary fragmentation and overhead. However, I believe imagining a spectrum between a single-file RMarkdown to a full-functioning R package is helpful to conscientiously make the decision where to draw the line. “Optimal stopping” at different stages may be the subject of a future post."
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#a-taxonomy-of-rmarkdown-chunks",
    "href": "post/rmarkdown-driven-development/index.html#a-taxonomy-of-rmarkdown-chunks",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "A Taxonomy of RMarkdown Chunks",
    "text": "A Taxonomy of RMarkdown Chunks\nFirst, let’s consider a high-level classification of RMarkdown chunks by different purposes. For this discussion, I disregard finer points of different language engines and chunk options and only consider different RMarkdown components by what they contribute to the final output.\n\nInfrastructure: These chunks set up the environment in which the RMarkdown is rendered. This includes code that helps add functions to your enviornment (e.g. library(), source()), load data (e.g. functions from readr or data.table::fread() or functions calling APIs or databases), or define analysis parameters (e.g. hardcoded values that are somehow used to change behavior later in the script)\nWrangling: These chunks contain code to transform the data that you’ve loaded into information desired for analysis/discussion (e.g. tidying, aggregation, model fitting)\nCommunication: These chunks help to produce meaningful output for a report such as data visualizations and summary tables"
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#remove",
    "href": "post/rmarkdown-driven-development/index.html#remove",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "(1) Remove Troublesome or Unsustainable Elements",
    "text": "(1) Remove Troublesome or Unsustainable Elements\n\nFirst and foremost, some things should (almost) never be in an RMarkdown. Even if you don’t want to make your code more readable and reusable, please consider doing the following to preserve your own sanity and security.\n\nDo not hardcode passwords. This is a good general principle of scripts, but especially important in an RMarkdown where they might accidentally “leak” into the rendered output (e.g. HTML) in a non-visible way without your realizing. If something in your file absolutely requires a password, one approach is to create a parameter and supply this upon knitting.\nDo not hardcode values, especially late in the script. Use parameters to bring important variables that alter subsequent input to the top.\nDo not hardcode absolute file paths. No one else has your specific set up of files, nor are you likely to if you change computers. This can lead to a lot of frustration.3 Try to use relative paths to reference any external files (e.g. data) being brought in to your report. This is significantly easier once the analysis becomes and R project. At minimum, move any brittle dependencies like this to the top of your script where they will at least be found more quickly to debug.\nDo not do complicated database queries. For simple RMarkdown files, sometimes it may be convenient to use the sql language engine and query a database. However, at least in my experience, it is generally not the best approach to attempt database queries in your RMarkdown. Sometimes, queries can take a long time to run, and you do not want to do this every time you find a typo or tweak some plot styling and want to reknit your document. Consider making your data pull a separate script and read the results into your RMarkdown.\nDon’t litter. Resist the temptation to save everything you tried that didn’t work and isn’t part of your analysis or narrative. Note that I’m not advocating against transparency around reporting all the tests you ran, all the models you attempted to fit, etc. More precisely, don’t leave half written code just in case you want to try to make some specific plot or graph later. Don’t let your RMarkdown become a “junk drawer” or take misadvantage of the ability to store unneeded code with the eval = FALSE chunk option.\nDon’t load unneccesary libraries. Often, you may add library loads in exploratory analysis “just in case” or have tried out using one package before deciding on a different approach. After you’ve removed the “litter” discussed previously, also be sure to clean up any side-effects of such litter. There isn’t a huge cost to excess library loads except that it can be confusing to users and raises (however slightly) the chance of a NAMESPACE conflict. You might also then cause some other user to install extra packages unneccesarily which, while not tragic, is inconvenient if there is no actual benefit.\n\n\nBonus Points I mostly keep this discussion to structure, not content, but even for the simplest of all markdowns, please attempt to format your code nicely. Even a little whitespace and meaningful variables names (e.g. not my_data_2) can go a long way. For help here, check our the lintr package or use Ctrl+Shift+A in RStudio.\n\nAfter stripping your RMarkdown of these basic risks and complications, next we can move on to restructuring."
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#rearrange",
    "href": "post/rmarkdown-driven-development/index.html#rearrange",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "(2) Rearrange the Chunks",
    "text": "(2) Rearrange the Chunks\n\nThe lowest hanging fruit in cleaning up an RMarkdown document is to rearrange your chunks so that all Infrastructure chunks come first and most all Data Wrangling chunks immediately follow. This has two benefits: exposing dependencies and frontloading errors.\nMoving all Infrastructure chunks to the beginning of your RMarkdown makes it clear what dependencies your RMarkdown has. Instantly upon opening a file, a new analyst can understand what libraries and external files it requires. This is optimal over a case where some obscure library in the penultimate chunk of some large RMarkdown which almost runs to completion before erroring out due to a missing dependency.\nThe rationale for front-loading a lot of wrangling chunks is similar. Because these chunks are the most computationally intense, they are most likely to throw errors. Having them at the beginning of your file means you will learn about your errors sooner; having all of them together will make it easier to debug. Additionally, in my experience, these are the chunks I most often want to edit, so it’s efficient not to have to scroll through code for plots and tables simply to find these chunks\nOf course, in this step, do be careful of being too “prescient”. If some of your data wrangling chunks are motivated by certain output and discussion later in your file, it may be confusing for this computation to be removed from its context and placed at the beginning. I’m reasonably convicted about this advice for creating standardized reporting frameworks, but I caution that the best structure becomes far more subjective for more creative analyses.\n\nBonus Points Now that your data load chunks are right at the top of your script, consider adding validation for the data getting loaded. Is it the same form that your code expects? Does it have the right variable names and types? Does it meet any logical checks or assumptions you deem necessary? A few good packages for this are validate and assertr. Depending on your project, you could put these in a separate code chunk with the chunk option include = FALSE so that data validation can be run manually or include them in you script to throw errors and prevent attempt to render incorrect data structures."
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#functions",
    "href": "post/rmarkdown-driven-development/index.html#functions",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "(3) Reduce Duplication with Functions",
    "text": "(3) Reduce Duplication with Functions\n\nReorganization offers other clear benefits, one of which is that code with similar purposes ends up physically closer in your document. This may make it easier for you to spot similarities. As you notice similarities in different chunks of wrangling or reporting chunks, keep in mind the rule of three. That is, similar code repeated multiple times should be turned into a function.\nFor example, I often encounter situations where I need to produce the same plots or tables for many different groups. While an analyst is in exploratory mode, they might reasonably copy-paste such code, edit some key parameters, and eagerly proceed to analyzing the results. Converting this code to functions makes is significantly easier to test and maintain. It also has the benefit of converting Reporting code into Infrastructure code which can be moved to the top of the RMarkdown, with the previously described benefits. Generally, I define any local functions after my library() and source() commands.\nFor good advice on how to modularize your functions, including naming4 and behavior, I recommend Maëlle Salmon’s blog post and rOpenSci’s package development guide.\n\nBonus Points Now that you have functions, it’s a good time to think about testing them. You could add a few tests of your functions in a chunk with include = FALSE as described in the last section for data validation. One helpful package here is testthat. Even if you don’t include these in your RMarkdown, save any informal tests you run in a text file. They will be useful if you decide to turn your analysis all the way into a package"
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#project",
    "href": "post/rmarkdown-driven-development/index.html#project",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "(4) Convert Your RMarkdown to Project",
    "text": "(4) Convert Your RMarkdown to Project\n\nAt this point, your RMarkdown ideally has clear requirements (library, file, and data dependencies) and minimal duplicated code. Particularly if you find that you are source()ing in a large number of files, defining many local functions, or reading in many different datasets, its worth considering whether to convert your single file RMarkdown into an R Project.\nR Projects are a special type of folder on your computer which automatically regards itself as the working directory. This has important benefits for shareability because it enables you to use relative paths.\nAdditionally, by using a standardized file structure within your project5, you can help others easily navigate your repository. If an entire team or organization decides on a single file structure convention, collaborators can easily navigate each others folders and have a good intuition where to find a specific file in someone else’s project.\nThere are many recommendations online for folder structures, but when modularizing an RMarkdown, I tend to use the following:\n\nanalysis: RMarkdown files that constitute my final narrative output\nsrc: R scripts that contain useful helper functions or other set-up tasks (e.g. data pulls)\ndata: Raw data - this folder should be considered “read only”!\noutput: Intermediate data objects created in my analysis. Typically, I save these as RDS files (with saveRDS and readRDS)\ndoc: Any long form documentation or set-up instructions I wish to include\next: Any miscellaneous external files that take part in my analysis\n\n\nBonus Points Now that you have a project, consider taking a more proactive stance on package management to ensure the future user has correct / compatible versions of any packages on which your project relies. As of writing this, RStudio’s new package management solution renv is still in development, but follow that project for more details!"
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#package",
    "href": "post/rmarkdown-driven-development/index.html#package",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "(5) Convert Your Project to a Package",
    "text": "(5) Convert Your Project to a Package\n\nOne of the beautiful things about R packages is their shocking simplicity. Before I wrote my first package, I always assumed that there was some mystical step change in the level of effort between writing everyday R code and writing a package. This is a misconception I frequently hear repeated by newer R users. In reality (admittedly, painting with a very broad brush), writing an R package is simply the art of putting things (R files) where they belong (in the right folders.)\nThere is a clear parallel between the “project” folders described above and the folders typically found in an R package.6\n\nFunctions in the project’s src/ folder can move to a package’s R/ folder\nRMarkdown documents in the analysis/ folder are worked examples of how your code solves a real problems - much like the contents of a vignettes/ folder\nIf you take the time to clean up those worked analyses, strip out any problem-specific context, and perhaps provide more user instructions, this could turn into an RMarkdown template which you can ship via the inst/ folder\nYour data/ (and possibly output/) folder(s) contain datasets that fuel your examples. This is the type of sample data that one often includes in a data/ folder\n\nIt’s also instructive to notice the differences between projects and packages. Following the description above, the biggest notable gaps are the lack of unit tests (which would live in the tests/ folder) and function documentation (which can be autogenerated from roxygen2 comments and live in docs/). These can easily be added when converting your project to a package.\nMore importantly, I encourage project developers to consider whether they shouldn’t be including this level of detail even if they never plan to make a package. Even if your functions are just for you, don’t you want to trust them and remember how to use them when you revisit your project next quarter or year?"
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#thats-a-wrap",
    "href": "post/rmarkdown-driven-development/index.html#thats-a-wrap",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "That’s a Wrap!",
    "text": "That’s a Wrap!\nVoila! Now our humble RMarkdown has evolved all the way into a complete R package. Again, is this always necessary? Definitely not. Is it always advisable? No. But regardless of what end-state you want to leave your RMarkdown in, it’s always worth considering how it can be groomed towards becoming a more sustainable data product."
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#updates",
    "href": "post/rmarkdown-driven-development/index.html#updates",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "Updates",
    "text": "Updates\nSince I initially wrote about this topic, I have since continued to explore this idea. Below is a list of more related content:\n\nSlides and video from my rstudio::conf 2020 talk\nFollow-up post giving a technical appendix of related R packages and tools that may help with this process"
  },
  {
    "objectID": "post/rmarkdown-driven-development/index.html#footnotes",
    "href": "post/rmarkdown-driven-development/index.html#footnotes",
    "title": "RMarkdown Driven Development (RmdDD)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHilary Parker explores this issue at length in her PeerJ preprint Opinionated Analysis Development and related tweet about what to call analysts who embrace such practices↩︎\nI would argue that RMarkdown offers a whole new way to “slide gradually into programming”. This has always been a key tennet of R and its predecesors, as Roger Peng explored in his 2018 useR! keynote Teaching R to New Users - From tapply to the Tidyverse ↩︎\nAnd Jenny Bryan will set your laptop on fire↩︎\nWhich is all to say that the illustration is, as it sounds, for illustrative purposes only. Please, please, please do not ever actually name a function anything as uninformative as viz_fx() or viz_fx2()!↩︎\nOne of the “Good Enough” practices recommended in this excellent article by Data Carpentry↩︎\nIn fact, one could use a package structure for their project to begin with. This idea is brought to life with the idea of creating an R research compendium. I personally prefer to keep the naming conventions separate to distinguish between projects and packages, and to use the name that is most appropriate for the contents in each case. ↩︎"
  },
  {
    "objectID": "post/resource-roundup-reproducible-research/index.html",
    "href": "post/resource-roundup-reproducible-research/index.html",
    "title": "Resource Round-Up: Reproducible Research Edition",
    "section": "",
    "text": "Photo by Sharon McCutcheon on Unsplash\nI frequently talk, think, and tweet about how open science principles apply to data analysts and scientists in industry. However, despite my interest in reproducibility, my process for sharing my favorite articles on this topic has been anything but reproducible – repeatedly searching my browser history, past “Sent” mails, or simply typing out the same summaries over and over.\nGoing forward, this post serves as a brief annotates bibliography for some of my favorite reads. Did I miss something? Please let me know!\nThere is little to say in way of introduction that is not already covered eloquently in the links below. However, I’ll share just a few words of context on why this issue interests me, outside the walls of academia. Like research scientists, industrial data users are often more incented to deliver quick business wins and use their knowledge to provide actionable and domain-specific insights. How this work gets done often gets less attention.\nThe consequences of in industry may be less dire (e.g. inability to reproduce an AB test on click-through rates versus clinical trial results), but they are no less tangible. The lack of building sustainable tools keeps knowledge siloed within teams and leads to a lot of unnecessary makework. However, despite the obvious benefits of working in code-based, shareable, and reproducible ways, prioritization (“always time to do it again; never time to do it right”) and training can be persistent disincentives for action.\nFortunately, the scientific world has a headstart on solving these problems and figuring out which computing practices deliver practitioners the most bang for the buck. For more on that, keep reading."
  },
  {
    "objectID": "post/resource-roundup-reproducible-research/index.html#on-scale",
    "href": "post/resource-roundup-reproducible-research/index.html#on-scale",
    "title": "Resource Round-Up: Reproducible Research Edition",
    "section": "On Scale",
    "text": "On Scale\nrOpenSci Packages: Development, Maintenance, and Peer Review\nBrooke Anderson, Scott Chamberlain, Anna Krystalli, Lincoln Mullen, Karthik Ram, Noam Ross, Maëlle Salmon, Melina Vidoni\nrOpenSci curates and maintains a collection of R packages that support various aspects of open and reproducible research. It accomplishes this feat by a fine-tuned set of processes and standards that enable an army of volunteers to make short-term, modular contributions. This effective mobilization of the community provides sustainable oversight without taxing too much of any single contributor’s time or resources.1\nrOpenSci is a fascinating case study of intentional infrastructure and operations creating an extremely high-functioning organization. However, rOpenSci is not just an aspirational gold-standard. Their “rOpenSci Packages” ebook describes the practices which has helped them succeed and contains much concrete advice for other open-source or innersource aggregators to use.\nThe book is their guide of advise for designing, developing, reviewing, and maintaining R packages. Beyond a mere checklist of rules or style guides, it contains much practical advise and wisdom for simultaneously sustaining open source tools and their surrounding communities.\nUnderstanding the InnerSource Checklist\nSilona Bonewald, PayPal\n“InnerSource” is the corporate world’s term of choice for reproducibility and “open” (within the bounds of that company) source.2 This guide from PayPal confronts head-on the challenges of building and InnerSource culture and the necessary structure for it to succeed in an enterprise setting.\nWhile some of this book pertains more to software development, two ideas of particular resonated with me from an analytical tooling perspective: the Trusted Commiter role and of passive documentation.\nThe discussion of the Trusted Commiter focuses largely around incentives for internal maintainers. Maintainer burnout is a common phenomenon in open source. In InnerSource, this can be further compounded by traditional performance management structures not being fully prepared to understand and reward the contributions of internal maintainers versus their peers doing glossy new projects. PayPal suggests rotations in and our of these roles at fixed durations.\nPassive documentation is another challenge imported from the open source world and exacerbated in the corporate setting. The key question here is how is knowledge from helping a userbase captured over time? In the “wild”, we have forums like StackExchange which make past questions easily discoverable. However, InnerSource risks more of this help moving to private messages or the proverbial watercooler. To mitigate this, the book recommends being proactive about using formal channels of communication like Slack."
  },
  {
    "objectID": "post/resource-roundup-reproducible-research/index.html#big-picture-perspectives",
    "href": "post/resource-roundup-reproducible-research/index.html#big-picture-perspectives",
    "title": "Resource Round-Up: Reproducible Research Edition",
    "section": "Big Picture Perspectives",
    "text": "Big Picture Perspectives\nSetting the Default to Reproducible\nVictoria Stodden\nVictoria Stodden, currently an Associate Professor at the University of Illinois Urbana-Champaign’s School of Informatics, is one of the main “faces” of computational reproducibility. I’ve enjoyed many of her talks, slides, and papers on the topic, and would happily recommend any of them.\nThe language in this article is more pedagogical than any of the others linked here, so this isn’t the most engaging introductory read. In fact, I tend to recommend it purely for the Appendix which provides a taxonomy of different levels of reproducibility. Stodden’s spectrum of reproducibility is a useful benchmark to created a shared language and common yardstick for how reproducible your own or your group’s work really is.\nThe five-level system ranges from reviewable (methodology described) to fully open and reproducible. From an industry perspective, I imagine there to be a sixth level of being “extensible” – that is, open and reproducible work designed in a way conducive to future addition and modification.3\nEverything Hertz Podcast: #69 Open Science Tools with Brian Nosek\nFor the more podcast-inclined, Everything Hertz is a fun, irreverent conversation that captures the zeitgeist of the open science movement. This episode is especially relevant as the hosts are joined by guest Brian Nosek, a social psychology professor and the cofounder and director of the University of Virginia’s Center for Open Science.\nThis clip assumes listeners have more background knowledge on the “reproducibility crisis” and the open science movement than the readings listed above. Also unlike the other readings, its far less tactical with no specific, actionable recommendations on the technical side of getting started. However, it is still an essential piece of an open science “starter kit” due to its thoughtful discussion of the real barriers to adoption.\nNosek spends a good deal of time examining the sociocultural and technological infrastructure needed for change to occur: common acknowledgment of the problem and appropriate change-making institutions. The first is relatively easy to come by, both in science and business; it’s easy to look at what’s in front of us and see its flaws. The latter is much harder. Nosek discusses the criticality of both engineering the right technological infrastructure to enable adoption of better practices (such as the Open Science Foundation has provided) and reshaping incentives so following new processes is attractive (or, at minimum, not destructive.)"
  },
  {
    "objectID": "post/resource-roundup-reproducible-research/index.html#footnotes",
    "href": "post/resource-roundup-reproducible-research/index.html#footnotes",
    "title": "Resource Round-Up: Reproducible Research Edition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEarlier this year, I had a great experience taking part in this process as a review of tradestatistics↩︎\nClearly, two movements about open knowledge sharing and collaboration must be developed separately under different names. Obligatory xkcd↩︎\nFor example, on episode 67 of the Data Stories podcast, Hadley Wickham discusses how concerted efforts to make `ggplot2 more extensible paid massive dividends↩︎"
  },
  {
    "objectID": "post/resource-roundup-causal/index.html",
    "href": "post/resource-roundup-causal/index.html",
    "title": "Resource Round-Up: Causal Inference",
    "section": "",
    "text": "Photo by Sharon McCutcheon on Unsplash\nIn my post on causal design patterns, I argue that these techniques are currently underutilized in industry because (at least, in part) they are so often couched in domain-specific language. Fortunately, the past few years have seen an explosion of fantastic resources and tools to help practitioners more readily learn and apply these methods. Below, I link some of my favorite readings. To help prioritize which resources (or which chapters within them) will be most relevant to you, please see my previous post for an “advertisement” and overview of some of the main techniques."
  },
  {
    "objectID": "post/resource-roundup-causal/index.html#free-books",
    "href": "post/resource-roundup-causal/index.html#free-books",
    "title": "Resource Round-Up: Causal Inference",
    "section": "Free Books",
    "text": "Free Books\n\nCausal Inference: What If by Miguel Hernan and Jamie Robins\n\nWritten from epidemiology perspective\nNice “model free” section introduces causal intuition\nCode supplement in R, python, SAS, Stata\n\nIntroduction to Causal Inference by Brady Neal\n\nWritten from ML perspective including “advanced” topics such as Bayesian networks, causal discovery\nBuilds strong theoretical basis with graphical and probabilistic proofs\nBook complemented by video lectures\n\nCausal Inference: the Mixtape by Scott Cunningham\n\nWritten from economics perspective\nProvides great insights into the history and relevance of different methods in economics literature\nIncludes interactive R code chunks to run as you read\n\nThe Effect: An Introduction to Research Design and Causality by Nick Huntington-Kline\n\nWritten from the economics perspective\nTakes a holistic approach to research design with rich examples from literature\n\nImpact Evaluation in Practice by Gertler, Martinez, Premand, Rawlings, Vermeersch of the World Bank\nHandbook of Field Experiments by Ahbijit Banerjee and Esther Duflo\n\nTechnically a type of experimentation not causal inference\nHowever, the real world challenges of field (versus clinical) research creates some nice “blended” methodologies.\nFor example encouragement designs are closely related to instrumental variable methods. These may have been inadvertently conducted in your business strategy and be available in historical data."
  },
  {
    "objectID": "post/resource-roundup-causal/index.html#course-material",
    "href": "post/resource-roundup-causal/index.html#course-material",
    "title": "Resource Round-Up: Causal Inference",
    "section": "Course Material",
    "text": "Course Material\n\nIntroduction to Causal Inference by Brady Neal\n\nML perspective including Bayesian networks, causal discovery\nSlides and video lectures to go along with book linked above\n\nProgram Evaluation by Andrew Heiss\n\nPublic policy perspective\nSlides and assignments\n\nCausal Inference (Propensity Score) Tutorial from UseR!2020 by Lucy D’Agostino McGowan and Malcolm Barrett\n\nEpidemiology perspective focused on propensity-score methods\nVideo tutorial with R code on GitHub"
  },
  {
    "objectID": "post/resource-roundup-causal/index.html#survey-papers-blogs",
    "href": "post/resource-roundup-causal/index.html#survey-papers-blogs",
    "title": "Resource Round-Up: Causal Inference",
    "section": "Survey Papers & Blogs",
    "text": "Survey Papers & Blogs\nSurveys\n\nUsing Causal Inference to Improve the Uber User Experience on the Uber Engineering blog\nThe Impact of Machine Learning on Economics by Susan Athey\nThe State of Applied Econometrics - Causality and Policy Evaluation by Susan Athey and Guido Imbens\n\nDeeper Dives\n\nCurated List of Recent Advances by Christine Cai\nPolicy Evaluation in COVID: fantastic survey of diff-in-diff and event study methods by Noah Haber, et al\n\nPropensity Score Focused\n\nUnderstanding propensity score weighting by Lucy D’Agostino McGowan\nTo Balance or Not To Balance by Ivan Diaz and Joseph Kelly (Unofficial Google Data Science Blog)"
  },
  {
    "objectID": "post/resource-roundup-causal/index.html#miscellaneous-advanced-topic-talks",
    "href": "post/resource-roundup-causal/index.html#miscellaneous-advanced-topic-talks",
    "title": "Resource Round-Up: Causal Inference",
    "section": "Miscellaneous Advanced Topic Talks",
    "text": "Miscellaneous Advanced Topic Talks\n\nCausal Science 2020 Meeting talks\nSynthetic Control lecture by Alberto Abadie\nHeterogenous Treatment Effects lecture by Susan Athey\nPersonalized Treatment Effect Estimation by Heidi Seibold"
  },
  {
    "objectID": "post/resource-roundup-causal/index.html#other-introductory-books",
    "href": "post/resource-roundup-causal/index.html#other-introductory-books",
    "title": "Resource Round-Up: Causal Inference",
    "section": "Other Introductory Books",
    "text": "Other Introductory Books\n\nLearning Microeconometrics with R by Christopher Adams\nMastering Metrics and Mostly Harmless Econometrics by Joshua Angrist and Jorn-Steffen Pischke\nThe Book of Why by Judea Pearl"
  },
  {
    "objectID": "post/quarto-comms/raw-analysis.html",
    "href": "post/quarto-comms/raw-analysis.html",
    "title": "Demo Notebook",
    "section": "",
    "text": "import requests\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import *"
  },
  {
    "objectID": "post/quarto-comms/raw-analysis.html#data-retrieval",
    "href": "post/quarto-comms/raw-analysis.html#data-retrieval",
    "title": "Demo Notebook",
    "section": "Data Retrieval",
    "text": "Data Retrieval\n\nres = []\nurl = 'https://www.federalregister.gov/api/v1/documents.json?fields[]=document_number&fields[]=excerpts&fields[]=page_length&fields[]=president&fields[]=publication_date&fields[]=raw_text_url&fields[]=signing_date&fields[]=title&fields[]=toc_subject&fields[]=topics&per_page=20&conditions[publication_date][gte]=2005-01-20&conditions[presidential_document_type][]=executive_order&conditions[president][]=george-w-bush&conditions[president][]=barack-obama&conditions[president][]=donald-trump&conditions[president][]=joe-biden&conditions[president][]=donald-trump'\n\nwhile url != '':\n\n    # retrieve results\n    response = requests.get(url)\n    response.raise_for_status()\n    res_temp = response.json()\n    res += [res_temp]\n\n    # get next url\n    url = res_temp.get('next_page_url','')"
  },
  {
    "objectID": "post/quarto-comms/raw-analysis.html#data-cleaning",
    "href": "post/quarto-comms/raw-analysis.html#data-cleaning",
    "title": "Demo Notebook",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\ndf_res = (\n    pl.DataFrame(res)\n    .select('results')\n    .explode('results')\n    .unnest('results')\n    .unnest('president')\n)\ndf_res.glimpse()\n\nRows: 969\nColumns: 11\n$ document_number         &lt;str&gt; '2025-14212', '2025-14217', '2025-14218', '2025-13925', '2025-12961', '2025-12962', '2025-12774', '2025-12775', '2025-12505', '2025-12506'\n$ excerpts               &lt;null&gt; None, None, None, None, None, None, None, None, None, None\n$ page_length             &lt;i64&gt; 4, 3, 3, 3, 2, 2, 3, 2, 1, 5\n$ identifier              &lt;str&gt; 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump'\n$ name                    &lt;str&gt; 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump'\n$ publication_date        &lt;str&gt; '2025-07-28', '2025-07-28', '2025-07-28', '2025-07-23', '2025-07-10', '2025-07-10', '2025-07-09', '2025-07-09', '2025-07-03', '2025-07-03'\n$ raw_text_url            &lt;str&gt; 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14212.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14217.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14218.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/23/2025-13925.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12961.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12962.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12774.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12775.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12505.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12506.txt'\n$ signing_date            &lt;str&gt; '2025-07-23', '2025-07-23', '2025-07-23', '2025-07-17', '2025-07-07', '2025-07-07', '2025-07-03', '2025-07-03', '2025-06-30', '2025-06-30'\n$ title                   &lt;str&gt; 'Accelerating Federal Permitting of Data Center Infrastructure', 'Preventing Woke AI in the Federal Government', 'Promoting the Export of the American AI Technology Stack', 'Creating Schedule G in the Excepted Service', 'Ending Market Distorting Subsidies for Unreliable, Foreign-Controlled Energy Sources', 'Extending the Modification of the Reciprocal Tariff Rates', \"Establishing the President's Make America Beautiful Again Commission\", 'Making America Beautiful Again by Improving Our National Parks', 'Establishing a White House Office for Special Peace Missions', 'Providing for the Revocation of Syria Sanctions'\n$ toc_subject             &lt;str&gt; None, 'Government Agencies and Employees', None, 'Government Agencies and Employees', None, None, None, None, None, None\n$ topics           &lt;list[null]&gt; [], [], [], [], [], [], [], [], [], []\n\nRows: 969\nColumns: 11\n$ document_number         &lt;str&gt; '2025-14212', '2025-14217', '2025-14218', '2025-13925', '2025-12961', '2025-12962', '2025-12774', '2025-12775', '2025-12505', '2025-12506'\n$ excerpts               &lt;null&gt; None, None, None, None, None, None, None, None, None, None\n$ page_length             &lt;i64&gt; 4, 3, 3, 3, 2, 2, 3, 2, 1, 5\n$ identifier              &lt;str&gt; 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump'\n$ name                    &lt;str&gt; 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump'\n$ publication_date        &lt;str&gt; '2025-07-28', '2025-07-28', '2025-07-28', '2025-07-23', '2025-07-10', '2025-07-10', '2025-07-09', '2025-07-09', '2025-07-03', '2025-07-03'\n$ raw_text_url            &lt;str&gt; 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14212.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14217.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14218.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/23/2025-13925.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12961.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12962.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12774.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12775.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12505.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12506.txt'\n$ signing_date            &lt;str&gt; '2025-07-23', '2025-07-23', '2025-07-23', '2025-07-17', '2025-07-07', '2025-07-07', '2025-07-03', '2025-07-03', '2025-06-30', '2025-06-30'\n$ title                   &lt;str&gt; 'Accelerating Federal Permitting of Data Center Infrastructure', 'Preventing Woke AI in the Federal Government', 'Promoting the Export of the American AI Technology Stack', 'Creating Schedule G in the Excepted Service', 'Ending Market Distorting Subsidies for Unreliable, Foreign-Controlled Energy Sources', 'Extending the Modification of the Reciprocal Tariff Rates', \"Establishing the President's Make America Beautiful Again Commission\", 'Making America Beautiful Again by Improving Our National Parks', 'Establishing a White House Office for Special Peace Missions', 'Providing for the Revocation of Syria Sanctions'\n$ toc_subject             &lt;str&gt; None, 'Government Agencies and Employees', None, 'Government Agencies and Employees', None, None, None, None, None, None\n$ topics           &lt;list[null]&gt; [], [], [], [], [], [], [], [], [], []\n\n\n\n\n# lookup table\n\nterm = [1,2,1,2,1,1,2]\nstart_year = [2001, 2005, 2009, 2013, 2017, 2021, 2025]\nstart_date = [f'{year}-01-20' for year in start_year]\nend_date = start_date[1:] + ['2029-01-20']\npresident_lname = ['Bush']*2 + ['Obama']*2 + ['Trump'] + ['Biden'] + ['Trump']\ndf_lkup = pl.DataFrame([term, start_year, start_date, end_date, president_lname], \n                       schema = ['term','start_year','start_date', 'end_date', 'president_lname'],\n                       orient = 'col')\ndf_lkup\n\n\n\nshape: (7, 5)\n\n\n\nterm\nstart_year\nstart_date\nend_date\npresident_lname\n\n\ni64\ni64\nstr\nstr\nstr\n\n\n\n\n1\n2001\n\"2001-01-20\"\n\"2005-01-20\"\n\"Bush\"\n\n\n2\n2005\n\"2005-01-20\"\n\"2009-01-20\"\n\"Bush\"\n\n\n1\n2009\n\"2009-01-20\"\n\"2013-01-20\"\n\"Obama\"\n\n\n2\n2013\n\"2013-01-20\"\n\"2017-01-20\"\n\"Obama\"\n\n\n1\n2017\n\"2017-01-20\"\n\"2021-01-20\"\n\"Trump\"\n\n\n1\n2021\n\"2021-01-20\"\n\"2025-01-20\"\n\"Biden\"\n\n\n2\n2025\n\"2025-01-20\"\n\"2029-01-20\"\n\"Trump\"\n\n\n\n\n\n\n\n\ndf_clean = (\n    df_res\n    .with_columns(\n        president_lname = pl.col('identifier')\n                            .str.extract('[A-Za-z ]+-([A-Za-z]+$)', group_index = 1)\n                            .str.to_titlecase(),\n    )\n    # there's a bug in polars join_asof() at the time of writing...\n    .join(df_lkup, on = 'president_lname')\n    .filter( pl.col('signing_date') &gt;= pl.col('start_date'))\n    .filter( pl.col('signing_date') &lt;  pl.col('end_date'))\n    .with_columns( pres_term = pl.concat_str( pl.col('president_lname'), pl.lit(' - '), pl.col('term')) )\n)\ndf_clean.glimpse()\n\nRows: 952\nColumns: 17\n$ document_number         &lt;str&gt; '2025-14212', '2025-14217', '2025-14218', '2025-13925', '2025-12961', '2025-12962', '2025-12774', '2025-12775', '2025-12505', '2025-12506'\n$ excerpts               &lt;null&gt; None, None, None, None, None, None, None, None, None, None\n$ page_length             &lt;i64&gt; 4, 3, 3, 3, 2, 2, 3, 2, 1, 5\n$ identifier              &lt;str&gt; 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump'\n$ name                    &lt;str&gt; 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump'\n$ publication_date        &lt;str&gt; '2025-07-28', '2025-07-28', '2025-07-28', '2025-07-23', '2025-07-10', '2025-07-10', '2025-07-09', '2025-07-09', '2025-07-03', '2025-07-03'\n$ raw_text_url            &lt;str&gt; 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14212.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14217.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14218.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/23/2025-13925.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12961.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12962.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12774.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12775.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12505.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12506.txt'\n$ signing_date            &lt;str&gt; '2025-07-23', '2025-07-23', '2025-07-23', '2025-07-17', '2025-07-07', '2025-07-07', '2025-07-03', '2025-07-03', '2025-06-30', '2025-06-30'\n$ title                   &lt;str&gt; 'Accelerating Federal Permitting of Data Center Infrastructure', 'Preventing Woke AI in the Federal Government', 'Promoting the Export of the American AI Technology Stack', 'Creating Schedule G in the Excepted Service', 'Ending Market Distorting Subsidies for Unreliable, Foreign-Controlled Energy Sources', 'Extending the Modification of the Reciprocal Tariff Rates', \"Establishing the President's Make America Beautiful Again Commission\", 'Making America Beautiful Again by Improving Our National Parks', 'Establishing a White House Office for Special Peace Missions', 'Providing for the Revocation of Syria Sanctions'\n$ toc_subject             &lt;str&gt; None, 'Government Agencies and Employees', None, 'Government Agencies and Employees', None, None, None, None, None, None\n$ topics           &lt;list[null]&gt; [], [], [], [], [], [], [], [], [], []\n$ president_lname         &lt;str&gt; 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump'\n$ term                    &lt;i64&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n$ start_year              &lt;i64&gt; 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025\n$ start_date              &lt;str&gt; '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20'\n$ end_date                &lt;str&gt; '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20'\n$ pres_term               &lt;str&gt; 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2'\n\nRows: 952\nColumns: 17\n$ document_number         &lt;str&gt; '2025-14212', '2025-14217', '2025-14218', '2025-13925', '2025-12961', '2025-12962', '2025-12774', '2025-12775', '2025-12505', '2025-12506'\n$ excerpts               &lt;null&gt; None, None, None, None, None, None, None, None, None, None\n$ page_length             &lt;i64&gt; 4, 3, 3, 3, 2, 2, 3, 2, 1, 5\n$ identifier              &lt;str&gt; 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump'\n$ name                    &lt;str&gt; 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump'\n$ publication_date        &lt;str&gt; '2025-07-28', '2025-07-28', '2025-07-28', '2025-07-23', '2025-07-10', '2025-07-10', '2025-07-09', '2025-07-09', '2025-07-03', '2025-07-03'\n$ raw_text_url            &lt;str&gt; 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14212.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14217.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14218.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/23/2025-13925.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12961.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12962.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12774.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12775.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12505.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12506.txt'\n$ signing_date            &lt;str&gt; '2025-07-23', '2025-07-23', '2025-07-23', '2025-07-17', '2025-07-07', '2025-07-07', '2025-07-03', '2025-07-03', '2025-06-30', '2025-06-30'\n$ title                   &lt;str&gt; 'Accelerating Federal Permitting of Data Center Infrastructure', 'Preventing Woke AI in the Federal Government', 'Promoting the Export of the American AI Technology Stack', 'Creating Schedule G in the Excepted Service', 'Ending Market Distorting Subsidies for Unreliable, Foreign-Controlled Energy Sources', 'Extending the Modification of the Reciprocal Tariff Rates', \"Establishing the President's Make America Beautiful Again Commission\", 'Making America Beautiful Again by Improving Our National Parks', 'Establishing a White House Office for Special Peace Missions', 'Providing for the Revocation of Syria Sanctions'\n$ toc_subject             &lt;str&gt; None, 'Government Agencies and Employees', None, 'Government Agencies and Employees', None, None, None, None, None, None\n$ topics           &lt;list[null]&gt; [], [], [], [], [], [], [], [], [], []\n$ president_lname         &lt;str&gt; 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump'\n$ term                    &lt;i64&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n$ start_year              &lt;i64&gt; 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025\n$ start_date              &lt;str&gt; '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20'\n$ end_date                &lt;str&gt; '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20'\n$ pres_term               &lt;str&gt; 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2'\n\n\n\n\ndf_agg = (\n    df_clean\n    .group_by('pres_term')\n    .agg(\n        n_eo_issued = pl.col('document_number').count(),\n        start_date = pl.col('publication_date').min()\n    )\n)\n\ndf_agg.sort('start_date')\n\n\n\nshape: (6, 3)\n\n\n\npres_term\nn_eo_issued\nstart_date\n\n\nstr\nu32\nstr\n\n\n\n\n\"Bush - 2\"\n118\n\"2005-01-31\"\n\n\n\"Obama - 1\"\n148\n\"2009-01-26\"\n\n\n\"Obama - 2\"\n130\n\"2013-02-19\"\n\n\n\"Trump - 1\"\n220\n\"2017-01-24\"\n\n\n\"Biden - 1\"\n162\n\"2021-01-25\"\n\n\n\"Trump - 2\"\n174\n\"2025-01-28\"\n\n\n\n\n\n\n\n\ndf_cumul = (\n    df_clean\n    .with_columns( cs.ends_with('date').cast(pl.Date) )\n    .with_columns(\n        day_since_start = ( pl.col('signing_date') - pl.col('start_date') ).dt.total_days(),\n        term_label = pl.concat_str( pl.lit('('), pl.col('start_date').dt.year(), pl.lit(')'),\n                                    pl.lit('\\n'),\n                                    pl.col('president_lname') )\n    )\n    .group_by('pres_term', 'term_label', 'day_since_start', 'signing_date')\n    .len()\n    .with_columns( n_cumul = pl.col('len').cum_sum().over('pres_term', order_by = 'day_since_start') )\n)\ndf_cumul.sort('pres_term', 'day_since_start').glimpse()\n\nRows: 711\nColumns: 6\n$ pres_term        &lt;str&gt; 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1'\n$ term_label       &lt;str&gt; '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden'\n$ day_since_start  &lt;i64&gt; 0, 1, 2, 5, 6, 7, 8, 13, 15, 21\n$ signing_date    &lt;date&gt; 2021-01-20, 2021-01-21, 2021-01-22, 2021-01-25, 2021-01-26, 2021-01-27, 2021-01-28, 2021-02-02, 2021-02-04, 2021-02-10\n$ len              &lt;u32&gt; 9, 8, 2, 2, 1, 2, 1, 3, 1, 1\n$ n_cumul          &lt;u32&gt; 9, 17, 19, 21, 22, 24, 25, 28, 29, 30\n\nRows: 711\nColumns: 6\n$ pres_term        &lt;str&gt; 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1'\n$ term_label       &lt;str&gt; '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden', '(2021)\\nBiden'\n$ day_since_start  &lt;i64&gt; 0, 1, 2, 5, 6, 7, 8, 13, 15, 21\n$ signing_date    &lt;date&gt; 2021-01-20, 2021-01-21, 2021-01-22, 2021-01-25, 2021-01-26, 2021-01-27, 2021-01-28, 2021-02-02, 2021-02-04, 2021-02-10\n$ len              &lt;u32&gt; 9, 8, 2, 2, 1, 2, 1, 3, 1, 1\n$ n_cumul          &lt;u32&gt; 9, 17, 19, 21, 22, 24, 25, 28, 29, 30\n\n\n\n\n# plot not included in final; the first plot you make often isn't the best!\n# for example, this one has censored data because (as of writing in 2025, Trump 2 is not complete)\n\n(\nggplot(df_cumul.group_by('term_label').agg( pl.col('n_cumul').max() )) +\naes(x = 'term_label', y = 'n_cumul', fill = 'term_label') +\ngeom_col() +\nlabs(title = 'Executive Orders Signed by Term', x = '', y = '') +\ntheme_538() +\nscale_color_brewer('qual', palette = 'Set1') +\ntheme(legend_position = 'none')\n)\n\n\n\n\n\n\n\n\n\n(\nggplot(df_cumul) +\naes(x = 'day_since_start', y = 'n_cumul', color = 'term_label') +\ngeom_line() +\nlabs(\n    title = 'Executive Orders Signed by Days in Office',\n    x = 'Days since Term Start', \n    y = 'EOs Issued', \n    color = 'Term') +\ntheme_538() +\nscale_color_brewer('qual', palette = 'Set1') +\ntheme(legend_position = 'bottom')\n)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\n\ndf_pit = (\n  df_cumul\n  .with_columns( max_dss = pl.when( pl.col('signing_date') == pl.col('signing_date').max() )\n                             .then( pl.col('day_since_start') )\n                             .otherwise( None )\n                             .max()\n  )\n  .group_by('term_label', 'max_dss')\n  .agg( \n    n_ttl_pit = pl.when( pl.col('day_since_start') &lt;= pl.col('max_dss') ).then(pl.col('n_cumul')).otherwise(None).max(),\n    n_ttl = pl.col('n_cumul').max(),\n   )\n   .with_columns( \n      n_ttl = pl.when( pl.col('term_label') == pl.col('term_label').max())\n                .then(None)\n                .otherwise( pl.col('n_ttl')) )\n  .sort('term_label')\n)\n\nn_days_in_office = df_pit.get_column('max_dss')[0]\n\n(\n  GT(df_pit)\n  .cols_hide('max_dss')\n  .tab_header(\n    title = 'Executive Orders Issued by Term',\n    subtitle = f'Normalized for first {n_days_in_office} days in office' +\n               f' ({n_days_in_office * 100 / (365*4 + 1):.1f}% of term)')\n  .tab_spanner(\n        label=\"Count\",\n        columns= cs.contains(\"ttl\")\n    )\n  .cols_label(\n    term_label = 'Term',\n    n_ttl_pit = 'Point in time',\n    n_ttl = 'Full term'\n  )\n  .cols_align('center')\n  .sub_missing(missing_text=\"-\")\n  .opt_row_striping()\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExecutive Orders Issued by Term\n\n\nNormalized for first 184 days in office (12.6% of term)\n\n\nTerm\nCount\n\n\nPoint in time\nFull term\n\n\n\n\n(2005) Bush\n13\n118\n\n\n(2009) Obama\n22\n148\n\n\n(2013) Obama\n14\n130\n\n\n(2017) Trump\n42\n220\n\n\n(2021) Biden\n52\n162\n\n\n(2025) Trump\n174\n-\n\n\n\n\n\n\n        \n\n\n            \n\n\n\nI have some thoughts…"
  },
  {
    "objectID": "post/quarto-auth-netlify/index.html",
    "href": "post/quarto-auth-netlify/index.html",
    "title": "Role-Based Access Control for Quarto sites with Netlify Identity",
    "section": "",
    "text": "Literate programming tools like R Markdown and Quarto make it easy to convert analyses into aesthetic documents, dashbaords, and websites for public sharing. But what if you don’t want your results too public?\nI recently was working on a project that required me to set up a large number of dashboards with similar content but different data for about 10 small, separate organizations. As I considered by tech stack, I found that many Quarto users were asking similar questions, but understandably the Quarto team had no one slam-dunk answer because authentication management (a serving / hosting problem) would be a substantial scope creep beyond the goals and core functionality of Quarto (an open-source publishing system).\nAfter evaluating my options, I found the best solution for my use case was role-based access controls with Netlify Identity. In this post, I’ll briefly describe how this solution works, how to set it up, and some of the pros and cons."
  },
  {
    "objectID": "post/quarto-auth-netlify/index.html#demo",
    "href": "post/quarto-auth-netlify/index.html#demo",
    "title": "Role-Based Access Control for Quarto sites with Netlify Identity",
    "section": "Demo",
    "text": "Demo\nUsing a minimal Netlify Identity set-up, you can be up and running with the following UX in about 10 minutes. For this post, I show the true “minimum viable deployment”, although the styling and aesthetics could be made much fancier.\nWhen users first visit your site’s homepage, they will be prompted that they need to sign-up or login to continue.\n\nIf users navigate to any other part of the site before logging in, they’ll receive an error message prompting them to return to the home screen. (This could be customized as you would a 404 Not Found error.)\n\nAfter clicking either button, an in-browser popup modal allows them to sign up, login in, or request forgotten credentials.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe example above shows the option to create a custom login or use Google to authenticate. Netlify also allows for the options to use other free (e.g. GitHub, GitLab) or paid (e.g. Okta) third-party login services.\nFor new signups, Netlify can automatically trigger confirmation emails with customized content based on a templated text or HTML file in your repository.\nOnce logged in, the homepage then offers the option to log back out.\n\nOtherwise, users can then proceed to the rest of the site as if it were publicly available."
  },
  {
    "objectID": "post/quarto-auth-netlify/index.html#set-up",
    "href": "post/quarto-auth-netlify/index.html#set-up",
    "title": "Role-Based Access Control for Quarto sites with Netlify Identity",
    "section": "Set Up",
    "text": "Set Up\nThe basics of how Netlify Identity works are described at length in this blog post. If you decide to implement this solution, I recommend reading those official documents for a more robust mental model. In short, Netlify Identity works by attaching a token to each user after they log in. This user-specific token can be assigned different roles on the backend, and depending on which roles a user has, they can be redirected to (or gated from) seeing different content.\nSetting up Netlify Identify requires a few small tweaks throughout your site:\n\nAdd Javascript to each page to handle the JSON Web Tokens (JWTs) set by Identity. This is done most easily through the _quarto.yml\nConfigure site redirects to response to the JWTs. This is contained in its own _redirects file\nEnsure you have a user interface that allows users to sign up and login, thus changing their JWTs and access. I put this in my index.qmd\n\nThen, finally, within the Netlify admin panel, you must:\n\nConfigure the user signup workflow (e.g. by invitation, open sign-up)\nAssign users to roles that determine what content they can see\nOptionally, enable third-party forms of authentication (e.g. Google, GitHub)\n\nLet’s take these one at a time.\n\nConfigure Role Authentiation\nNetlify maintains an Identity widget that handles recognizing authenticated users and their roles from their JWTs. To inject this Javascript snippet into every page, open the _quarto.yml file and add the Javascript snippet to the include-in-header: key under the HTML format, e.g.:\nformat:\n  html: \n    include-in-header: \n      text: |\n        &lt;script type=\"text/javascript\" src=\"https://identity.netlify.com/v1/netlify-identity-widget.js\"&gt;&lt;/script&gt;\n        &lt;script&gt;\n        window.netlifyIdentity.on('login', (user) =&gt; {\n        window.netlifyIdentity.refresh(true).then(() =&gt; {\n          console.log(user);\n        });\n        });\n        window.netlifyIdentity.on('logout', (user) =&gt; {\n        window.location.href = '/login';\n        });\n        window.netlifyIdentity.init({ container: '#netlify' });\n        &lt;/script&gt;\nNote, the official widget is injected using the src field of the first script tag.\n\n\nConfigure Site Redirects\nNext, create a _redirects file at the top level of your project (or open the existing file) and add the following lines:\n/login /\n/*  /:splat  200!  Role=admin\n/site_libs/* /site_libs/:splat 200!\n/   /        200!\n/*  /  401!\nSyntax for the _redirects file is described here, but basically each line defines a rule with the structure:\n&lt;what was requested&gt; &lt;where to go&gt; &lt;the result to give&gt; &lt;conditional on role&gt;\nAnd, like a case when statement, the first “matching” rule dominates.\nSo, the example above can roughly be read in English as:\nIf users go to the /login page, take them back to home\nIf users try to go anywhere else on my site and they have role admin, let them do that \nIf users try to go to the hompage of my site (regardless of their role), let them do that\nIf users otherwise try to go to other parts of the site (and they don't have admin), give an error\nOf course, this could be further customized to set different rules for different subdirectories.\n\n\nCreate User Interface\nTo create the user interface for the login screen, I added code to inject a Netlify-maintained login widget to my site’s index.qmd, e.g.:\n---\ndate: last-modified\n---\n\n# Home {.unnumbered}\n\n&lt;div data-netlify-identity-menu&gt;&lt;/div&gt;\n\nWelcome! Please sign in to view the dashboard. \n\nIf you are a first time user, please create a login and email [emilyriederer@gmail.com](mailto:emilyriederer@gmail.com?subject=Dashboard%20Access%20Request) to elevate your access.\n\n\nUser Onboarding\nAfter the changes above to your actual Quarto site, the rest of the work lies in the Netlify admin panel. For a small number of users, you can manually change their role in the user interface.\n\nHowever, to work at any scale, you may need a more automated solution. For that, Netlify’s docs explain how to configure initial role assignment via lambda functions. However, out-of-the box functionality that I found to be lacking was assigning default roles for new users or the ability to configure basic logic such as assigning the same role to any new users onboarding from a certain email domain."
  },
  {
    "objectID": "post/quarto-auth-netlify/index.html#is-it-for-you",
    "href": "post/quarto-auth-netlify/index.html#is-it-for-you",
    "title": "Role-Based Access Control for Quarto sites with Netlify Identity",
    "section": "Is it for you?",
    "text": "Is it for you?\nNetlify Identity isn’t the perfect solution for all use cases, but for many small websites and blogs it’s possibly one of the lowest friction solutions available.\nThis solution is easy to set up initially, allows some degree of self-service for users (account set-up and password resets), user communication (email management), and third-party integration (e.g. authenticate with GitHub or Google). It also has a robust free tier, allowing 1K users to self register (and 5 registrations-by-invitation), and is a substantial step up over locking down HTML content with a single common password.\nHowever, Netlify Identity is not a bullet-proof end-to-end security solution and could become painful or expensive at large scale. This solution, for example, doesn’t contemplate securing your website’s full “supply chain” (e.g. if the source code in in a public GitHub repo) and certainly is less secure than hosting your site completely within a sanboxed environment or intranet. For a large number of users, I also feel there’s a large opportunity to allow simple business rules to configure initial roles.\nIn summary, I would generally recommend Netlify Identity if you’re already using Netlify, expect a small number of users, and are comfortable adding friction to your sign-in process versus absolute security. For larger projects with higher usage and more bullet-proof security needs, it may be worth considering alternatives."
  },
  {
    "objectID": "post/py-rgo-base/index.html",
    "href": "post/py-rgo-base/index.html",
    "title": "Base Python Rgonomic Patterns",
    "section": "",
    "text": "Photo credit to David Clode on Unsplash\nIn the past few weeks, I’ve been writing about a stack of tools and specific packages like polars that may help R users feel “at home” when working in python due to similiar ergonomics. However, one common snag in switching languages is ramping up on common “recipes” for higher-level workflows (e.g. how to build a sklearn modeling pipeline) but missing a languages’s fundamentals that make writing glue code feel smooth (and dare I say pleasant?) It’s a maddening feeling to get code for a complex task to finish only to have the result wrapped in an object that you can’t suss out how to save or manipulate.\nThis post goes back to the basics. We’ll briefly reflect on a few aspects of usability that have led to the success of many workflow packages in R. Then, I’ll demonstrate a grab bag of coding patterns in python that make it feel more elegant to connect bits of code into a coherent workflow.\nWe’ll look at the kind of functionality that you didn’t know to miss until it was gone, you may not be quite sure what to search to figure out how to get it back, and you wonder if it’s even reasonable to hope there’s an analog1. This won’t be anything groundbreaking – just some nuts and bolts. Specifically: helper functions for data and time manipulation, advanced string interpolation, list comprehensions for more functional programming, and object serialization."
  },
  {
    "objectID": "post/py-rgo-base/index.html#what-other-r-ergonomics-do-we-enjoy",
    "href": "post/py-rgo-base/index.html#what-other-r-ergonomics-do-we-enjoy",
    "title": "Base Python Rgonomic Patterns",
    "section": "What other R ergonomics do we enjoy?",
    "text": "What other R ergonomics do we enjoy?\nR’s passionate user and developer community has invested a lot in building tools that smooth over rough edges and provide slick, concise APIs to rote tasks. Sepcifically, a number of packages are devoted to:\n\nUtility functions: Things that make it easier to “automate the boring stuff” like fs for naviating file systems or lubridate for more semantic date wrangling\nFormatting functions: Things that help us make things look nice for users like cli and glue to improve human readability of terminal output and string interpolation\nEfficiency functions: Things that help us write efficient workflows like purrr which provides a concise, typesafe interface for iteration\n\nAll of these capabilities are things we could somewhat trivially write ourselves, but we don’t want to and we don’t need to. Fortunately, we don’t need to in python either."
  },
  {
    "objectID": "post/py-rgo-base/index.html#wrangling-things-date-manipulation",
    "href": "post/py-rgo-base/index.html#wrangling-things-date-manipulation",
    "title": "Base Python Rgonomic Patterns",
    "section": "Wrangling Things (Date Manipulation)",
    "text": "Wrangling Things (Date Manipulation)\nI don’t know a data person who loves dates. In the R world, many enjoy lubridate’s wide range of helper functions for cleaning, formatting, and computing on dates.\nPython’s datetime module is similarly effective. We can easily create and manage dates in date or datetime classes which make them easy to work with.\n\nimport datetime\nfrom datetime import date\ntoday = date.today()\nprint(today)\ntype(today)\n\n2024-01-20\n\n\ndatetime.date\n\n\nTwo of the most important functions are strftime() and strptime().\nstrftime() formats dates into strings. It accepts both a date and the desired string format. Below, we demonstrate by commiting the cardinal sin of writing a date in non-ISO8601.\n\ntoday_str = datetime.datetime.strftime(today, '%m/%d/%Y')\nprint(today_str)\ntype(today_str)\n\n01/20/2024\n\n\nstr\n\n\nstrptime() does the opposite and turns a string encoding a date into an actual date. It can try to guess the format, or we can be nice and provide it guidance.\n\nsomeday_dtm = datetime.datetime.strptime('2023-01-01', '%Y-%m-%d')\nprint(someday_dtm)\ntype(someday_dtm)\n\n2023-01-01 00:00:00\n\n\ndatetime.datetime\n\n\nDate math is also relatively easy with datetime. For example, you can see we calculate the date difference simply by… taking the difference! From the resulting delta object, we can access the days attribute.\n\nn_days_diff = ( today - someday_dtm.date() )\nprint(n_days_diff)\ntype(n_days_diff)\ntype(n_days_diff.days)\n\n384 days, 0:00:00\n\n\nint"
  },
  {
    "objectID": "post/py-rgo-base/index.html#formatting-things-f-strings",
    "href": "post/py-rgo-base/index.html#formatting-things-f-strings",
    "title": "Base Python Rgonomic Patterns",
    "section": "Formatting Things (f-strings)",
    "text": "Formatting Things (f-strings)\nR’s glue is beloved for it’s ability to easily combine variables and texts into complex strings without a lot of ugly, nested paste() functions.\npython has a number of ways of doing this, but the most readable is the newest: f-strings. Simply put an f before the string and put any variable names to be interpolated in {curly braces}.\n\nname = \"Emily\"\nprint(f\"This blog post is written by {name}\")\n\nThis blog post is written by Emily\n\n\nf-strings also support formatting with formats specified after a colon. Below, we format a long float to round to 2 digits.\n\nproportion = 0.123456789\nprint(f\"The proportion is {proportion:.2f}\")\n\nThe proportion is 0.12\n\n\nAny python expression – not just a single variable – can go in curly braces. So, we can instead format that propotion as a percent.\n\nproportion = 0.123456789\nprint(f\"The proportion is {proportion*100:.1f}%\")\n\nThe proportion is 12.3%\n\n\nDespite the slickness of f-strings, sometimes other string interpolation approaches can be useful. For example, if all the variables I want to interpolate are in a dictionary (as often will happen, for example, with REST API responses), the string format() method is a nice alternative. It allows us to pass in the dictionary, “unpacking” the argument with **2\n\nresult = {\n    'dog_name': 'Squeak',\n    'dog_type': 'Chihuahua'\n}\nprint(\"{dog_name} is a {dog_type}\".format(**result))\n\nSqueak is a Chihuahua\n\n\n\nApplication: Generating File Names\nCombining what we’ve discussed about datetime and f-strings, here’s a pattern I use frequently. If I am logging results from a run of some script, I might save the results in a file suffixed with the run timestamp. We can generate this easily.\n\ndt_stub = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\nfile_name = f\"output-{dt_stub}.csv\"\nprint(file_name)\n\noutput-20240120_071517.csv"
  },
  {
    "objectID": "post/py-rgo-base/index.html#repeating-things-iteration-functional-programming",
    "href": "post/py-rgo-base/index.html#repeating-things-iteration-functional-programming",
    "title": "Base Python Rgonomic Patterns",
    "section": "Repeating Things (Iteration / Functional Programming)",
    "text": "Repeating Things (Iteration / Functional Programming)\nThanks in part to a modern-day fiction that for loops in R are inefficient, R users have gravitated towards concise mapping functions for iteration. These can include the *apply() family3, purrr’s map_*() functions, or the parallelized version of either.\nPython too has a nice pattern for arbitrary iteration in list comprehensions. For any iterable, we can use a list comprehension to make a list of outputs by processing a list of inputs, with optional conditional and default expressions.\nHere are some trivial examples:\n\nl = [1,2,3]\n[i+1 for i in l]\n\n[2, 3, 4]\n\n\n\n[i+1 for i in l if i % 2 == 1]\n\n[2, 4]\n\n\n\n[i+1 if i % 2 == 1 else i for i in l]\n\n[2, 2, 4]\n\n\nThere are also closer analogs to purrr like python’s map() function. map() takes a function and an iterable object and applies the function to each element. Like with purrr, functions can be anonymous (as defined in python with lambda functions) or named. List comprehensions are popular for their concise syntax, but there are many different thoughts on the matter as expressed in this StackOverflow post.\n\ndef add_one(i): \n  return i+1\n\n# these are the same\nlist(map(lambda i: i+1, l))\nlist(map(add_one, l))\n\n[2, 3, 4]\n\n\n\nApplication: Simulation\nAs a (slightly) more realistic(ish) example, let’s consider how list comprehensions might help us conduct a numerical simulation or sensitivity analysis.\nSuppose we want to simulate 100 draws from a Bernoulli distribution with different success probabilites and see how close our empirically calculated rate is to the true rate.\nWe can define the probabilites we want to simulate in a list and use a list comprehension to run the simulations.\n\nimport numpy as np\nimport numpy.random as rnd\n\nprobs = [0.1, 0.25, 0.5, 0.75, 0.9]\ncoin_flips = [ np.mean(np.random.binomial(1, p, 100)) for p in probs ]\ncoin_flips\n\n[0.05, 0.3, 0.48, 0.77, 0.87]\n\n\nAlternatively, instead of returning a list of the same length, our resulting list could include whatever we want – like a list of lists! If we wanted to keep the raw simulation results, we could. The following code returns a list of 5 lists - one with the raw simulation results.\n\ncoin_flips = [ list(np.random.binomial(1, p, 100)) for p in probs ]\nprint(f\"\"\"\n  coin_flips has {len(coin_flips)} elements\n  Each element is itself a {type(coin_flips[0])}\n  Each element is of length {len(coin_flips[0])}\n  \"\"\")\n\n\n  coin_flips has 5 elements\n  Each element is itself a &lt;class 'list'&gt;\n  Each element is of length 100\n  \n\n\nIf one wished, they could then put these into a polars dataframe and pivot those list-of-lists (going from a 5-row dataset to a 500-row dataset)to conduct whatever sort of analysis with want with all the replicates.\n\nimport polars as pl\n\ndf_flips = pl.DataFrame({'prob': probs, 'flip': coin_flips})\ndf_flips.explode('flip').glimpse()\n\nRows: 500\nColumns: 2\n$ prob &lt;f64&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1\n$ flip &lt;i32&gt; 0, 0, 0, 0, 1, 0, 1, 1, 0, 0\n\n\n\nWe’ll return to list comprehensions in the next section."
  },
  {
    "objectID": "post/py-rgo-base/index.html#faking-things-data-generation",
    "href": "post/py-rgo-base/index.html#faking-things-data-generation",
    "title": "Base Python Rgonomic Patterns",
    "section": "Faking Things (Data Generation)",
    "text": "Faking Things (Data Generation)\nCreating simple miniature datasets is often useful in analysis. When working with a new packages, it’s an important part of learning, developing, debugging, and eventually unit testing. We can easily run our code on a simplified data object where the desired outcome is easy to determine to sanity-check our work, or we can use fake data to confirm our understanding of how a program will handle edge cases (like the diversity of ways different programs handle null values). Simple datasets can also be used and spines and scaffolds for more complex data wrangling tasks (e.g. joining event data onto a date spine).\nIn R, data.frame() and expand.grid() are go-to functions, coupled with vector generators like rep() and seq(). Python has many similar options.\n\nFake Datasets\nFor the simplest of datasets, we can manually write a few entries as with data.frame() in R. Here, we define series in a named dictionary where each dictionary key turns into a column name.\n\nimport polars as pl\n\npl.DataFrame({\n  'a': [1,2,3],\n  'b': ['x','y','z']\n})\n\n\n\nshape: (3, 2)\n\n\n\na\nb\n\n\ni64\nstr\n\n\n\n\n1\n\"x\"\n\n\n2\n\"y\"\n\n\n3\n\"z\"\n\n\n\n\n\n\n\nIf we need longer datasets, we can use helper functions in packages like numpy to generate the series. Methods like arange and linspace work similarly to R’s seq().\n\nimport polars as pl\nimport numpy as np\n\npl.DataFrame({\n  'a': np.arange(stop = 3),\n  'b': np.linspace(start = 9, stop = 24, num = 3)\n})\n\n\n\nshape: (3, 2)\n\n\n\na\nb\n\n\ni32\nf64\n\n\n\n\n0\n9.0\n\n\n1\n16.5\n\n\n2\n24.0\n\n\n\n\n\n\n\nIf we need groups in our sample data, we can use np.repeat() which works like R’s rep(each = TRUE).\n\npl.DataFrame({\n  'a': np.repeat(np.arange(stop = 3), 2),\n  'b': np.linspace(start = 3, stop = 27, num = 6)\n})\n\n\n\nshape: (6, 2)\n\n\n\na\nb\n\n\ni32\nf64\n\n\n\n\n0\n3.0\n\n\n0\n7.8\n\n\n1\n12.6\n\n\n1\n17.4\n\n\n2\n22.2\n\n\n2\n27.0\n\n\n\n\n\n\n\nAlternatively, for more control and succinct typing, we can created a nested dataset in polars and explode it out.\n\n(\n  pl.DataFrame({\n    'a': [1, 2, 3],\n    'b': [\"a b c\", \"d e f\", \"g h i\"]\n  })\n  .with_columns(pl.col('b').str.split(\" \"))\n  .explode('b')\n)\n\n\n\nshape: (9, 2)\n\n\n\na\nb\n\n\ni64\nstr\n\n\n\n\n1\n\"a\"\n\n\n1\n\"b\"\n\n\n1\n\"c\"\n\n\n2\n\"d\"\n\n\n2\n\"e\"\n\n\n2\n\"f\"\n\n\n3\n\"g\"\n\n\n3\n\"h\"\n\n\n3\n\"i\"\n\n\n\n\n\n\n\nSimilarly, we could use what we’ve learned about polars list columns and list comprehensions.\n\na = [1, 2, 3]\nb = [ [q*i for q in [1, 2, 3]] for i in a]\npl.DataFrame({'a':a,'b':b}).explode('b')\n\n\n\nshape: (9, 2)\n\n\n\na\nb\n\n\ni64\ni64\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n1\n3\n\n\n2\n2\n\n\n2\n4\n\n\n2\n6\n\n\n3\n3\n\n\n3\n6\n\n\n3\n9\n\n\n\n\n\n\n\nIn fact, multidimensional list comprehensions can be used to mimic R’s expand.grid() function.\n\npl.DataFrame(\n  [(x, y) for x in range(3) for y in range(3)],\n  schema = ['x','y']\n  )\n\n\n\nshape: (9, 2)\n\n\n\nx\ny\n\n\ni64\ni64\n\n\n\n\n0\n0\n\n\n0\n1\n\n\n0\n2\n\n\n1\n0\n\n\n1\n1\n\n\n1\n2\n\n\n2\n0\n\n\n2\n1\n\n\n2\n2\n\n\n\n\n\n\n\n\n\nBuilt-In Data\nR has a number of canonical datasets like iris built in to the core language. This can be easy to quickly grab for experimentation4. While base python doesn’t include such capabilities, many of the exact same or similar datasets can be found in seaborn.\nseaborn.get_dataset_names() provides the list of available options. Below, we load the Palmers Penguins data and, if you wish, convert it from pandas to polars.\n\nimport seaborn as sns\nimport polars as pl\n\ndf_pd = sns.load_dataset('penguins')\ndf = pl.from_pandas(df_pd)\ndf.glimpse()\n\nRows: 344\nColumns: 7\n$ species           &lt;str&gt; 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie', 'Adelie'\n$ island            &lt;str&gt; 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen', 'Torgersen'\n$ bill_length_mm    &lt;f64&gt; 39.1, 39.5, 40.3, None, 36.7, 39.3, 38.9, 39.2, 34.1, 42.0\n$ bill_depth_mm     &lt;f64&gt; 18.7, 17.4, 18.0, None, 19.3, 20.6, 17.8, 19.6, 18.1, 20.2\n$ flipper_length_mm &lt;f64&gt; 181.0, 186.0, 195.0, None, 193.0, 190.0, 181.0, 195.0, 193.0, 190.0\n$ body_mass_g       &lt;f64&gt; 3750.0, 3800.0, 3250.0, None, 3450.0, 3650.0, 3625.0, 4675.0, 3475.0, 4250.0\n$ sex               &lt;str&gt; 'Male', 'Female', 'Female', None, 'Female', 'Male', 'Female', 'Male', None, None"
  },
  {
    "objectID": "post/py-rgo-base/index.html#saving-things-object-serialization",
    "href": "post/py-rgo-base/index.html#saving-things-object-serialization",
    "title": "Base Python Rgonomic Patterns",
    "section": "Saving Things (Object Serialization)",
    "text": "Saving Things (Object Serialization)\nSometimes, it can be useful to save objects as they existed in RAM in an active programming environment. R users may have experienced this if they’ve used .rds, .rda, or .Rdata files to save individual variables or their entire environment. These objects can often be faster to reload than plaintext and can better preserve information that may be lost in other formats (e.g. storing a dataframe in a way that preserves its datatypes versus writing to a CSV file5 or storing a complex object that can’t be easily reduced to plaintext like a model with training data, hyperparameters, learned tree splits or weights or whatnot for future predictions.) This is called object serializaton6\nPython has comparable capabilities in the pickle module. There aren’t really style points here, so I’ve not much to add beyond “this exists” and “read the documentation”. But, at a high level, it looks something like this:\n\n# to write a pickle\nwith open('my-obj.pickle', 'wb') as handle:\n    pickle.dump(my_object, handle, protocol = pickle.HIGHEST_PROTOCOL)\n\n# to read a pickle\nmy_object = pickle.load(open('my-obj.pickle','rb'))"
  },
  {
    "objectID": "post/py-rgo-base/index.html#footnotes",
    "href": "post/py-rgo-base/index.html#footnotes",
    "title": "Base Python Rgonomic Patterns",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI defined this odd scope to help limit the infinite number of workflow topics that could be included like “how to write a function” or “how to source code from another script”↩︎\nThis is called “**kwargs” and works a bit like do.call() in base R. You can read more about it here.↩︎\nSpeaking of non-ergonomic things in R, the *apply() family is notoriously diverse in its number and order of arguments↩︎\nParticularly if you want to set wildly unrealistic expectations for the efficacy of k-means clustering, but I digress↩︎\nAnd yes, you can and should use Parquet and then my example falls apart – but that’s not the point!↩︎\nAnd, if you want to go incredibly deep here, check out this awesome post by Danielle Navarro.↩︎"
  },
  {
    "objectID": "post/py-rgo/index.html",
    "href": "post/py-rgo/index.html",
    "title": "Python Rgonomics",
    "section": "",
    "text": "Photo credit to the inimitable Allison Horst\nInteroperability was a key theme in open-source data languages in 2023. Ongoing innovations in Arrow (a language-agnostic in-memory standard for data storage), growing adoption of Quarto (the language-agnostic heir apparent to R Markdown), and even pandas creator Wes McKinney joining Posit (the language-agnostic rebranding of RStudio) all illustrate the ongoing investment in breaking down barriers between different programming languages and paradigms.\nDespite these advances in technical interoperability, individual developers will always face more friction than state-of-the-art tools when moving between languages. Learning a new language is easily enough done; programming 101 concepts like truth tables and control flow translate seamlessly. But ergonomics of a language do not. The tips and tricks we learn to be hyper productive in a primary language are comfortable, familiar, elegant, and effective. They just feel good. Working in a new language, developers often face a choice between forcing their favored workflows into a new tool where they may not “fit”, writing technically correct yet plodding code to get the job done, or approaching a new language as a true beginner to learn it’s “feel” from the ground up.\nFortunately, some of these higher-level paradigms have begun to bleed across languages, enriching previously isolated tribes with the and enabling developers to take their advanced skillsets with them across languages. For any R users who aim to upskill in python in 2024, recent tools and versions of old favorites have made strides in converging the R and python data science stacks. In this post, I will overview some recommended tools that are both truly pythonic while capturing the comfort and familiarity of some favorite R packages of the tidyverse variety.1"
  },
  {
    "objectID": "post/py-rgo/index.html#what-this-post-is-not",
    "href": "post/py-rgo/index.html#what-this-post-is-not",
    "title": "Python Rgonomics",
    "section": "What this post is not",
    "text": "What this post is not\nJust to be clear:\n\nThis is not a post about why python is better than R so R users should switch all their work to python\nThis is not a post about why R is better than python so R semantics and conventions should be forced into python\nThis is not a post about why python users are better than R users so R users need coddling\nThis is not a post about why R users are better than python users and have superior tastes for their toolkit\nThis is not a post about why these python tools are the only good tools and others are bad tools\n\nIf you told me you liked the New York’s Museum of Metropolitan Art, I might say that you might also like Chicago’s Art Institute. That doesn’t mean you should only go to the museum in Chicago or that you should never go to the Louvre in Paris. That’s not how recommendations (by human or recsys) work. This is an “opinionated” post in the sense that “I like this” and not opinionated in the sense that “you must do this”."
  },
  {
    "objectID": "post/py-rgo/index.html#on-picking-tools",
    "href": "post/py-rgo/index.html#on-picking-tools",
    "title": "Python Rgonomics",
    "section": "On picking tools",
    "text": "On picking tools\nThe tools I highlight below tend to have two competing features:\n\nThey have aspects of their workflow and ergonomics that should feel very comfortable to users of favored R tools\nThey should be independently accepted, successful, and well-maintained python projects with the true pythonic spirit\n\nThe former is important because otherwise there’s nothing tailored about these recommendations; the latter is important so users actually engage with the python language and community instead of dabbling around in its more peripheral edges. In short, these two principles exclude tools that are direct ports between languages with that as their sole or main benefit.2\nFor example, siuba and plotnine were written with the direct intent of mirroring R syntax. They have seen some success and adoption, but more niche tools come with liabilities. With smaller user-bases, they tend to lack in the pace of development, community support, prior art, StackOverflow questions, blog posts, conference talks, discussions, others to collaborate with, cache in a portfolio, etc. Instead of enjoying the ergonomics of an old language or embracing the challenge of learning a new one, ports can sometimes force developers to invest energy into a “secret third thing” of learning tools that isolate them from both communities and facing inevitable snags by themselves.\nWhen in Rome, do as the Romans do – but if you’re coming from the U.S. that doesn’t mean you can’t bring a universal adapter that can help charge your devices in European outlets."
  },
  {
    "objectID": "post/py-rgo/index.html#the-stack",
    "href": "post/py-rgo/index.html#the-stack",
    "title": "Python Rgonomics",
    "section": "The stack",
    "text": "The stack\nWIth that preamble out of the way, below are a few recommendations for the most ergonomic tools for getting set up, conducting core data analysis, and communication results.\nTo preview these recommendations:\nSet Up\n\nInstallation: pyenv\nIDE: VS Code\n\nAnalysis\n\nWrangling: polars\nVisualization: seaborn\n\nCommunication\n\nTables: Great Tables\nNotebooks: Quarto\n\nMiscellaneous\n\nEnvironment Management: pdm\nCode Quality: ruff\n\n\n\n\n\n\n\nNote\n\n\n\nI don’t want this advice to set up users for a potential snag. If you are on Windows and install python with pyenv-win, Quarto (as of writing on v1.3) may struggle to find the correct executable. Better support for this is on the backlog, but if you run into this issue, checkout this brilliant fix.\n\n\n\nFor setting up\nThe first hurdle is often getting started – both in terms of installing the tools you’ll need and getting into a comfortable IDE to run them.\n\nInstallation: R keeps installation simple; there’s one way to do it* so you do and it’s done. But before python converts can print(\"hello world\"), they face a range of options (system Python, Python installer UI, Anaconda, Miniconda, etc.) each with its own kinks. These decisions are made harder in Python since projects tend to have stronger dependencies of the language, requiring one to switch between versions. For both of these reasons, I favor the pyenv (or pyenv-win for those on Windows) for easily managing python installation(s) from the command line. While the installation process of pyenv may be technically different, it’s similar in that it “just works” with just a few commands. In fact, the workflow is so slick that things seem to have gone 180 degrees with pyenv inspiring similar project called rig to manage R installations. This may sound intimidating, but the learning curve is actually quite shallow:\n\npyenv install --list: To see what python versions are available to install\npyenv install &lt;version number&gt;: To install a specific version\npyenv versions: To see what python versions are installed on your system\npyenv global &lt;version number&gt;: The set one python version as a global default\npyenv local &lt;version number&gt;: The set a python version to be used within a specific directory/project\n\nIntegrated Development Environment: Once R is install, R users are typically off to the races with the intuitive RStudio IDE which helps them get immediately hands-on with the REPL. With the UI divided into quadrants, users can write an R script, run it to see results in the console, conceptualize what the program “knows” with the variable explorer, and navigate files through a file explorer. Once again, python is not lacking in IDE options, but users are confronted with yet another decision point before they even get started. Pycharm, Sublime, Spyder, Eclipse, Atom, Neovim, oh my! I find that VS Code offers the best functionality. It’s rich extension ecosystem also means that most major tools (e.g. Quarto, git, linters and stylers, etc.) have nice add-ons so, like RStudio, you can customize your platform to perform many side-tasks in plaintext or with the support of extra UI components.3\n\n\n\nFor data analysis\nAs data practitioners know, we’ll spend most of our time on cleaning and wrangling. As such, R users may struggle particularly to abandon their favorite tools for exploratory data analysis like dplyr and ggplot2. Fans of those packages often appreciate how their functional paradigm helps achieve a “flow state”. Precise syntax may differ, but new developments in the python wrangling stack provide increasingly close analogs to some of these beloved Rgonomics.\n\nData Wrangling: Although pandas is undoubtedly the best-known wrangling tool in the python space, I believe the growing polars project offers the best experience for a transitioning developer (along with other nice-to-have benefits like being dependency free and blazingly fast). polars may feel more natural and less error-prone to R users for may reasons:\n\nit has more internal consistent (and similar to dplyr) syntax such as select, filter, etc. and has demonstrated that the project values a clean API (e.g. recently renaming groupby to group_by)\nit does not rely on the distinction between columns and indexes which can feel unintuitive and introduces a new set of concepts to learn\nit consistently returns copies of dataframes (while pandas sometimes alters in-place) so code is more idempotent and avoids a whole class of failure modes for new users\nit enables many of the same “advanced” wrangling workflows in dplyr with high-level, semantic code like making the transformation of multiple variables at once fast with column selectors, concisely expressing window functions, and working with nested data (or what dplyr calls “list columns”) with lists and structs\nsupporting users working with increasingly large data. Similar to dplyr’s many backends (e.g. dbplyr), polars can be used to write lazily-evaluated, optimized transformations and it’s syntax is reminiscent of pyspark should users ever need to switch between\n\nVisualization: Even some of R’s critics will acknowledge the strength of ggplot2 for visualization, both in terms of it’s intuitive and incremental API and the stunning graphics it can produce. seaborn’s object interface seems to strike a great balance between offering a similar workflow (which cites ggplot2 as an inspiration) while bringing all the benefits of using an industry-standard tool\n\n\n\nFor communication\nHistorically, one possible dividing line between R and python has been framed as “python is good at working with computers, R is good at working with people”. While that is partially inspired by reductive takes that R is not production-grade, it is not without truth that the R’s academic roots spurred it to overinvest in a rich “communication stack” and translating analytical outputs into human-readable, publishable outputs. Here, too, the gaps have begun to close.\n\nTables: R has no shortage of packages for creating nicely formatted tables, an area that has historically lacked a bit in python both in workflow and outcomes. Barring strong competition from the native python space, the one “port” I am bullish about is the recently announced Great Tables package. This is a pythonic clone of R’s gt package. I’m more comfortable recommending this since it’s maintained by the same developer as the R version (to support long-term feature parity), backed by an institution not just an individual (to ensure it’s not a short-lived hobby project), and the design feels like it does a good job balancing R inspiration with pythonic practices\nComputational notebooks: Jupyter Notebooks are widely used, widely critiqued parts of many python workflows. While the ability to mix markdown and code chunks. However, notebooks can introduce new types of bugs for the uninitiated; for example, they are hard to version control and easy to execute in the wrong environment. For those coming from the world of R Markdown, plaintext computational notebooks like Quarto may provide a more transparent development experience. While Quarto allows users to write in .qmd files which are more like their .rmd predecessors, its renderer can also handle Jupyter notebooks to enable collaboration across team members with different preferences\n\n\n\nMiscellaneous\nA few more tools may be helpful and familiar to some R users who tend towards the more “developer” versus “analyst” side of the spectrum. These, in my mind, have even more varied pros and cons, but I’ll leave them for consideration:\n\nEnvironment Management: Joining the python world means never having to settle on an environment management tool for installing packages. There’s a truly overwhelming number of ways to manage project-level dependencies (virtualenv, conda, piptools, pipenv, poetry, and that doesn’t even scratch the surface) with different pros and cons and phenomenal amount of ink/pixels have been spilled over litigating these trade-offs. Putting all that aside, lately, I’ve been favoring pdm because it prioritizes features I care most about (auto-updating pyproject.toml, isolating dependencies from dependencies-of-dependencies, active development and error handling, mostly just works pretty undramatically)\nDeveloper Tools: ruff provides a range of linting and styling options (think R’s lintr and styler) and provides a one-stop-shop over what can be an overwhelming number of atomic tools in this space (isort, black, flake8, etc.). ruff is super fast, has a nice VS Code extension, and, while this class of tools is generally considered more advanced, I think linters can be a fantastic “coach” for new users about best practices"
  },
  {
    "objectID": "post/py-rgo/index.html#more-to-come",
    "href": "post/py-rgo/index.html#more-to-come",
    "title": "Python Rgonomics",
    "section": "More to come!",
    "text": "More to come!\nEach recommendation here itself could be its own tutorial or post. In particular, I hope to showcase the Rgonomics of polars, seaborn, and great_tables in future posts."
  },
  {
    "objectID": "post/py-rgo/index.html#footnotes",
    "href": "post/py-rgo/index.html#footnotes",
    "title": "Python Rgonomics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOf course, languages have their own subcultures too. The tidyverse and data.table parts of the R world tend to favor different semantics and ergonomics. This post caters more to the former.↩︎\nThere is no doubt a place for language ports, especially for earlier stage project where no native language-specific standard exists. For example, I like Karandeep Singh’s lab work on a tidyverse for Julia and maintain my own dbtplyr package to port dplyr’s select helpers to dbt↩︎\n If anything, the one challenge of VS Code is the sheer number of set up options, but to start out, you can see these excellent tutorials from Rami Krispin on recommended python and R configurations ↩︎"
  },
  {
    "objectID": "post/nulls-polyglot/index.html",
    "href": "post/nulls-polyglot/index.html",
    "title": "Oh, I’m sure it’s probably nothing",
    "section": "",
    "text": "Photo credit to Davide Ragusa on Unsplash\nLanguage interoperability and different ways of enabling “polyglot” workflows have seemed to take centerstage in the data world recently:\nAs a general matter, these are all exciting advances with great potential to aid in different workflows when used judiciously. However, it also poses the question: what cognitive burdens do we alleviate and which do we add when our projects begin to leverage multiple languages?\nDespite common data analysis tools like SQL, R, and python being high-level languages with declarative interfaces (in the case of R’s tidyverse and python’s pandas), successful usage still requires understanding the underlying assumptions and operations of each tool. There is not such thing as a truly declarative language; only those that generally make decisions that the user likes well-enough to ask for the “what” and delegate the “how”. These differences can emerge at many different levels: such as foundational issues like whether data structures are copied or modified in-place or broader design choices like default hyperparameters in machine learning libraries (e.g. python’s scikitlearn notoriously uses regularized logistic regression as the default for logistic regression.) Somewhere along that spectrum lies the fickle issue of handling null values.\nIn this post, I recap a quick case study of how incautious null handling risks data analysis validity. Then, taking a step back, I compare how R, python, and SQL behave differently when confront with null values and the implications for analysts switching between languages."
  },
  {
    "objectID": "post/nulls-polyglot/index.html#tldr",
    "href": "post/nulls-polyglot/index.html#tldr",
    "title": "Oh, I’m sure it’s probably nothing",
    "section": "TLDR",
    "text": "TLDR\nA summary of these different behaviors is provided below:\n\n\n\n\nR\npython\nSQL\n\n\n\n\nColumn Aggregation\nNA\nnp: NApd: Value\nValue\n\n\nRow-wise Transformation\nNA\nNA\nNA\n\n\nJoining\nMatch by default\nMatch\nNo match\n\n\nFiltering\nNo match\nMatch\nNo match"
  },
  {
    "objectID": "post/nulls-polyglot/index.html#case-study",
    "href": "post/nulls-polyglot/index.html#case-study",
    "title": "Oh, I’m sure it’s probably nothing",
    "section": "Case Study",
    "text": "Case Study\nBefore comparing different languages, let’s walk through a brief case study to see all the way that “lurking” nulls can surprise a junior analyst in any one language and observe a few different “contours” of the problem space.\nConsider two tables in a retailer’s database. The spend table reports total sales by month and store identifier (null if online).\n\n\n  STORE_ID MONTH AMT_SPEND\n1        1     1 100.12011\n2        2     1 100.31441\n3       NA     1 100.40517\n4        1     2  99.67098\n5        2     2  98.39703\n6       NA     2  98.81231\n7        1     3 102.27124\n8        2     3 100.20843\n9       NA     3        NA\n\n\nSimilarly, the returns table reports returned sales at the same grain.\n\n\n  STORE_ID MONTH AMT_RETURN\n1        1     1         NA\n2        2     1   9.972159\n3       NA     1  10.071639\n4        1     2   9.798444\n5        2     2  10.254347\n6       NA     2   9.881071\n7        1     3  10.108880\n8        2     3   9.951398\n9       NA     3   9.849277\n\n\nIn both cases, nulls are used in the 'AMT_*' fields to denote zeros for the respective month x store_id combinations`.\nTo calculate something as simple as the average gross spend per store across months, an analyst might attempt to write:\n\nselect \n  store_id, \n  avg(amt_spend)\nfrom spend\ngroup by 1\norder by 1\n\n\n\nstore_id\navg(amt_spend)\n\n\n\n\nNA\n99.60874\n\n\n1\n100.68744\n\n\n2\n99.63996\n\n\n\n\nHowever, because SQL silently drops nulls in column aggregations, the online spend is not appropriately “penalized” for its lack of March spend. The averages across all three stores look nearly equal.\nNot only is this answer “wrong”, it can also be thought of as fundamentally changing the computand (a word I just made up. In statistics, we talk about estimands as “the conceptual thing we are trying to estimate with an estimator”. Here, we aren’t estimating anything – just computing. But, there’s still a concentual “thing we are trying to measure” and in this case, it’s our tools and not our methods that are imposing assumptions on that) to one that answers a fundamentally different question:\nInstead of measuring “average monthly spend in Q1 by store”, we’re measuring “averaging monthly spend in Q1 by store conditional on there being spend”.\nTo obtain the correct result, one would write:\n\nselect \n  store_id, \n  -- wrong answers\n  avg(amt_spend) as wrong1,  \n  sum(amt_spend) / count(amt_spend) as wrong2,\n  -- right answers\n  sum(amt_spend) / count(1) as right1,\n  avg(coalesce(amt_spend, 0)) as right2\nfrom spend\ngroup by 1\norder by 1\n\n\n\nstore_id\nwrong1\nwrong2\nright1\nright2\n\n\n\n\nNA\n99.60874\n99.60874\n66.40583\n66.40583\n\n\n1\n100.68744\n100.68744\n100.68744\n100.68744\n\n\n2\n99.63996\n99.63996\n99.63996\n99.63996\n\n\n\n\nWith a better understand of gross sales, the analyst might next proceed to compute net sales.\nThis first requires joining the spend and returns tables. Naively, they might attempt:\n\nselect \n  spend.*,\n  returns.amt_return\nfrom \n  spend\n  inner join\n  returns \n  on\n  spend.store_id = returns.store_id and\n  spend.month = returns.month\n\n\n\nSTORE_ID\nMONTH\nAMT_SPEND\namt_return\n\n\n\n\n1\n1\n100.12011\nNA\n\n\n2\n1\n100.31441\n9.972159\n\n\n1\n2\n99.67098\n9.798444\n\n\n2\n2\n98.39703\n10.254347\n\n\n1\n3\n102.27124\n10.108880\n\n\n2\n3\n100.20843\n9.951398\n\n\n\n\nHowever, this once again fails. Why? Although SQL handled nulls “permissively” when aggregating a column, it took a stricted stance when making the comparison on spend.store_id = returns.store_id in the join clause. SQL doesn’t recognize different nulls as equal. To the extent than null means “I dunno” versus “The field is not relevant to this observation”, it’s reasonable that SQL should find it hard to decide whether two “I dunno”s are equal.\nOnce again, this isn’t a “random” or inconsequential error. Continuing to use this corrupted dataset changes the computand from “net sales by month” to “net sales by month at physical retail locations”.\nTo remedy this, we can force store_id to take on a value:\n\nselect\n  spend.*,\n  returns.amt_return\nfrom \n  spend\n  inner join\n  returns \n  on\n  coalesce(spend.store_id, 999) = coalesce(returns.store_id, 999) and\n  spend.month = returns.month\n\n\n\nSTORE_ID\nMONTH\nAMT_SPEND\namt_return\n\n\n\n\n1\n1\n100.12011\nNA\n\n\n2\n1\n100.31441\n9.972159\n\n\nNA\n1\n100.40517\n10.071639\n\n\n1\n2\n99.67098\n9.798444\n\n\n2\n2\n98.39703\n10.254347\n\n\nNA\n2\n98.81231\n9.881071\n\n\n1\n3\n102.27124\n10.108880\n\n\n2\n3\n100.20843\n9.951398\n\n\nNA\n3\nNA\n9.849277\n\n\n\n\nAnd next we proceed with computing sales by month net of returns across all stores:\n\nselect\n  spend.month, \n  sum(amt_spend - amt_return) as net_spend\nfrom \n  spend\n  inner join\n  returns \n  on\n  coalesce(spend.store_id, 999) = coalesce(returns.store_id, 999) and\n  spend.month = returns.month\ngroup by 1\norder by 1\n\n\n\nmonth\nnet_spend\n\n\n\n\n1\n180.6758\n\n\n2\n266.9465\n\n\n3\n182.4194\n\n\n\n\nHowever, by now, you should not be surprised that this result is also incorrect. If we inspect the sequence of computations, we realize that SQL is also stricter in its null handing in rowwise computations than column-wise aggregations. The subtraction of amt_spend and amt_return obliterates the total when either is null. So, we fail to include the gross spend at Store 1 in January simply because there were no returns (and vice versa for Internet sales in March).\n\nselect\n  spend.month, \n  spend.store_id,\n  amt_spend,\n  amt_return,\n  amt_spend - amt_return as net_spend\nfrom \n  spend\n  inner join\n  returns \n  on\n  coalesce(spend.store_id, 999) = coalesce(returns.store_id, 999) and\n  spend.month = returns.month\n\n\n\nmonth\nstore_id\namt_spend\namt_return\nnet_spend\n\n\n\n\n1\n1\n100.12011\nNA\nNA\n\n\n1\n2\n100.31441\n9.972159\n90.34225\n\n\n1\nNA\n100.40517\n10.071639\n90.33353\n\n\n2\n1\n99.67098\n9.798444\n89.87254\n\n\n2\n2\n98.39703\n10.254347\n88.14268\n\n\n2\nNA\n98.81231\n9.881071\n88.93124\n\n\n3\n1\n102.27124\n10.108880\n92.16236\n\n\n3\n2\n100.20843\n9.951398\n90.25704\n\n\n3\nNA\nNA\n9.849277\nNA\n\n\n\n\nA few ways to get the correct answer are shown below:\n\nselect\n  spend.month, \n  sum(coalesce(amt_spend,0) - coalesce(amt_return,0)) as right1,\n  sum(amt_spend) - sum(amt_return) as right2\nfrom \n  spend\n  inner join\n  returns \n  on\n  coalesce(spend.store_id, 999) = coalesce(returns.store_id, 999) and\n  spend.month = returns.month\ngroup by 1\norder by 1\n\n\n\nmonth\nright1\nright2\n\n\n\n\n1\n280.7959\n280.7959\n\n\n2\n266.9465\n266.9465\n\n\n3\n172.5701\n172.5701"
  },
  {
    "objectID": "post/nulls-polyglot/index.html#observations",
    "href": "post/nulls-polyglot/index.html#observations",
    "title": "Oh, I’m sure it’s probably nothing",
    "section": "Observations",
    "text": "Observations\nThe preceding example hopefully illustrates a few points:\n\nNulls can cause issues in the most basic of analyses\nBeyond causing random or marginal errors, null handling changes the questions being answered\nEven within a language, null handling may feel inconsistent (w.r.t. strictness) across different operations\n\nSo, with that, let’s compare languages!"
  },
  {
    "objectID": "post/nulls-polyglot/index.html#comparison",
    "href": "post/nulls-polyglot/index.html#comparison",
    "title": "Oh, I’m sure it’s probably nothing",
    "section": "Comparison",
    "text": "Comparison\nBelow, we compare how R, SQL, and python handle column aggregation, rowwise transformation, joining, and filtering.\n\nAggregation\nSQL, as we saw before, simply ignores nulls in aggregation functions.\n\nselect \n  sum(x) as sum_x, \n  sum(if(x is null,1,0)) as n_null_x\nfrom tbl\n\n\n\nsum_x\nn_null_x\n\n\n\n\n3\n1\n\n\n\n\nBuilt by and for statistician’s, R is scandalized at the very idea of attempting to do math with null columns. For aggregation functions, it returns NA as a form of protest should any entry of the vector provided be null. (This can be overridden with the na.rm parameter.)\n\nx &lt;- c(1,2,NA)\nsum(x)\n\ndf &lt;- data.frame(x = x)\ndplyr::summarize(df, x = sum(x))\n\n[1] NA\n\n\n\n\n\nx\n\n\n\n\nNA\n\n\n\n\n\nWhen it comes to python, well, it depends. Base and numpy operations act more like R whereas pandas aggregation acts more like SQL.\n\nimport pandas as pd\nimport numpy as np\nx = [1,2,np.nan]\ny = [3,4,5]\ndf = pd.DataFrame({'x':x,'y':y})\nsum(x)\nnp.sum(x)\ndf.agg({'x': ['sum']})\n\nnan\nnan\n       x\nsum  3.0\n\n\n\n\nTransformation\nAll of SQL, R, and python return NA when NAs are used in atomic or rowwise transformations.\nIn SQL:\n\nselect *, x-y as z\nfrom tbl\n\n\n\nx\ny\nz\n\n\n\n\n1\n3\n-2\n\n\n2\n4\n-2\n\n\nNA\n5\nNA\n\n\n\n\nIn R:\n\ndf &lt;- data.frame(x = c(1,2,NA), y = 3:5)\ndplyr::mutate(df, z = x-y)\n\ndf$z &lt;- with(df, x-y)\ndf\n\n\n\n\nx\ny\nz\n\n\n\n\n1\n3\n-2\n\n\n2\n4\n-2\n\n\nNA\n5\nNA\n\n\n\n\n\n\nx\ny\nz\n\n\n\n\n1\n3\n-2\n\n\n2\n4\n-2\n\n\nNA\n5\nNA\n\n\n\n\n\nIn python:\n\nnp.array(x) - np.array(y)\ndf.assign(z = lambda d: d.x - d.y)\n\narray([-2., -2., nan])\n     x  y    z\n0  1.0  3 -2.0\n1  2.0  4 -2.0\n2  NaN  5  NaN\n\n\n\n\nJoining\nThe situation with joins may feel like the opposite of aggregation. Here, R and python’s most popular data wrangling packages are more permissive than SQL.\nAs we saw in the case study, SQL does not match on nulls.\nConsider tbl1 and tbl2 as shown below:\n\nselect * from tbl1\n\n\n\nA\nB\nX\n\n\n\n\n1\nNA\nTRUE\n\n\n\n\n\nselect * from tbl2\n\n\n\nA\nB\nY\n\n\n\n\n1\nNA\nFALSE\n\n\n\n\nAttempts to join return no results:\n\nselect tbl1.*, tbl2.Y \nfrom \n  tbl1 inner join tbl2 \n  on \n  tbl1.A = tbl2.A and \n  tbl1.B = tbl2.B\n\n\n\nA\nB\nX\ny\n\n\n\n\n\n\nIn contrast, default behavior for base R’s merge and dplyr does match on nulls. (Although, either behavior can be altered with the incomparables or na_matches arguments, respectively.)\n\ndf1 &lt;- data.frame(A = 1, B = NA, X = TRUE)\ndf2 &lt;- data.frame(A = 1, B = NA, Y = FALSE)\nmerge(df1, df2, by = c(\"A\", \"B\"))\ndplyr::inner_join(df1, df2, by = c(\"A\", \"B\"))\n\n\n\n\nA\nB\nX\nY\n\n\n\n\n1\nNA\nTRUE\nFALSE\n\n\n\n\n\n\nA\nB\nX\nY\n\n\n\n\n1\nNA\nTRUE\nFALSE\n\n\n\n\n\nSimilarly, pandas also matches on nulls for joining.\n\nimport numpy as np\nimport pandas as pd\ndf1 = pd.DataFrame([[1, np.nan, True]], columns = ['A','B','X'])\ndf2 = pd.DataFrame([[1, np.nan, False]], columns = ['A','B','Y'])\npd.merge(df1, df2, on = ['A','B'])\n\n   A   B     X      Y\n0  1 NaN  True  False\n\n\nR and python’s behavior here seems most surprising. One might expect joining to work the same as raw logical evaluation works. However, neither language “likes” null comparison in its raw form. Instead, the default behavior is intentionally altered in these higher-level joining functions.\nIn R:\n\nNA == NA\n\n[1] NA\n\n\nIn python:\n\nnp.nan == np.nan\n\nFalse\n\n\n\n\nFiltering\nFinally, both SQL and R drop null records used in filtering statements since comparisons with these values are incapable of returning a TRUE/FALSE value that is used to subset the rows. In python, however, pandas does preserve nulls in filter conditions.\nUsing the same tbl1 shown above, we can also confirm that SQL proactively drops nulls in where clauses where they cannot be readily compared to non-null values. This seems quite consistent with its behavior in the joining case.\n\nselect A, B, X \nfrom tbl1 \nwhere B != 1\n\n\n\na\nb\nx\n\n\n\n\n\n\nBoth base R and dplyr paradigms follow suit here.\n\ndf1 &lt;- data.frame(A = 1, B = NA, X = TRUE)\ndf1[df1$B != 1,]\ndplyr::filter(df1, B != 1)\n\n\n\n\n\nA\nB\nX\n\n\n\n\nNA\nNA\nNA\nNA\n\n\n\n\n\n\nA\nB\nX\n\n\n\n\n\n\n\nHowever, bucking the trend, multiple approaches to subsetting pandas data will not drop nulls in filtering comparisons.\n\ndf1 = pd.DataFrame([[1, np.nan, True]], columns = ['A','B','X'])\ndf1[df1.B != 1]\ndf1.query('B != 1')\n\n   A   B     X\n0  1 NaN  True\n   A   B     X\n0  1 NaN  True"
  },
  {
    "objectID": "post/nulls-polyglot/index.html#conclusion",
    "href": "post/nulls-polyglot/index.html#conclusion",
    "title": "Oh, I’m sure it’s probably nothing",
    "section": "Conclusion",
    "text": "Conclusion\nIn data computation and analysis, the devil is often in the details. It’s not breaking news that low-level reasoning on the careful handling of null values can jeopardize the resulting analyses. However, as analysts take on increasingly complex tasks and using a plehora of different tools, it’s more important than ever for both data producers and consumers to consider the choices they are making in encoding and handling these values across the stack."
  },
  {
    "objectID": "post/latent-lasting-documentation/index.html",
    "href": "post/latent-lasting-documentation/index.html",
    "title": "Resource Round-Up: Latent and Lasting Documentation",
    "section": "",
    "text": "Photo by Sharon McCutcheon on Unsplash\nThe importance of documentation is uncontroversial. For many data and analytical products, documentation is the user interface and key to promoting user success and future reuse. However, when project timelines get tight, too many data products are considered complete without appropriate documentation. Even when these resources initially exist, they too quickly grow stale as they are not maintained along with their projects.\nIdeally, documentation should be latent (proactively captured from artifacts of the development and maintenance process) and lasting (easy to maintain). This can be accomplished by treating documentation more like code itself and incorporating principles of reuse and modularity.\nLately, I’ve encountered a number of related concepts which aim to address these issues. In this post, I will share a few good reads and key take-aways on how to craft documentation that is both latent and lasting."
  },
  {
    "objectID": "post/latent-lasting-documentation/index.html#latent-documentation",
    "href": "post/latent-lasting-documentation/index.html#latent-documentation",
    "title": "Resource Round-Up: Latent and Lasting Documentation",
    "section": "Latent Documentation",
    "text": "Latent Documentation\nWe create latent documentation simply by doing existing aspects of our work more conscientiously. Throughout the design, development, and evangelization of any data product, developers necessarily think about and communicate what a tool is intended to do and how it works. Latent documentation, then, can be harvested by more conscientiously saving the mental artifacts we created while building and growing a new product. This can include documenting our project requirements when planning, adhering to strict naming conventions when building, and answering questions in public forums while maintaining.\nThe following articles provide compelling examples of each of those strategies in more detail.\nData Dictionary: A How To and Best Practices by Carl Anderson\nIn this article, Carl Anderson explains that proactively capturing the intent and requirements for a new data product can easily lead to high quality documentation. In particular, he demonstrates how data dictionaries can be used as a way to collect requirements and seek consensus from customers before a data product is built.\nThinking of documentation as an active part of product discovery makes it an uncontroversial task to invest in and leads to a better outcome by ensuring user needs are truly being met. This also makes documentation a collaborative exercise; engaging users in the process may offload developer from bearing the task of documentation alone and may even lead to documentation written more in the voice and from the perspective of the ultimate consumer.\nThis concept applies broadly beyond data dictionaries. For example, before building a custom R package, one can imagine writing a vignette with a detailed explanation of the problem that the package intends to solve and a conceptual approach to doing so. Customers could review this document to make sure they are aligned before any development begins, and ultimately this document could ship with the final package as part of the “why” documentation (as we will discuss in the Etsy article below.)\nNaming Things by Jenny Bryan\nDuring the development process, documentation can also be achieved by conscientious naming of folders and files in one’s project. Instead of having a lengthy README explaining what different files do and where to find them, defining a taxonomy of highly intentional naming standard makes the purpose of each file at least somewhat self-evident.\nThis approach makes projects more navigable to developers, maintainers, and users alike. Beyond reducing the burden of documentation, there are many additional benefits to conscientious naming conventions. To name a few, standardized naming eliminates mental overhead (you can understand instead of remember how files relate) and helps your code “play nice” with the auto-complete features in many modern IDEs.\nWhile the linked presentation focuses on files within a single project, standardized naming taxonomies are a powerful structure for organizing many different products – from naming variables in a data set to naming projects in a GitHub organization. The specific conventions imposed may vary, but the idea stays the same: define and document a taxonomy in one place and use the resulting “grammar” to flexibly describe each individual entity of that type (variable, file, folder, etc.)\nUnderstanding the InnerSource Checklist\nSilona Bonewald\nI’ve previously praised this book about PayPal’s innersource journey in my post on reproducible research resources. More specifically to this conversation, the book introduces passive documentation as an antidote to the burden of maintainership and user support for innersource developers. Passive documentation is the practice of pushing all user support to public and openly accessible forums (e.g. GitHub issues versus Slack DMs) so that future users can self-service from the conversations of past users with project maintainers (just as StackOverflow and GitHub issues are used in open source). This relationship can be formalized in many ways, such as labeling issues with a special tag (such as “faq”) or asking users to instead as questions by editing a wiki and sending a PR to a markdown file."
  },
  {
    "objectID": "post/latent-lasting-documentation/index.html#lasting-documentation",
    "href": "post/latent-lasting-documentation/index.html#lasting-documentation",
    "title": "Resource Round-Up: Latent and Lasting Documentation",
    "section": "Lasting Documentation",
    "text": "Lasting Documentation\nLasting documentation is documentation that requires minimal effort to maintain. Key strategies here are decoupling documentation of intent with specific implementation. Intent documentation is relatively stable over time and can help users learn about the process your product solves. Implementation documentation is more likely to change, but it can be further enhanced by being made dynamic and linked more directly to the core workflow. For example, this documentation can recycle material from unit tests or validation pipelines.\nThe following real-world examples demonstrate strategies for lasting documentation.\nEtsy’s Experiment with Immutable Documentation\nPaul-Jean Letourneau\nThis post from Etsy uses the example of API documentation to demonstrate that documentation can be largely broken into two categories: the how and the why. “How” documentation explains specific chunks of code whereas “why” documentation describes the motivations and intended effect of a process.\nConscious decoupling of these categories allows the “why” documents to exist stand-alone and require relatively little maintenance over time. Simultaneously, the “how” documents are more isolated and easily updated. The post goes on to demonstrate that leaner “how” documents have additional benefits. For example, it is easy to compare changes over time which are in themselves useful documentation.\nroxytest R package\nMikkel Meyer Anderson\nroxytest allows R package developers to build more lasting function documentation by recycling unit test cases as user-facing usage examples. Tests are critical to the maintenance of any substantial project; they ensure computational correctness and stability over time. Tests also are run regularly when changes are made to a package and are often automated in a CI pipeline. Since they ensure the validity of the code, they are neccesarily in-sync and up-to-date with said code (or there are bigger problems to worry about!) In contrast, usage examples are often written during initial package development and pose a higher risk of going stale or being neglected during updates.\nDespite their differences, both ultimately work by demonstrating that simple minimal-dependency function calls produce the expected results. roxytest exploits this shared aim by enabling developers to write unit tests within the main function documentation and then automatically reformatting these as required for human-readability in the documentation and for machine execution (across a variety of testing frameworks) as tests.\npointblank R package\nRich Iannone\npointblank is a promising new R package1 for data validation which offers a clever and automated approach to better data quality documentation. Key features include a flexible and friendly API for describing data validation checks, operation on local or remote data sets (e.g. in database or Spark), and workflows for either human-readable data quality reporting or pipeline automation.\nThese dual workflows are particularly relevant for documentation. Many data validation checks can be done as part of an automated pipeline to detect baseline quality issues (e.g. are there duplicate rows?); however, it is important that end-users understand what checks have been done in order to understand what additional validation is needed to ensure that the data is fit-for-purpose. Thus, automatically generating aesthetic and interactive reports on a data quality pipeline is an excellent compliment to other forms of data set documentation."
  },
  {
    "objectID": "post/latent-lasting-documentation/index.html#footnotes",
    "href": "post/latent-lasting-documentation/index.html#footnotes",
    "title": "Resource Round-Up: Latent and Lasting Documentation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInitially, it strikes me as the best answer in the R ecosystem to the highly successful Great Expectations project.↩︎"
  },
  {
    "objectID": "post/grouping-data-quality-update/index.html",
    "href": "post/grouping-data-quality-update/index.html",
    "title": "Update: grouped data quality check PR merged to dbt-utils",
    "section": "",
    "text": "Photo credit to Greyson Joralemon on Unsplash\nLast fall, I wrote about the unreasonably effectiveness of grouping in data quality checks. In this follow-up, I want to share that my pull request for such features has just been merged into the development branch of the dbt-utils package, a common add-on to the dbt data transformation stack. This feature will officially “go live” in the 1.0.0 version release that is planned for later this fall.\nIn this brief post, I’ll recall the benefits of such checks (which my original post further illustrates with NYC subway data) and demonstrate how these checks can now be implemented in dbt-utils.\nFor those interested, I’ll also provide a brief overview of how I implemented this change, but I recommend checking out the PR itself for complete details."
  },
  {
    "objectID": "post/grouping-data-quality-update/index.html#recap",
    "href": "post/grouping-data-quality-update/index.html#recap",
    "title": "Update: grouped data quality check PR merged to dbt-utils",
    "section": "Recap",
    "text": "Recap\nTo recap the benefits of such checks from my initial post:\n\nSome data checks can only be expressed within a group (e.g. ID values should be unique within a group but can be repeated between groups)\nSome data checks are more precise when done by group (e.g. not only should table row-counts be equal but the counts within each group should be equal)\n\nOf course, these benefits are more or less relevant to different types of data checks. My PR updates the following tests:\n\nequal_rowcount()\nrecency()\nfewer_rows_than()\nat_least_one()\nnot_constant()\nnon_null_proportion()\nsequential_values()\n\nOf these checks, most fall in the category of providing more rigor when being conducted at the group level. Only the sequential_values() test is often unable to be expressed without grouping."
  },
  {
    "objectID": "post/grouping-data-quality-update/index.html#demo",
    "href": "post/grouping-data-quality-update/index.html#demo",
    "title": "Update: grouped data quality check PR merged to dbt-utils",
    "section": "Demo",
    "text": "Demo\nData tests in dbt are specified in the schema.yml file for relevant models. Adding grouping to the tests listed above will now be as simple as adding a group_by_columns key-value pair to the tests, as desired, which accepts either a single variable name or a list of variables to be used for grouping.\n{yaml eval = FALSE}   - name: data_test_at_least_one     columns:       - name: field         tests:           - dbt_utils.at_least_one:               group_by_columns: ['grouping_column']\nFor those that have not used dbt’s data testing framework before, this configuration is then used to generate SQL (now with the custom GROUP BY clause) which are evaluated when dbt test is run."
  },
  {
    "objectID": "post/grouping-data-quality-update/index.html#implementation",
    "href": "post/grouping-data-quality-update/index.html#implementation",
    "title": "Update: grouped data quality check PR merged to dbt-utils",
    "section": "Implementation",
    "text": "Implementation\nIn implementing this PR, I considered a few core principles:\n\nMake this feature as unobtrusive and isolated as possible with respect to the macros broader implementation\nFollow standard DRY principles (e.g. specifically, render needed text as few times as possible)\nImplement consistently across macros\n\nWith these principles in mind, the majority of implementations are like that of the recency macro where all relevant SQL strings are pre-computed:\n{% set threshold = dbt_utils.dateadd(datepart, interval * -1, dbt_utils.current_timestamp()) %}\n{% if group_by_columns|length() &gt; 0 %}\n  {% set select_gb_cols = group_by_columns|join(' ,') + ', ' %}\n  {% set groupby_gb_cols = 'group by ' + group_by_columns|join(',') %}\n{% endif %}\nThe main deviations to this were the sequential() macro (requiring a window function) and the equal_rowcount()/fewer_rows_than() (requiring joins)"
  },
  {
    "objectID": "post/grouping-data-quality/index.html",
    "href": "post/grouping-data-quality/index.html",
    "title": "Make grouping a first-class citizen in data quality checks",
    "section": "",
    "text": "Photo credit to Greyson Joralemon on Unsplash\nWhich of these numbers doesn’t belong? -1, 0, 1, NA.\nIt may be hard to tell. If the data in question should be non-negative, -1 is clearly wrong; if it should be complete, the NA is problematic; if it represents the signs to be used in summation, 0 is questionable. In short, there is no data quality without data context.\nThe past few years have seen an explosion in different solutions for monitoring in-production data quality. These tools, including software like dbt and Great Expectations as well as platforms like Monte Carlo, bring a more DevOps flavor to data production with important functionality like automated testing within pipelines (not just at the end), expressive and configurable semantics for common data checks, and more.\nHowever, despite all these features, I notice a common gap across the landscape which may limit the ability of these tools to encode richer domain context and detect common classes of data failures. I previously wrote about the importance of validating data based on its data generating process – both along the technical and conceptual dimensions. Following this logic, an important and lacking functionality1 across the data quality monitoring landscape, is the ability to readily apply checks separately to groups of data. On a quick survey, I count about 8/14 dbt tests (from the add-on dbt-utils package), 15/37 Great Expectations column tests, and most all of the Monte Carlo field health metrics that would be improved with first-class grouping functionality. (Lists at the bottom of the post.)\nGroup-based checks can be important for fully articulating good “business rules” against which to assess data quality. For example, groups could reflect either computationally-relevant dimensions of the ETL process (e.g. data loaded from different sources) or semantically-relevant dimensions of the real-world process that our data captures (e.g. repeated measures pertaining to many individual customers, patients, product lines, etc.)\nIn this post, I make my case for why grouping should be a first-class citizen in data quality tooling."
  },
  {
    "objectID": "post/grouping-data-quality/index.html#use-cases",
    "href": "post/grouping-data-quality/index.html#use-cases",
    "title": "Make grouping a first-class citizen in data quality checks",
    "section": "Use Cases",
    "text": "Use Cases\nThere are three main use-cases for enabling easy data quality checks by group: checks that can only be expressed by group, checks that can be more rigorous by group, and checks that are more semantically intuitive by group.\nSome checks can be more rigorous by group. Consider a recency check (i.e. that the maximum date represented in the data is appropriately close to the present.) If the data loads from multiple sources (e.g. customer acquisitions from web and mobile perhaps logging to different source systems), the maximum value of the field could pass the check if any one source loaded, but unless the data is grouped in such a way that reflects different data sources and each group’s maximum date is checked, stale data could go undetected.\nSome types of checks can only be expressed by group. Consider a check for consecutive data values. If a table that measures some sort of user engagements, there might be fields for the USER_ID and MONTHS_SINCE_ACQUISITION. A month counter will most certainly not be strictly increasing across the entire dataset but absolutely should be monotonic within each user.\nSome checks are more semantically intuitive by group. Consider a uniqueness check for the same example as above. The month counter is also not unique across the whole table but could be checked for uniqueness within each user. Group semantics would not be required to accomplish this; a simple USER_ID x MONTHS_SINCE_ACQUISITION composite variable could be produced and checked for uniqueness. However, it feels cumbersome and less semantically meaningful to derive additional fields just to fully check the properties of existing fields. (But for the other two categories, this alone would not justify adding such new features.)\nThese categories demonstrate the specific use cases for grouped data checks. However, there are also soft benefits.\nMost prevalent, having group-based checks be first class citizens opens up a new pit of success2. If you believe, as do I, that this is a often a fundamentally important aspect of confirming data quality and not getting “false promises” (as in the first case where non-grouped checks are less rigorous), configuring checks this way should be as close to zero-friction as possible."
  },
  {
    "objectID": "post/grouping-data-quality/index.html#demo-nyc-turnstile-data",
    "href": "post/grouping-data-quality/index.html#demo-nyc-turnstile-data",
    "title": "Make grouping a first-class citizen in data quality checks",
    "section": "Demo: NYC turnstile data",
    "text": "Demo: NYC turnstile data\n\nContext\nTo better motivate this need, we will look at a real-world example. For this, I turn to New York City’s subway turnstile data which I can always count on to have plenty of data quality quirks (which, to be clear, I do not say as a criticism. There’s nothing unexpected about this in real-world “data as residue” data.)\nSpecifically, we’ll pull down data extracts for roughly the first quarter of 2020 (published on Jan 11 - April 11, corresponding to weeks ending Jan 10 - April 10.)3\nThis data contains 2869965 records, corresponding to one unique record per unique control area (CA), turnstile unit (UNIT), and individual turnstile device (SCP) at internals of four hours of the day.\n\nlibrary(dplyr)\nnrow(full_data)\n\n[1] 2869965\n\nnrow(distinct(full_data, CA, UNIT, SCP, DATE, TIME))\n\n[1] 2869965\n\n\nBecause this is raw turnstile data, the values for ENTRIES and EXITS may not be what one first expects. These fields contain the cumulative number of turns of the turnstile since it was last zeroed-out. Thus, to get the actual number of incremental entries during a given time period, one must take the difference between the current and previous number of entries at the turnstile level. Thus, missing or corrupted values4 could cascade in unpredictable ways throughout the transformation process.\nLooking at the data for a single turnstile unit makes the way this data is encoded more clear:\n\nfull_data %&gt;%\n  filter(CA == \"A002\", \n         UNIT == \"R051\", \n         SCP == \"02-00-00\", \n         DATE == \"2020-01-04\") %&gt;%\n  arrange(TIME) %&gt;%\n  select(TIME, ENTRIES, EXITS)\n\n# A tibble: 6 x 3\n  TIME       ENTRIES   EXITS\n  &lt;hms&gt;        &lt;int&gt;   &lt;int&gt;\n1 10800 secs 7331213 2484849\n2 25200 secs 7331224 2484861\n3 39600 secs 7331281 2484936\n4 54000 secs 7331454 2485014\n5 68400 secs 7331759 2485106\n6 82800 secs 7331951 2485166\n\n\n\n\nQuality checks\nSo how does this map to the use cases above?\nFirst, lets consider checks that are more rigorous by group. One example of this is checking for completeness of the range of data.\nLooking at the aggregate level, the data appears complete. The minimum data is the correct start date for the week ending Jan 10 and the maximum date is the correct end date for the week ending April 10.\n\nsummarize(full_data,\n          min(DATE),\n          max(DATE))\n\n# A tibble: 1 x 2\n  `min(DATE)` `max(DATE)`\n  &lt;date&gt;      &lt;date&gt;     \n1 2020-01-04  2020-04-10 \n\n\nThis check might provide a false sense of security.\nHowever, what happens if we repeat this check at the actual grain of records which we need to be complete and subsequent calculations probably assume5 are complete? We find many individual units whose data does not appear appropriately recent.\n\nfull_data %&gt;%\n  group_by(CA, UNIT, SCP) %&gt;%\n  summarize(MAX = max(DATE)) %&gt;%\n  filter(MAX != \"2020-04-10\")\n\n`summarise()` has grouped output by 'CA', 'UNIT'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 54 x 4\n# Groups:   CA, UNIT [24]\n   CA    UNIT  SCP      MAX       \n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;date&gt;    \n 1 C021  R212  00-00-00 2020-01-06\n 2 C021  R212  00-00-01 2020-01-06\n 3 C021  R212  00-00-02 2020-01-06\n 4 C021  R212  00-00-03 2020-01-06\n 5 H007  R248  00-00-00 2020-03-07\n 6 H007  R248  00-00-01 2020-02-15\n 7 H007  R248  00-03-00 2020-02-15\n 8 H007  R248  00-03-01 2020-02-15\n 9 H007  R248  00-03-02 2020-02-15\n10 H009  R235  00-03-04 2020-03-20\n# i 44 more rows\n\n\nNext, consider checks that are only expressible by group. One example of this is a monotonicity (value always increasing) check.\nFor example, ENTRIES is certainly not monotonically increasing at the row level. Each individual turnstile device is counting up according to its own usage. However, in an ideal world, these fields should be monotonic over time at the level of individual devices. (Spoiler alert: due to the maintenance, malfunction, and maxing-out scenarios mentioned above, it’s not.) Thus, this type of check is only possible at the grouped level.\n\nfull_data %&gt;%\n  group_by(CA, UNIT, SCP) %&gt;%\n  arrange(DATE, TIME) %&gt;%\n  mutate(LAG_ENTRIES = lag(ENTRIES)) %&gt;%\n  filter(ENTRIES &lt; LAG_ENTRIES, DATE &gt; '2020-01-25') %&gt;%\n  select(CA, UNIT, SCP, DATE, TIME, ENTRIES, LAG_ENTRIES) %&gt;%\n  arrange(ENTRIES - LAG_ENTRIES)\n\n# A tibble: 19,281 x 7\n# Groups:   CA, UNIT, SCP [262]\n   CA    UNIT  SCP      DATE       TIME        ENTRIES LAG_ENTRIES\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;    &lt;date&gt;     &lt;hms&gt;         &lt;int&gt;       &lt;int&gt;\n 1 R231  R176  00-00-05 2020-03-09 75600 secs       99  1054865694\n 2 R412  R146  00-03-03 2020-02-04 43200 secs 51627403   318991420\n 3 N029  R333  01-00-00 2020-03-15 75600 secs       18   168628048\n 4 R327  R361  01-06-00 2020-03-03 72000 secs   524397   135382887\n 5 R312  R405  00-05-00 2020-02-16 57600 secs   131089   118174528\n 6 N091  R029  02-05-00 2020-02-01 54000 secs   524368   118146213\n 7 A025  R023  01-06-00 2020-03-04 82800 secs 11525743    67822764\n 8 A025  R023  01-00-00 2020-03-04 82800 secs  5276291    28448967\n 9 R238  R046  00-03-00 2020-02-15 82800 secs        5    16336060\n10 R533  R055  00-03-04 2020-03-14 43200 secs      104    15209650\n# i 19,271 more rows"
  },
  {
    "objectID": "post/grouping-data-quality/index.html#alternatives-considered",
    "href": "post/grouping-data-quality/index.html#alternatives-considered",
    "title": "Make grouping a first-class citizen in data quality checks",
    "section": "Alternatives Considered",
    "text": "Alternatives Considered\nGiven that this post is, to some extent, a feature request across all data quality tools ever, it’s only polite to discuss downsides and alternative solutions that I considered. Clearly, finer-grained checks incur a greater computational cost and could, in some cases, be achieved via other means.\nGrouped data check might seem excessive. After all, data quality checks do not, perhaps, aim to guarantee every field is 100% correct. Rather, they are higher-level metrics which aim to catch signals of deeper issues. My counterargument is largely based in the first use case listed above. Without testing data at the right level of granularity, checks could almost do more harm than good if they promote a false sense of data quality by masking issues.\nNot all grains of data are equally likely to break. Taking the previous point into account, we likely cannot check everything, so we ought to focus our attention on some combination of the most “important” errors and the most “likely” errors. In the subway example, turnstile-level failures are likely because each individual turnstile is a sensor that is independently involved in the data collection process and can break in its own unique ways. However, for something like a clickstream for different users on a website, the data collection process is centralized, so it would be less like (and infeasible to check) for individual customer-level data to break in dramatically different ways.\nHigh-risk grouped data is possibly ungrouped further upstream. Following the logic that grouped data is more dangerous if groups denote units responsible for their own data collection, in theory this data is being transmitted separately at some point in the pipeline. Thus, in some cases it could be checked before it is grouped. However, we cannot always get infinitely far upstream in the data pipeline as some pieces may be outside of our control or produced atomically by a third-party platform.\nSome grouped checks can be achieved in other ways. Some (but not all) of these checks can be mocked by creating composite variables, using other built-in features6, or writing custom checks 7. However, there solutions seem to defy part of the benefits of these tools: semantically meaningful checks wrapped in readable syntax and ready for use out-of-the-box. This also implies that grouped operations are far less than first-class citizens. This also limits the ability to make use of some of the excellent functionality these tools offer for documenting data quality checks in metadata and reporting on their outcomes.\nI also considered the possibility that this is a niche, personal need moreso than a general one because I work with a lot of panel data. However, I generally believe most data is nested in some way. I can at least confirm that I’ve not completely alone in this desire with a peak at GitHub issue feature requests in different data quality tools. For example, three stale stale GitHub issues on the Great Expectations repo (1, 2, 3) request similar functionality."
  },
  {
    "objectID": "post/grouping-data-quality/index.html#downsides",
    "href": "post/grouping-data-quality/index.html#downsides",
    "title": "Make grouping a first-class citizen in data quality checks",
    "section": "Downsides",
    "text": "Downsides\nThere’s no such thing as a free lunch or a free feature enhancement. My point is in no way to criticize existing data quality tools that do not have this functionality. Designing any tool is a process of trade-offs, and it’s only fair to discuss the downsides. These issues are exacerbated further when adding “just one more thing” to mature, heavily used tools as opposed to developing new ones.\nGrouped checks are more computational expensive. Partitioning and grouping can make data check operations more expensive by disabling certain computational shortcuts8 and requiring more total data to be retained. This is particularly true if the data is indexed or partitioned along different dimensions than the groups used for checks. The extra time required to run more fine-grained checks could become intractable or at least unappealing, particularly in an interactive or continuous integration context. However, in many cases it could be a better use of time to more rigorously test recently loaded data as opposed to (or in conjunction with) running higher-level checks across larger swaths of data.\nAPI bloat makes tools less navigable. Any new feature has to be documented by developers and comprehended by users. Having too many “first-class citizen” features can lead to features being ignored, unknown, or misused. It’s easy to point to any one feature in isolation and claim it is important; it’s much harder to stare at a full backlog and decide where the benefits are worth the cost.\nIncremental functionality adds more overhead. Every new feature demands careful programming and testing. Beyond that, there’s a substantial mental tax in thinking through how that feature needs to interact with existing functionality while, at the same time, preserving backwards compatibility.\nEvery feature built means a different one isn’t. As a software user, it’s easy to have a great idea for a feature that should absolutely be added. That’s a far different challenge than that faced by the developers and maintainers who must prioritize a rich backlog full of competing priorities."
  },
  {
    "objectID": "post/grouping-data-quality/index.html#survey-of-available-tools",
    "href": "post/grouping-data-quality/index.html#survey-of-available-tools",
    "title": "Make grouping a first-class citizen in data quality checks",
    "section": "Survey of available tools",
    "text": "Survey of available tools\nMy goal is in no way to critique any of the amazing, feature-rich data quality tools available today. However, to further illustrate my point, I pulled down key data checks from a few prominent packages to assess how many of their tests would be potentially enhanced with the ability to provided grouping parameters. Below are lists for dbt-utils, Great Expectations, and Monte Carlo with relevant tests in bold.\n\ndbt-utils (8 / 14)\n\nequal_rowcount\nequality\nexpression_is_true\nrecency\n\nat_least_one\nnot_constant\ncardinality_equality\nunique_where\nnot_null_where\n\nnot_null_proportion\nrelationships_where\nmutually_exclusive_ranges\nunique_combination_of_columns (but less important - only for semantics)\naccepted_range\n\n\n\nGreat Expectations (15 / 37)\n\nexpect_column_values_to_be_unique (but less important - only for semantics)\nexpect_column_values_to_not_be_null\n\nexpect_column_values_to_be_null\n\nexpect_column_values_to_be_of_type\n\nexpect_column_values_to_be_in_type_list\nexpect_column_values_to_be_in_set\nexpect_column_values_to_not_be_in_set\nexpect_column_values_to_be_between\n\nexpect_column_values_to_be_increasing\nexpect_column_values_to_be_decreasing\nexpect_column_value_lengths_to_be_between\nexpect_column_value_lengths_to_equal\nexpect_column_values_to_match_regex\nexpect_column_values_to_not_match_regex\nexpect_column_values_to_match_regex_list\nexpect_column_values_to_not_match_regex_list\nexpect_column_values_to_match_like_pattern\nexpect_column_values_to_not_match_like_pattern\nexpect_column_values_to_match_like_pattern_list\nexpect_column_values_to_not_match_like_pattern_list\nexpect_column_values_to_match_strftime_format\nexpect_column_values_to_be_dateutil_parseable\nexpect_column_values_to_be_json_parseable\nexpect_column_values_to_match_json_schema\nexpect_column_distinct_values_to_be_in_set\nexpect_column_distinct_values_to_contain_set\nexpect_column_distinct_values_to_equal_set\nexpect_column_mean_to_be_between\nexpect_column_median_to_be_between\nexpect_column_quantile_values_to_be_between\nexpect_column_stdev_to_be_between\nexpect_column_unique_value_count_to_be_between\nexpect_column_proportion_of_unique_values_to_be_between\nexpect_column_most_common_value_to_be_in_set\nexpect_column_max_to_be_between\nexpect_column_min_to_be_between\nexpect_column_sum_to_be_between\n\n\n\nMonte Carlo (All)\nAny of Monte Carlo’s checks might be more sensitive to detecting changes with subgrouping. Since these “health metrics” tend to represent distributional properties, it can be useful to ensure that “good groups” aren’t pulling down the average value and masking errors in “bad groups”.\n\nPct NUll\n\nPct Unique\nPct Zero\n\nPct Negative\n\nMin\np20\np40\np60\np80\nMean\nStd\nMax\nPct Whitespace\nPct Integer\nPct “Null”/“None”\nPct Float\nPct UUID"
  },
  {
    "objectID": "post/grouping-data-quality/index.html#code-appendix",
    "href": "post/grouping-data-quality/index.html#code-appendix",
    "title": "Make grouping a first-class citizen in data quality checks",
    "section": "Code Appendix",
    "text": "Code Appendix\n\n# data source: http://web.mta.info/developers/turnstile.html\nlibrary(ggplot2)\nlibrary(readr)\n\n# define read function with schema ----\nread_data &lt;- function(url) {\n  \n  readr::read_csv(url,\n                  col_names = TRUE,\n                  col_types =\n                    cols(\n                      `C/A` = col_character(),\n                      UNIT = col_character(),\n                      SCP = col_character(),\n                      STATION = col_character(),\n                      LINENAME = col_character(),\n                      DIVISION = col_character(),\n                      DATE = col_date(format = \"%m/%d/%Y\"),\n                      TIME = col_time(format = \"\"),\n                      DESC = col_character(),\n                      ENTRIES = col_integer(),\n                      EXITS = col_integer()\n                    ))\n  \n}\n\n# ridership data ----\ndates &lt;- seq.Date(from = as.Date('2020-01-11'), to =, as.Date('2020-04-11'), by = '7 days')\ndates_str &lt;- format(dates, format = '%y%m%d')\ndates_url &lt;- sprintf('http://web.mta.info/developers/data/nyct/turnstile/turnstile_%s.txt', dates_str)\ndatasets &lt;- lapply(dates_url, FUN = read_data)\nfull_data &lt;- do.call(rbind, datasets)\nfull_data &lt;- full_data[full_data$DESC == \"REGULAR\",]\nnames(full_data)[1] &lt;- \"CA\""
  },
  {
    "objectID": "post/grouping-data-quality/index.html#footnotes",
    "href": "post/grouping-data-quality/index.html#footnotes",
    "title": "Make grouping a first-class citizen in data quality checks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith the exception of pointblank which kindly entertained an issue I opened on this topic: https://github.com/rich-iannone/pointblank/issues/300↩︎\nThe “pit of success” is the idea that well-designed tools can help nudge people to do the “right” thing by default because its also the easiest. I first learned of it in a talk by Hadley Wickham, and it is originally attributed to Microsoft program manage Rico Mariani.↩︎\nCode for this pull is at the bottom of the post.↩︎\nThis can happen for many reasons including turnstile maintenance or replacement. My goal in this post is not to go into all of the nuances of this particular dataset, of which much has already been written, so I’m simplifying somewhat to keep it as a tractable motivating example.↩︎\nTransformations should probably never assume this. Any real ETL process using this data would like have to account for incompleteness in an automated fashion because it really is not a rare event. Again, we are simplifying here for the sake of example↩︎\nFor example, Great Expectations does offer conditional expectations which can be executed on manually-specified subsets of data. This could be a tractable solution for applying data checks to a small number of categorical variables, but less so for large or ill-defined categories like user IDs. More here: https://legacy.docs.greatexpectations.io/en/latest/reference/core_concepts/conditional_expectations.html↩︎\nLike, in the case of Great Expectation’s python-based API, writing custom code to partition data before applying checks, or, in the case of dbt/dbt-utils writing a custom macro.↩︎\nFor example, the maximum of a set of numbers is the maximum of the maximums of the subsets. Thus, if my data is distributed, I can find the max by comparing only summary statistics from the distributed subsets instead of pulling all of the raw data back together.↩︎"
  },
  {
    "objectID": "post/duckdb-carolina/index.html",
    "href": "post/duckdb-carolina/index.html",
    "title": "Goin’ to Carolina in my mind (or on my hard drive)",
    "section": "",
    "text": "Photo Credit to Element5 Digital on Unsplash\nThere comes a time in every analyst’s life when data becomes too big for their laptop’s RAM. While open-source tools like R, python, and SQL have made “team of one” data analysts ever more powerful, analysts abilities to derive value from their skillsets are highly interdependent with the tools at their disposal.\nFor R and python, the size of datasets becomes a limiting factor to local processing; for a SQL-focused analyst, the existence of a database is prerequisite, as the gap between “democratized” SQL querying skills and data engineering and database management skills is not insignificant. The ever-increasing number of managed cloud services (from data warehouses, containers, hosted IDEs and notebooks) offer a trendy and effective solution. However, budget constraints, technical know-how, security concerns, or tight-timelines can all be headwinds to adoption.\nSo what’s an analyst to do when they have the knowledge and tools but not the infrastructure to tackle their problem?\nDuckDB is quickly gaining popularity as a solution to some of these problems. DuckDB is a no-dependency, serverless database management system that can help parse massive amounts of data out-of-memory via familiar SQL, python, and R APIs. Key features include:\nThis combination of features can empower analysts to use what they have and what they know to ease into the processing of much larger datasets.\nIn this post, I’ll walk through a scrappy, minimum-viable setup for analysts using DuckDB, motivated by the North Carolina State Board of Election’s rich voter data. Those interested can follow along in this repo and put it to the test by launching a free 8GB RAM GitHub Codespaces.\nThis is very much not a demonstration of best practices of anything. It’s also not a technical benchmarking of the speed and capabilities of DuckDB versus alternatives. (That ground is well-trod. If interested, see a head-to-head to pandas or a matrix of comparisons across database alternatives.) If anything, it is perhaps a “user experience benchmark”, or a description of a minimum-viable set-up to help analysts use what they know to do what they need to do."
  },
  {
    "objectID": "post/duckdb-carolina/index.html#motivation-north-carolina-election-data",
    "href": "post/duckdb-carolina/index.html#motivation-north-carolina-election-data",
    "title": "Goin’ to Carolina in my mind (or on my hard drive)",
    "section": "Motivation: North Carolina election data",
    "text": "Motivation: North Carolina election data\nNorth Carolina (which began accepting ballots in early September for the upcoming November midterm elections) offers a rich collection of voter data, including daily-updating information on the current election, full voter registration data, and ten years of voting history.\n\nNC 2022 midterm early vote data from NCSBE (~6K records as-of 9/23 and growing fast!)\nNC voter registration file from NCSBE (~9M records / 3.7G unzipped, will be static for this cycle once registration closes in October)\nNC 10-year voter history file from NCSBE (~22M records / 5G unzipped, static)\n\nAll of these files are released as zipped full-population (as opposed to delta) CSV files.\nOne can imagine that this data is of great interest to campaign staff, political scientists, pollsters, and run-of-the-mill political junkies and prognosticators. However, the file sizes of registration and history data, which is critical for predicting turnout and detecting divergent trends, could be prohibitive.\nBeyond these files, analysis using this data could surely be enriched by additional third-party sources such as:\n\nCurrent Population Survey 2022 November voting supplement from US Census Bureau\nCounty-level past election results from MIT Election Lab via Harvard Dataverse\nCountless other data sources either from the US Census, public or internal campaign polls, organization-specific mobilizaton efforts, etc.\n\nYour mileage may vary based on your system RAM, but many run-of-the-mill consumer laptops might struggle to let R or python load all of this data into memory. Or, a SQL-focused analyst might yearn for a database to handle all these complex joins.\nSo how can DuckDB assist?"
  },
  {
    "objectID": "post/duckdb-carolina/index.html#duckdb-highlights",
    "href": "post/duckdb-carolina/index.html#duckdb-highlights",
    "title": "Goin’ to Carolina in my mind (or on my hard drive)",
    "section": "DuckDB highlights",
    "text": "DuckDB highlights\nTo explain, we’ll first level-set with a brief demo of some of the most relevant features of DuckDB.\nSuppose we have flat files of data, like a sample.csv (just many orders of magnitude larger!)\n\nimport pandas as pd\ndf = pd.DataFrame({'a':[1,2,3], 'b':[4,5,6], 'c':[7,8,9]})\ndf.head()\n\n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\ndf.to_csv('sample.csv', index = False)\n\nDuckDB can directly infer it’s schema and read it in a SQL-like interface by using functions like read_csv_auto() in the FROM clause.\n\nimport duckdb\ncon = duckdb.connect()\ndf = con.execute(\"select * from read_csv_auto('sample.csv')\").fetchdf()\ndf.head()\n\n   a  b  c\n0  1  4  7\n1  2  5  8\n2  3  6  9\n\ncon.close()\n\nWhile very useful, this is of course bulky to type. We may also set-up a persistent DuckDB database as a .duckdb file as save tables with CTAS statements, as with any normal relational database. Below, we create the sample-db.duckdb database and add one table and one view with our data.\n\ncon = duckdb.connect('sample-db.duckdb')\nctas = \"create or replace table sample as (select * from read_csv_auto('sample.csv'));\"\ncon.execute(ctas)\n\n&lt;duckdb.DuckDBPyConnection object at 0x0000000030026F70&gt;\n\ncvas = \"create or replace view sample_vw as (select * from read_csv_auto('sample.csv'));\" \ncon.execute(cvas)\n\n&lt;duckdb.DuckDBPyConnection object at 0x0000000030026F70&gt;\n\ncon.close()\n\nNow, suppose the data in sample.csv changes (now with 4 rows versus 3).\n\ndf = pd.DataFrame({'a':[1,2,3,4], 'b':[4,5,6,7], 'c':[7,8,9,10]})\ndf.to_csv('sample.csv', index = False)\n\nOur table stored the data directly within the database (“disconnected” from the file) so it remains the same as before whereas our view changed.\n\ncon = duckdb.connect('sample-db.duckdb')\ndf1 = con.execute(\"select count(1) from sample\").fetchdf()\ndf2 = con.execute(\"select count(1) from sample_vw\").fetchdf()\ncon.close()\ndf1.head()\n\n   count(1)\n0         3\n\ndf2.head()\n\n   count(1)\n0         4\n\n\n(Here, I focus just on the features we will use; not strictly the coolest or most important. I highly encourage taking a spin through the docs for countless features not discussed – like directly querying from or fetching to pandas and Arrow formats, an alternative relational API, etc.)"
  },
  {
    "objectID": "post/duckdb-carolina/index.html#data-management-pattern",
    "href": "post/duckdb-carolina/index.html#data-management-pattern",
    "title": "Goin’ to Carolina in my mind (or on my hard drive)",
    "section": "Data management pattern",
    "text": "Data management pattern\nWith these features in mind, we return to the problem at hand. How can an analyst mimic the experience of having the infrastructure needed to do their work?\nOne approach could look something like the following. As a one-time exercise someone would:\n\nDownload all relevant files\n(Optionally) Convert large, static files to Parquet versus CSV. DuckDB handles both well, but Parquet has some benefits that we’ll discuss in the next section\nCreate a DuckDB database with references to the files as views\n\nThen, any analyst wanting to interact with the data could:\n\nInteract with DuckDB as with any database connection\nWhenever needed, re-download the files to the same name/directory to “refresh” the “database”\n\nThe nc-votes-duckdb GitHub repo shows this flow in practice. If you want to follow along, you can click Code &gt; Create codespaces on master and follow the more detailed instructions in the README.md or at the bottom of this post.\n\nOne-time set-up\nThe scripts for the first set of steps are in the etl subdirectory. The e-step (extract) isn’t all that interesting – just some basic python scripts for downloading files from the internet, unzipping, and moving them around. These land the raw data in the data/raw subdirectory.\nData transformation mostly involves converting large CSVs to Parquet format (and dropping personally-identifying fields from the data on principle). As mentioned above, this step is optional but has some benefits. First, if one person is “configuring” a database for many analysts, Parquet compression makes files smaller for storage and sharing. Second, at query-time Parquet is:\n\nMore reliably structured with a well-defined schema\nFaster to retrieve due to columnar storage\nAble to be pruned by a savvy database optimizer (when appropriately partitioned by columns relevant to common query patterns)\n\nConversion from CSV to Parquet itself can be done with DuckDB. However, as of writing, I don’t believe that writing to a Hive-partitioned dataset is possible, so for this step, I used pyarrow, the python interface to Apache Arrow (another promising, memory-conserving data processing framework.)\nThis snippet from etl/transform-register.py demonstrates streaming a CSV by chunk and writing it out to county-level partitions:\n\n# convert to hive-partitioned parquet\nif os.path.exists(path_temp):\n    shutil.rmtree(path_temp)\n\nwith csv.open_csv(path_raw, \n                  convert_options= opts_convr_reg, \n                  parse_options = opts_parse,\n                  read_options = opts_read_reg) as reader:\n\n    for next_chunk in reader:\n        if next_chunk is None:\n            break\n        tbl = pa.Table.from_batches([next_chunk])\n        pq.write_to_dataset(\n                tbl,\n                root_path = path_temp,\n                use_dictionary = cols_reg_dict,\n                partition_cols= ['county_id']\n        )\n\n(Notably: counties are rather imbalanced in size and not the most important geography in many election contexts. This is for example purpose only, but partitions should always be picked based on how you expect to use the data. )\nOnce all the data in transformed, we can “load” our DuckDB database with relative-path references to our data. Again, this step can be done through any DuckDB API or the command line. Below, I use python in the etl/load-db.py to create the nc.duckdb database and create references to the different datasets.\n\nimport duckdb\nimport os\n\n# clean-up if already exists\nif os.path.exists('nc.duckdb'):\n  os.remove('nc.duckdb')\n\n# create new duckdb files \ncon = duckdb.connect('nc.duckdb')\n\n# generate SQL to register tables\ntemplate = \"\"\"\n  CREATE VIEW {view_name} as \n  (select * from read_parquet('{path}'{opts}))\n  \"\"\"\ndata_dict = {\n  'early_vote': 'data/early_vt.parquet',\n  'hist_gen': 'data/history_general/*/*.parquet',\n  'hist_oth': 'data/history_other/*/*.parquet',\n  'register': 'data/register/*/*.parquet',\n  'cps_suppl': 'data/cps_suppl.parquet'\n}\npartitioned = ['hist_gen', 'hist_pri', 'register']\n\nfor k,v in data_dict.items():\n\n  print(\"Loading {view_name} data...\".format(view_name = k))\n  opt = ', HIVE_PARTITIONING=1' if k in partitioned else ''\n  cvas = template.format(view_name = k, path = v, opts = opt)\n  con.execute(cvas)\n\ncon.close()\n\nSimilarly, other views could be defined as desired that query these views to do further data transformation.\n\n\nOngoing usage\nDue to the decoupling of storage and compute, ongoing data management is nearly trivial. With this “infrastructure” set-up, analysts would need only to selectively redownload any changed datasets (in my project, using the extract-.*.py scripts as needed) to allow their queries to pull in the latest data.\nBig picture, that means that (after initial set-up) an analyst would have no more overhead “managing their database” than they would with a “typical” CSV-driven workflow. Specifically for this project, the early votes data is the only dataset that changes frequently. For ease-of-use, it could even be left in CSV format to make the download process even easier for any users."
  },
  {
    "objectID": "post/duckdb-carolina/index.html#data-access-patterns",
    "href": "post/duckdb-carolina/index.html#data-access-patterns",
    "title": "Goin’ to Carolina in my mind (or on my hard drive)",
    "section": "Data access patterns",
    "text": "Data access patterns\nWith this set-up in place, analysts can then use their favorite tools to query the data.\n\npython\n\nimport duckdb\ncon = duckdb.connect('nc.duckdb')\ndf = con.execute('select count(1) from early_vote').fetchdf()\ncon.close()\n\n\n\nR\n\nlibrary(duckdb)\ncon &lt;- dbConnect( duckdb('nc.duckdb') )\ndf &lt;- dbGetQuery('select count(1) from early_vote')\ndbDisconnect(con, shutdown = TRUE)\n\n\n\nCLI\nduckdb my-db.duckdb\n&gt; select count(1) from early_vote\n\n\nSQL IDE (DBeaver)\nDuckDB also works with open-source database IDEs like DBeaver for the full, “traditional” database experience. The DuckDB website gives full set-up instructions. With DBeaver, analysts get the “full” database experience with navigable access to table schemas and metadata.\n\n\nNotably if you are using relative file paths in your view definitions, you have to launch DBeaver from your command line after moving into the appropriate working directory. (Thanks to Elliana May on Twitter for the pointer.) (In the terminal: cd my/dir/path; dbeaver)"
  },
  {
    "objectID": "post/duckdb-carolina/index.html#codespaces-demo",
    "href": "post/duckdb-carolina/index.html#codespaces-demo",
    "title": "Goin’ to Carolina in my mind (or on my hard drive)",
    "section": "Codespaces Demo",
    "text": "Codespaces Demo\nSo can DuckDB help analysts wrangle the whole state of North Carolina with 8GB RAM? To find out, launch a GitHub Codespaces from the nc-votes-duckdb repo and see for yourself!\n\nLaunch on Codespaces\nSet-up environment:\n\npython3 -m venv venv\nsource venv/bin/activate\npython3 -m pip install -r requirements.txt\n\nPull all raw data:\n\nchmod +x etl/extract-all.sh\netl/extract-all.sh\n\nTransform all raw data:\n\nchmod +x etl/transform-all.sh\netl/transform-all.sh\n\nCreate duckdb database:\n\npython etl/load-db.py\n\n(Optional) Install duckdb CLI\n\nchmod +x get-duckdb-cli.sh\n./get-duckdb-cli.sh\n\nRun sample queries\n\n7a. Run sample queries in CLI\nLaunch the CLI:\n./duckdb nc.duckdb\n.timer on\n(Note: you can exit CLI with Ctrl+D)\nTry out some sample queries. For example, we might wonder how many past general elections that early voters have voted in before:\nwith voter_general as (\nselect early_vote.ncid, count(1) as n\nfrom \n  early_vote \n  left join \n  hist_gen \n  on early_vote.ncid = hist_gen.ncid \ngroup by 1)\nselect n, count(1) as freq\nfrom voter_general\ngroup by 1\norder by 1\n;\nAnd, this question is more interesting if we join on registration data to learn how many prior general elections each voter was eligible to vote in:\nwith voter_general as (\nselect \n  early_vote.ncid, \n  extract('year' from register.registr_dt) as register_year, \n  count(1) as n\nfrom \n  early_vote \n  left join \n  hist_gen \n  on early_vote.ncid = hist_gen.ncid \n  left join\n  register\n  on early_vote.ncid = register.ncid\ngroup by 1,2)\nselect\n  n, \n  case \n  when register_year &lt; 2012 then 'Pre-2012'\n  else register_year\n  end as register_year,\n  count(1) as freq\nfrom voter_general\ngroup by 1,2\norder by 1,2\n;\n(Yes, of course date matters more than year here, etc. etc. This is purely to demonstrate duckdb not rigorous analysis!)\n7b. Run sample queries in python\nIn python: See sample queries in test-query.py file\n\nRun free in the terminal to marvel at what 8GB of RAM can do!"
  },
  {
    "objectID": "post/docs-closer-than-you-think/index.html",
    "href": "post/docs-closer-than-you-think/index.html",
    "title": "Crosspost: Why you’re closer to data documentation than you think",
    "section": "",
    "text": "Documentation can be a make-or-break for the success of a data initiative, but it’s too often considered an optional nice-to-have. I’m a big believer that writing is thinking. Similarly, documenting is planning, executing, and validating.\nPreviously, I’ve explored how we can create latent and lasting documentation of data products and how column names can be self documenting.\nRecently, I had the opportunity to expand on these ideas in a cross-post with Select Star. I argue that teams can produce high-quality and maintainable documentation with low overhead with a form of “documentation-driven development”. That is, smartly structuring and re-using artifacts from the development process into long-term documentation. For example:\n\nAt the planning stage:\n\nStructuring requirements docs in the form of data dictionaries\nCreating early alignment on higher-order concepts like entity definitions (and writing them down)\nMentally beta testing data usability with an entity-relationship diagram\n\nAt the development stage:\n\nEnsuring relevant parts of internal “development documentation” (e.g. dbt column definitions, docstrings) are published to a format and location accessible to users\nWith different information but similar motivation to ER diagrams, sharing the full orchestration DAG to help users trace column-level lineage and internalize how each field maps to a real-world data generating process\nSharing data tests being executed (the “user contract”) and their results\n\nThroughout the lifecycle:\n\nAnswering questions “in public” (e.g. Slack versus email) to create a searchable collection of insights\nProducing table usage statistics to help large, decentralized orgs capture the “wisdom of the crowds”\n\n\nIf you or your team works on data documentation, I’d love to hear what other patterns you’ve found to collect useful documentation assets during a data development process."
  },
  {
    "objectID": "post/data-error-gen/index.html",
    "href": "post/data-error-gen/index.html",
    "title": "Understanding the data (error) generating processes for data validation",
    "section": "",
    "text": "Statistics literature often makes reference to the data generating process (DGP): an idealized description of a real-world system responsible for producing observed data. This leads to a modeling approach focused on describing that system as opposed to blindly fitting observations to a common functional form.1\nAs a trivial example, if one wished to model the height of a group of adults, they might suppose that the height of women and the height of men each is normally distributed with separate means and standard deviations. Then the overall distribution of population heights could be models as a mixture of samples from these two distributions.2\nHowever, DGPs are not only useful for modeling. Conceptualizing the DGP of our observations can also lead to more principled data validation if we broaden the scope of the DGP to include the subsequent manufacturing of the data not just the originating mechanism.\nUnfortunately, consumers of analytical data may not always be familiar with the craft of data production (including data engineering, data modeling, and data management). Without an understanding of the general flow of data processing between collection and publication to a data warehouse, data consumers are less able to theorize about failure modes. Instead, similarly to blindly fitting models without an underlying theory, consumers may default to cursory checks of summary statistics without hypotheses for the kind of errors they are trying to detect or how these checks might help them.\nThis post explores the DGP of system-generated data and the common ways that these processes can introduce risks to data quality. As we discuss data validation, we will make reference to the six dimensions of data quality defined by DAMA: completeness, uniqueness, timeliness, validity, accuracy, and consistency. Along the way, we will explore how understanding how understanding key failure modes in the data production process can lead to more principled analytical data validation.3"
  },
  {
    "objectID": "post/data-error-gen/index.html#the-four-dgps-for-data-management",
    "href": "post/data-error-gen/index.html#the-four-dgps-for-data-management",
    "title": "Understanding the data (error) generating processes for data validation",
    "section": "The Four DGPs for Data Management",
    "text": "The Four DGPs for Data Management\nTo better theorize about data quality issues, it’s useful to think of four DGPs: the real-world DGP, the data collection/extraction DGP4, the data loading DGP, and the data transformation DGP.\n\nFor example, consider the role of each of these four DGPs for e-commerce data:\n\nReal-world DGP: Supply, demand, marketing, and a range of factors motivate a consumer to visit a website and make a purchase\nData collection DGP: Parts of the website are instrumented to log certain customer actions. This log is then extracted from the different operational system (login platforms, payment platforms, account records) to be used for analysis\nData loading DGP: Data recorded by different systems is moved to a data warehouse for further processing through some sort of manual, scheduled, or orchestrated job. These different systems may make data available at different frequencies.\nData transformation DGP: To arrive at that final data presentation requires creating a data model to describe domain-specific attributes with key variables crafted with data transformations\n\nOr, consider the role of each of these four DGPs for subway ridership data5:\n\nReal-world DGP: Riders are motivated to use public transportation to commute, run errands, or visit friends. Different motivating factors may cause different weekly and annual seasonality\nData collection DGP: To ride the subway, riders go to a station and enter and exit through turnstiles. The mechanical rotation of the turnstile caused by a rider passing through is recorded\nData loading DGP: Data recorded at each turnstile is collected through a centralized computer system at the station. Once a week, each station uploads a flat file of this data to a data lake owned by the city’s Department of Transportation\nData transformation DGP: Turnstiles from different companies may have different data formats. Transformation may include harmonizing disparate sources, coding system-generated codes (e.g. Station XYZ) to semantically meaningful names (e.g. Main Street Station), and publishing a final unified representation across stations and across time\n\nIn the next sections, we’ll explore how understanding key concepts about each of these DGPs can help build a consumers’ intuition on where to look for problems."
  },
  {
    "objectID": "post/data-error-gen/index.html#data-collection",
    "href": "post/data-error-gen/index.html#data-collection",
    "title": "Understanding the data (error) generating processes for data validation",
    "section": "Data Collection",
    "text": "Data Collection\nData collection is necessarily the first step in data production, but the very goal of data collection: translating complex human concepts into tabular data records is fraught with error. Data collection is effectively dimensionality reduction, and just like statistical dimensionality reduction, it must sometimes sacrifice accuracy for clarity.\nThis tradeoff makes data collection vulnerable to one of the largest risks to data validity: not that the data itself is incorrect given its stated purpose but rather that users misconstrue the population or metrics it includes. Thus, understanding what systems are intending to capture, publish, and extract and how they chose to encode information for those observations is essential for data validation and subsequent analysis.\nData collection can happen in countless different ways: experimentation, surveys, observation, sensors, etc. In many business settings, data is often extracted from source systems whose primary purpose is to execute some sort of real-world process.6 Such systems may naturally collect data for operational purposes or may be instrumented to collect and log data as they are used. This production data is then often extracted from a source system to an alternative location such as a data warehouse for analysis.\n\nWhat Counts\nOne of the tricky nuances of data collection is understanding what precisely is getting captured and logged in the first place.\nConsider something as simple as a login system where users must enter their credentials, endure a Captcha-like verification process to prove that they are not a robot, and enter a multi-factor authentication code.\n\nWhich of these events gets collected and recorded has a significant impact on subsequent data processing. In a technical sense, no inclusion/exclusion decision here is incorrect, persay, but if the producers’ choices don’t match the consumers’ understandings, it can lead to misleading results.\nFor example, an analyst might seek out a logins table in order to calculate the rate of successful website logins. Reasonably enough, they might compute this rate as the sum of successful events over the total. Now, suppose two users attempt to login to their account, and ultimately, one succeeds in accessing their private information and the other doesn’t. The analyst would probably hope to compute and report a 50% login success rate. However, depending on how the data is represented, they could quite easily compute nearly any value from 0% to 100%.\n\n\nPer Attempt: If data is logged once per overall login attempt, successful attempts only trigger one event, but a user who forgot their password may try (and fail) to login multiple times. In the case illustrated above, that deflates the successful login rate to 25%.\nPer Event: If the logins table contains a row for every login-related event, each ‘success’ will trigger a large number of positive events and each ‘failure’ will trigger a negative event preceded by zero or more positive events. In the case illustrated below, this inflates our successful login rate to 86%.\nPer Conditional: If the collector decided to only look at downstream events, perhaps to circumvent record duplication, they might decide to create a record only to denote the success or failure of the final step in the login process (MFA). However, login attempts that failed an upstream step would not generate any record for this stage because they’ve already fallen out of the funnel. In this case, the computed rate could reach 100%\nPer Intermediate: Similarly, if the login was defined specifically as successful password verification, the computed rate could his 100% even if some users subsequently fail MFA\n\n\n\n\n\nSession\nAttempt\nAttempt\nOutcome\nIntermediate\n\n\n\n\nSuccess\n1\n1\n6\n1\n2\n\n\nTotal\n2\n4\n7\n1\n2\n\n\nRate\n50%\n25%\n86%\n100%\n100%\n\n\n\nWhile humans have a shared intuition of what concepts like a user, session, or login are, the act of collecting data forces us to map that intuition onto an atomic event . Any misunderstanding in precisely what that definition is can have massive impact on the perceived data quality; “per event” data will appear heavily duplicated if it is assumed to be “per session” data.\nIn some cases, this could be obvious to detect. If the system outputs fields that are incredibly specific (e.g. with some hyperbole, imagine a step_in_the_login_process field with values taking any of the human-readable descriptions of the fifteen processes listed in the image above), but depending how this source is organized (e.g. in contrast to above, if we only have fields like sourceid and processid with unintuitive alphanumeric encoded values) and defined, it could be nearly impossible to understand the nuances without uncovering quality metadata or talking to a data producer.7\n\n\nWhat Doesn’t Count\nAlong with thinking about what does count (or gets logged), it’s equally important to understand what systemically does not generate a record. Consider users who have the intent or desire to login (motivated by a real-world DGP) but cannot find the login page, or users who load the login page but never click a button because they know that they’ve forgotten their password and see no way to request it. Often, some of these corner cases may be some of the most critical and informative (e.g. here, demonstrating some major flaws in our UI). It’s hard to computationally validate what data doesn’t exist, so conceptual data validation is critical.\n\n\nThe Many Meanings of Null\nRelated to the presence and absence of full records is the presence or absence of individual fields. If records contain some but not all relevant information, they may be published with explicitly missing fields or the full record may not be published at all.\n\nUnderstanding what the system implies by each explicitly missing data field is also critical for validation and analysis. Checks for data completeness usually include counting null values, but null data isn’t always incorrect. In fact, null data can be highly informative if we know what it means. Some meanings of null data might include:\n\nField is not relevant: Perhaps our logins table reports the mobile phone operating system (iOS or Android) that was used to access the login page to track platform-specific issues. However, there is no valid value for this\nRelevant value is not known: Our logins table might also have an account_id field which attempts to match login attempts to known accounts/customers using different metadata like cookies or IP addresses. In theory, almost everyone trying to log in should have an account identifier, but our methods may not be good enough to identify them in all cases\nRelevant value is null: Of course, sometimes someone without an account at all might try to log in for some reason. In this case, the correct value for an account_id field truly is null\nRelevant value was recorded incorrectly: Sometimes systems have glitches. Without a doubt, every single login attempt should have a timestamp, but such a field could be null if this data was somehow lost or corrupted at the source\n\nSimilarly, different systems might or might not report out these nulls in different ways such as:\n\nTrue nulls: Literally the entry in the resulting dataset is null\nNull-like non-nulls: Blank values like an empty string ('') that contain a null amount of information but won’t be detected when counting null values\nPlaceholder values: Meaningless values like an account_id of 00000000 for all unidentified accounts which preserve data validity (the expected structure) but have no intrinsic meaning\nSentinel/shadow values: Abnormal values which attempt to indicate the reasons for null-ness such as an account_id of -1 when no browser cookies were found or -2 when cookies were found but did not help link to any specific customer record\n\nEach of these encoding choices changes the definitions of appropriate completeness and validity for each field and, even more critically, impacts the expectations and assertions we should form for data accuracy. We can’t expect 100% completeness if nulls are a relevant value; we can’t check validity of ranges as easily if sentinel values are used with values that are outside the normal range (hopefully, or we have much bigger problems!) So, understanding how upstream systems should work is essential for assessing if they do work."
  },
  {
    "objectID": "post/data-error-gen/index.html#data-loading",
    "href": "post/data-error-gen/index.html#data-loading",
    "title": "Understanding the data (error) generating processes for data validation",
    "section": "Data Loading",
    "text": "Data Loading\nChecking that data contains expected and only expected records (that is, completeness, uniqueness, and timeliness) is one of the most common first steps in data validation. However, the superficially simple act of loading data into a data warehouse or updating data between tables can introduce a variety of risks to data completeness which require different strategies to detect. Data loading errors can result in data that is stale, missing, duplicate, inconsistently up-to-date across sources, or complete but for only a subset of the range you think.\nWhile the data quality principles of completeness, uniqueness, and timeliness would suggest that records should exist once and only once, the reality of many haphazard data loading process means data may appear sometime between zero and a handful of times. Data loads can occur in many different ways. For example, they might be:\n\nmanually executed\nscheduled (like a cron job)\norchestrated (with a tool like Airflow or Prefect)\n\nNo approach is free from challenges. For example, scheduled jobs risk executing before an upstream process has completed (resulting in stale or missing data); poorly orchestrated jobs may be prevented from working due to one missing dependency or might allow multiple stream to get out of sync (resulting in multisource missing data). Regardless of the method, all approaches must be carefully configured to handle failures gracefully to avoid creating duplicates, and the frequency at which they are executed may cause partial loading issues if it is incompatible with the granularity of the source data.\n\nData Load Failure Modes\nFor example, suppose in the diagram below that each row of boxes represents one day of records in a table.\n\n\nStale data occurs when the data is not as up-to-date as would be expected from is regular refresh cadence. This could happen if a manual step was skipped, a scheduled job was executed before the upstream source was available, or orchestrated data checks found errors and quarantined new records\nMissing data occurs when one data load fails but subsequent loads have succeeded\nDuplicate data occurs when one data load is executed multiple times\nMultisource missing data occurs when a table is loaded from multiple sources, and some have continued to update as expected while others have not\nPartial data occurs when a table is loaded correctly as intended by the producer but contains less data than expected by the consumer (e.g. a table loads ever 12 hours but because there is some data for a given date, the user assumes that all relevant records for that date have been loaded)\n\nThe differences in these failure modes become important when an analyst attempts to assess data completeness. One of the first approaches an analyst might consider is simply to check the min() and max() event dates in their table. However, this can only help detect stale data. To catch missing data, an analyst might instead attempt to count the number of distinct days represented in the data; to detect duplicate data, that analyst might need to count records by day and examine the pattern.\n\n\n\n\n\n\n\n\n\n\n\nMetric\nStale\nMissing\nDuplicate\nMulti\nPartial\n\n\n\n\nmin(date) max(date)\n13\n14\n14\n14\n14\n\n\ncount(distinct date)\n3\n3\n4\n4\n4\n\n\ncount(1) by date\n1001001000\n1001000100\n100100200100\n1001006666\n10010010050\n\n\ncount(1) count(distinct PKs)\n300300\n300300\n400300\n332332\n350350\n\n\n\nIn a case like the toy example above where the correct number of rows per date is highly predictable and the number of dates is small, such eyeballing is feasible; however when the expected number of records varies day-to-day or time series are long, this approach becomes subjective, error-prone, and intractable. Additionally, it still might be hard to catch errors in mutli-source data or partial loads if the lower number of records was still within the bounds of reasonable deviation for a series. These last two types deserve further exploration.\n\n\nMulti-Source\nA more effective strategy for assessing data completeness requires a better understanding of how data is being collected and loaded. In the case of multi-source data, one single source stopping loading may not be a big enough change to disrupt aggregate counts but could still jeopardize meaningful analysis. It would be more useful to conduct completeness checks by subgroup to identify these discrepancies.\nBut not any subgroup will do; the subgroup must correspond to the various data sources. For example, suppose we run an e-commerce store and wish to look at sales from the past month by category. Naturally, we might think to check the completeness of the data by category. But what if sales data is sourced from three separate locations: our Shopify site (80%), our Amazon Storefront (15%), and phone sales (5%). Unless we explicitly check completeness by channel (a dimension we don’t particularly care about for our analysis), it would be easy to miss if our data source for phone sales has stopped working or loads at a different frequency.\nAnother interesting aspect of multi-source data, is multiple sources can contribute either to different rows/records or different columns/variables. Table-level frequency counts won’t help us in the latter case since other sources might create the right total number of records but result in some specific fields in those records being missing or inaccurate.\n\n\nPartial Loads\nPartial loads really are not data errors at all, but are still important to detect since they can jeopardize an analysis. A common scenario might occur if a job loads new data every 12 hours (say, data from the morning and afternoon of day n-1 loads on day n at 12AM and 12PM, respectively). An analyst retrieving data at 11AM may be concerned to see an approximate ~50% drop in sales in the past day, despite confirming that their data looks to be “complete” since the maximum record date is, in fact, day n-1.8\n\n\nDelayed or Transient Records\nThe interaction between choices made in the data collection and data loading phases can introduce their own sets of problems.\nConsider an orders table for an e-commerce company that analysts may use to track customer orders. It might contain one record per order_id x event (placement, processing, shipment), one record per order placed, one record per order shipping, or one record per order with a status field that changes over time to denote the order’s current stage of life.\n\nAny of these modeling choices seem reasonable and the difference between them might appear immaterial. But consider the collection choice to record and report shipped events. Perhaps this might be operationally easier if shipment come from one source system whereas orders could come from many. However, an interesting thing about shipments is that they are often lagged in a variable way from the order date.\n\nSuppose the e-commerce company in question offers three shipping speeds at checkout. The chart below shows the range of possible shipment dates based on the order dates for the three different speeds (shown in different bars/colors). How might this effect our perceived data quality?\n\nOrder data could appear stale or not timely since orders with a given order_date would only load days later once shipped\nSimilar to missing or multisource data, the data range in the table could lead to deceptive and incomplete data validation because some orders from a later order date might ship (and thus be logged) before all orders from a previous order date\nPut another way, we could have multiple order dates demonstrating partial data loads\nThese features of the data might behave inconsistently across time due to seasonality (e.g. no shipping on weekends or federal holidays), so heuristics developed to clean the data based on a small number of observations could fail\nFrom an analytical perspective, orders with faster shipping would be disproportionately overrepresented in the “tail” (most recent) data. If shipping category correlated with other characteristics like total order spend, this could create an artificial trend in the data\n\nOnce again, understanding that data is collected at point of shipment and reasoning how shipment timing varies and impacts loading is necessary for successful validation."
  },
  {
    "objectID": "post/data-error-gen/index.html#data-transformation",
    "href": "post/data-error-gen/index.html#data-transformation",
    "title": "Understanding the data (error) generating processes for data validation",
    "section": "Data Transformation",
    "text": "Data Transformation\nFinally, once the data is roughly where we want it, it likely undergoes many transformations to translate all of the system-generated fields we discussed in data collection into semantically-relevant dimensions for analytical consumers. Of course, the types of transformations that could be done are innumerable with far more variation than data loading. So, we’ll just look at a few examples of common failure patterns.\n\nPre-Aggregation\nData transformations may include aggregating data up to higher levels of granularity for easier analysis. For example, a transformation might add up item-level purchase data to make it easier for an analyst to look at spend per order of a specific user.\nData transformations not only transform our data, but they also transform how the dimensions of data quality manifest. If data with some of the completeness or uniqueness issues we discussed with data loading is pre-aggregated, these problems can turn into problems of accuracy. For example, the duplicate or partial data loads that we discussed when aggregated could suggest inaccurately high or low quantities respectively.\n\n\n\nField Encoding\nWhen we assess data consistency across tables,\nCategorical fields in a data set might be created in any number of ways including:\n\nDirectly taken from the source\nCoded in a transformation script\nTransformed with logic in a shared user-defined function (UDFs) or macro\nJoined from a shared look-up table\n\nEach approach has different implications on data consistency and usability.\n\nUsing fields from the source simply is what it is – there’s no subjectivity or room for manual human error. If multiple tables come from the same source, it’s likely but not guaranteed that they will be encoded in the same way.\nCoding transformations in the ELT process is easy for data producers. There’s no need to coordinate across multiple processes or use cases, and the transformation can be immediately modified when needed. However, that same lack of coordination can lead to different results for fields that should be the same.\nAlternatively, macros, UDFs, and look-up tables provided centralized ways to map source data inputs to desired analytical data outputs in a systemic and consistent way. Of course, centralization has its own challenges. If something in the source data changes, the process of updating a centralized UDF or look-up table may be slowed down by the need to seek consensus and collaborate. So, data is more consistent but potentially less accurate.\nRegardless, such engineered values require scrutiny – paticularly if they are being used as a key to join multiple tables – and the distinct values in them should be carefully examined.\n\n\nUpdating Transformations\nOf course, data consistency is not only a problem across different data sources but within one data source. Regardless of the method of field encoding used in the previous step, the intersection of data loading and data transformation strategies can introduce data consistency errors over time.\nOften, for computation efficiency, analytical tables are loaded using an incremental loading strategy. This means that only new records (determined by time period, a set of unique keys, or other criteria) from the upstream source are loaded to the downstream table. This is in contrast to a full refresh where the entire downstream table is recreated on each update.\n\nIncremental loads have many advantages. Rebuilding tables in entirety can be very time consuming and computationally expensive. In particular, in non-cloud data warehouses that are not able to scale computing power on demand, this sort of heavy duty processing job can noticeably drain resources from other queries that are trying to run in the database. Additionally, if the upstream staging data is ephemeral, fully rebuilding the table could mean failing to retain history.\nHowever, in the case that our data transformations change, incremental loads may introduce inconsistency in our data overtime as only new records are created and inserted with the new logic.\n\nThis is also a problem more broadly if some short-term error is discovered either with data loading or transformation in historical data. Incremental strategies may not always update to include the corrected version of the data.\n\nRegardless, this underscores the need to validate entire datasets and to re-validate when repulling data."
  },
  {
    "objectID": "post/data-error-gen/index.html#conclusion",
    "href": "post/data-error-gen/index.html#conclusion",
    "title": "Understanding the data (error) generating processes for data validation",
    "section": "Conclusion",
    "text": "Conclusion\nIn statistical modeling, the goal of considering the data generating process is not to understand an encode every single nuance of the complete DGP. After all, if all of that were known, we wouldn’t need a model: we could simulate the universe.\nSimilarly for data validation, data consumers cannot know everything about the data production DGP without taking over the data production process in its entirety. But understanding some of the key failure modes faced by data producers can support data validation by helping consumers develop more realistic theories and expectations for the ways data may ‘break’ and how to refine strategies for detection them."
  },
  {
    "objectID": "post/data-error-gen/index.html#footnotes",
    "href": "post/data-error-gen/index.html#footnotes",
    "title": "Understanding the data (error) generating processes for data validation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMichael Betancourt’s tutorial is a lovely example. Thanks to Joseph Lewis on Twitter for the reference.↩︎\nThe open-source text Modern Statistics for Modern Biology by Susan Holmes and Wolfgang Huber contains more examples.↩︎\nOf course, strategies for collection, moving, transforming, storing, and validating data are innumerable. This is not intended to be a comprehensive guide on any of these topics but simply to illustrate why its important for analysts to keep in mind the interplay between these steps.↩︎\nI don’t mean to imply statisticians do not regularly think about the data collection DGP! The rich literatures on missing data imputation, censored data in survival analysis, and non-response bias is survey data collection are just a few examples of how carefully statisticians think about how data collection impacts analysis. I chose to break it out here to discuss the more technical aspects of collection↩︎\nLike NYC’s infamously messy turnstile data. I don’t claim to know precisely how this dataset is created, but many of the specific challenges it contains are highly relevant.↩︎\nAs Angela Bass so aptly writes: “Data isn’t ground truth. Data are artifacts of systems.”↩︎\nOf course, this isn’t the only potential type of issue in data collection. While instrumentation often leads to these definitional challenges, other types of data collection like sensors can have other types of challenges like systematically failing to capture certain observations. Consider, for example, bus ridership data collected as riders scan their pass upon entering. If students can ride free by showing the driver their student ID, these observations may be systemically not recorded. Again, relying on an operational system could lead analytics uses astray (like failing to account for peak usage times for this demographic.)↩︎\nOf course, this concern could be somewhat easily allayed if they then checked a timestamp field, but such a field might not exists or might not have been used for validation since its harder to anticipate the appropriate maximum timestamp than it is the maximum date.↩︎"
  },
  {
    "objectID": "post/crosstalk/index.html",
    "href": "post/crosstalk/index.html",
    "title": "crosstalk: Dynamic filtering for R Markdown",
    "section": "",
    "text": "This week, I was pleased to become an official RStudio Certified Instructor after completing Greg Wilson’s training program, based on his excellent book Teaching Tech Together. Part of the certification process involved teaching a 10 minute lesson. I chose to create a brief introduction to using the crosstalk package for dynamic interaction and filtering of plots, maps, and tables.\nThe primary intent of this post is simply to point out these materials, which are available on GitHub. The lesson teaches the skills needed to create an three-panel application like this (as shown in the screnshot above) with a scatterplot, map, and table comparing ridership at a sample of Chicago train stations in April of 2019 versus 2020 (not long after COVID shutdowns).\nTo make this more accessible, I converted to previous “lecture” component into a step-by-step tutorial hosted on the repo’s GitHub Pages.1\nNot convinced? Let me tell you just a bit more about crosstalk.\ncrosstalk is, in my opinion, an exciting and underutilized tool for interactive graphics. Shiny is the R world’s go-to tool for creating interactive applications. However, Shiny has a higher learning curve and can be difficult to share with non-R users because it must be deployed on a server. crosstalk, on the other hand, requires minimal additional syntax and enables browser-based interactions which can be shared in a typical HTML R Markdown output.\nThis convenience comes at the cost of the feature set. crosstalk can handle interactive filtering and highlighting across multiple plots, maps, and tables to feature the same data subsets. However, unlike Shiny, t cannot do operations that would require R to recompute the data (for example, aggregation or refitting a model). Arguably, this limitation can also be a strength of crosstalk; since interaction is limited, the onus still lies firmly on the analyst to find a clear story and think carefully about which relationships to highlight.\nLet’s look at an example.\nThe screenshots above demonstrate the types of exploration that can be done with crosstalk’s dynamic filtering. In the overall scatterplot (above), we see a positive correlation between a higher proportion of weekend (versus weekend) trips in 2019 and lower declines in ridership from 2019 to 2020. This makes sense since business areas have lower weekend travel and would also be more affected by “stay at home” orders. We can use crosstalk to confirm this theory. Highlight the stations with the greatest decline in trips (left), Chicagoans could quickly recognize the mapped stations to be in the Chicago Loop, the heart of the business district. Conversely, those with the greatest weekend travel and lowest ride decline tend to be located in more residential areas on the west and south sides. (This is, of course, a rather simplistic outcome. I simply use this example to illustrate how the different pieces come together; not to draw any particular conclusion.) You can recreate these exact views in the demo, as linked above.\nIf this tutorial peaks your curiosity, check our package developer Carson Sievert’s book Interactive web-based data visualization with R, plotly, and shiny from CRC Press for more information and advanced features."
  },
  {
    "objectID": "post/crosstalk/index.html#footnotes",
    "href": "post/crosstalk/index.html#footnotes",
    "title": "crosstalk: Dynamic filtering for R Markdown",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInitially, I was going to republish the post in it’s entirety, but be warned: hugodown, which I was then using to publish my blog has known issues with including output from htmlwidgets R packages (such as leaflet and DT) in the Hugo Academic theme. This is not a problem with Quarto, normal rmarkdown::html_output, or blogdown. ↩︎"
  },
  {
    "objectID": "post/convo-dbt-update/index.html",
    "href": "post/convo-dbt-update/index.html",
    "title": "Update: column-name contracts with dbtplyr",
    "section": "",
    "text": "Diff of original dbt model using dbtplyr’s concise, declarative macros\nEarlier this year, I wrote about embedding column-name contracts in data pipelines with dbt. This, in turn, built off my post regarding the general theory of using controlled vocabularies to define data models.\nThe general idea of the post was:\nMy first post illustrated these concepts using packages from R’s tidyverse. This suite of packages has an expressive, declarative API that inadvertently shaped a lot of my thinking around “naming things”. Clever naming schemes make things just work. The latter, as the name suggests, used SQL and dbt. While this too proved effective, it felt less elegant and satisfying. This inspired me: what if more dplyr syntax existed in dbt?"
  },
  {
    "objectID": "post/convo-dbt-update/index.html#tldr",
    "href": "post/convo-dbt-update/index.html#tldr",
    "title": "Update: column-name contracts with dbtplyr",
    "section": "TLDR",
    "text": "TLDR\nThis inspired the creation of the dbtplyr dbt package (find it on GitHub). This package aims to port the semantic sugar of dplyr’s select-helpers and related functionality to dbt to support both controlled vocabularies and, more broadly, more concise and semantic code.\nA full explanation is provided below. Additionally, for those that have read the previous post, I have also rewritten my COVID data modeling example to use the macros available in dbtplyr. Comparing the diff of the model_monitor model between the two versions illustrates how dbtplyr’s semantic sugar creates more concise and readable templating code."
  },
  {
    "objectID": "post/convo-dbt-update/index.html#introducing-dbtplyr",
    "href": "post/convo-dbt-update/index.html#introducing-dbtplyr",
    "title": "Update: column-name contracts with dbtplyr",
    "section": "Introducing dbtplyr",
    "text": "Introducing dbtplyr\nTo paraphrase the README:\nThis add-on package enhances dbt by providing macros which programmatically select columns based on their column names. It is inspired by the across() function and the select helpers in the R package dplyr.\ndplyr (&gt;= 1.0.0) has helpful semantics for selecting and applying transformations to variables based on their names. For example, if one wishes to take the sum of all variables with name prefixes of N and the mean of all variables with name prefixes of IND in the dataset mydata, they may write:\nsummarize(\n  mydata, \n  across( starts_with('N'), sum),\n  across( starts_with('IND', mean)\n)\nThis package enables us to similarly write dbt data models with commands like:\n{% set cols = dbtplyr.get_column_names( ref('mydata') ) %}\n{% set cols_n = dbtplyr.starts_with('N', cols) %}\n{% set cols_ind = dbtplyr.starts_with('IND', cols) %}\n\nselect\n\n  {{ dbtplyr.across(cols_n, \"sum({{var}}) as {{var}}_tot\") }},\n  {{ dbtplyr.across(cols_ind, \"mean({{var}}) as {{var}}_avg\") }}\n\nfrom {{ ref('mydata') }}\nwhich dbt then compiles to standard SQL.\nAlternatively, to protect against cases where no column names matched the pattern provided (e.g. no variables start with n so cols_n is an empty list), one may instead internalize the final comma so that it is only compiled to SQL when relevant by using the final_comma parameter of across.\n  {{ dbtplyr.across(cols_n, \"sum({{var}}) as {{var}}_tot\", final_comma = true) }}\nNote that, slightly more dplyr-like, you may also write:\nselect\n\n  {{ dbtplyr.across(dbtplyr.starts_with('N', ref('mydata')), \"sum({{var}}) as {{var}}_tot\") }},\n  {{ dbtplyr.across(dbtplyr.starts_with('IND', ref('mydata')), \"mean({{var}}) as {{var}}_avg\") }}\n\nfrom {{ ref('mydata') }}\nBut, as each function call is a bit longer than the equivalent dplyr code, I personally find the first form more readable.\nThe complete list of macros included are:\nFunctions to apply operation across columns\n\nacross(var_list, script_string, final_comma)\nc_across(var_list, script_string)\n\nFunctions to evaluation condition across columns\n\nif_any(var_list, script_string)\nif_all(var_list, script_string)\n\nFunctions to subset columns by naming conventions\n\nstarts_with(string, relation or list)\nends_with(string, relation or list)\ncontains(string, relation or list)\nnot_contains(string, relation or list)\none_of(string_list, relation or list)\nnot_one_of(string_list, relation or list)\nmatches(string, relation)\neverything(relation)\nwhere(fn, relation) where fn is the string name of a Column type-checker (e.g. “is_number”)\n\nNote that all of the select-helper functions that take a relation as an argument can optionally be passed a list of names instead.\nDocumentation for these functions is available on the package website and in the macros/macro.yml file."
  },
  {
    "objectID": "post/column-name-contracts/index.html",
    "href": "post/column-name-contracts/index.html",
    "title": "Column Names as Contracts",
    "section": "",
    "text": "Software products use a range of strategies to make promises or contracts with their users. Mature code packages and APIs document expected inputs and outputs, check adherence with unit tests, and transparently report code coverage. Programs with graphical user interfaces form such contracts by labeling and illustrating interactive components to explain their intent (for example, the “Save” button typically does not bulk-delete files).\nPublished data tables, however, exist in an ambiguous gray area; static enough not to be considered a “service” or “software”, yet too raw to earn the attention to user experience given to interfaces. This ambiguity can create a strange symbiosis between data producers and consumers. Producers may publish whatever data is accessible by the system or seems relevant, and consumers may be quick to assume tables or fields that “sound right” happen to be custom-fit for their top-of-mind question. Producers wonder why consumers aren’t satisfied, and consumers wonder why the data is never “right”.\nMetadata management solutions aim to solve this problem, and there are many promising developments in this space including Lyft’s Amundsen, LinkedIn’s DataHub, and Netflix’s Metacat. However, metadata solutions generally require a great degree of cooperation: producers must vigilantly maintain the documentation and consumers must studiously check it – despite such tools almost always living outside of either party’s core toolkit and workflow.\nUsing controlled vocabularies for column names is a low-tech, low-friction approach to building a shared understanding of how each field in a data set is intended to work. In this post, I’ll introduce the concept with an example and demonstrate how controlled vocabularies can offer lightweight solutions to rote data validation, discoverability, and wrangling. I’ll illustrate these usability benefits with R packages including pointblank, collapsibleTree, and dplyr, but we’ll conclude by demonstrating how the same principles apply to other packages and languages."
  },
  {
    "objectID": "post/column-name-contracts/index.html#controlled-vocabulary",
    "href": "post/column-name-contracts/index.html#controlled-vocabulary",
    "title": "Column Names as Contracts",
    "section": "Controlled Vocabulary",
    "text": "Controlled Vocabulary\nThe basic idea of controlled vocabularies is to define upfront a set of words, phrases, or stubs with well-defined meanings which can be used to index information. When these stubs are defined for different types of information and pieces together in a consistent order, the vocabulary becomes a descriptive grammar that we can use to describe more complex content and behavior.\nIn the context of a data set, this vocabulary can also serve as a latent contract between data producers and data consumers and carry promises regarding different aspects of the data lineage, valid values, and appropriate uses. When used consistently across all of an organization’s tables, it can significantly scale data management and increase usability as knowledge from working with one dataset easily transfers to another.\nFor example, imagine we work at a ride-share company and are building a data table with one record per trip. What might a controlled vocabulary look like?1\nLevel 1: Measure Types\nFor reasons that will be evident in the examples, I like the first level of the hierarchy to generally capture a semi-generic “type” of the variable. This is not quite the same as data types in a programming language (e.g. bool, double, float) although everything with the same prefix should ultimately be cast in the same type. Instead, these data types imply both a type of information and appropriate usage patterns:\n\nID: Unique identified for an entity.\n\nNumeric for more efficient storage and joins unless system of record generates IDs with characters\nLikely a primary key in some other table\n\nIND: Binary 0 or 1 indicator or an event occurrence\n\nBecause always 0 or 1, can be averaged to find proportion occurrence\nCan consider calling IS instead of IND for even less ambiguity which case is labeled 1\n\nN: Count of quantity or event occurrence\n\nAlways a non-negative integer\n\nAMT: Sum-able real number amount. That is, any non-count amount that is “denominator-free”\nVAL: Numeric variables that are not inherently sum-able\n\nFor example, rates and ratios that cannot be combined or numeric values like latitude and longitude for which typical arithmetic operations don’t make sense\n\nDT: Date of some event\n\nAlways cast as a YYYY-MM-DD date\n\nTM: Time stamp of some event\n\nAlways cast as a YYYY-MM-DD HH:MM:SS time stamp\nDistinguishing dates from time stamps will avoid faulty joins of two date fields arranged differently\n\nCAT: Categorical variable as a character string (potentially encoded from an ID field)\n\nWhile these are relatively generic, domain-specific categories can also be used. For example, since location is so important for ride-sharing, it might be worth having ADDR as a level 1 category.\nLevel 2: Measure Subjects\nThe best hierarchy varies widely by industry and the overall contents of a database – not just one table. Here, we expect to be interested in trip-attributed about many different subjects: the rider, driver, trip, etc. so the measure subject might be the logical next tier. We can define:\n\nDRIVER: Information about the driver\nRIDER: Information about the rider, the passenger who called the ride-share\nTRIP: Information about the trip itself\nORIG: Information about the trip start (time and geography)\nDEST: Information about the trip destination (time and geography)\nCOST: Information about components of the total cost (could be a subset of TRIP, but pertains to all parties and has high cardinality at the next tier, so we’ll break it out)\n\nOf course, in a highly normalized database, measures of these different entities would exist in separate tables. However, this discipline in naming them would still be beneficial so quantities are unambiguous when an analyst combines them.\nLevels 3-n: Details\nThe first few tiers of the hierarchy are critical to standardize to make our “performance promises” and to aid in data searchability. Later levels will be measure specific and may not be worth defining upfront. However, for concepts that are going to exist across many tables, it is worthwhile to pre-specify their names and precise formats. For example:\n\nCITY: Should this be in all upper case? How should spaces in city name be treated?\nZIP: Should 6 digit or 10 digit zip codes be used?\nLAT/LON: To how many decimals should latitude and longitude be geocoded? If the company only operate in certain geographic areas (e.g. the continental US), coarse cut-offs for these can be determined\nDIST: Is distance measured in miles? Kilometers?\nTIME: Are durations measured in seconds? Minutes?\nRATING: What are valid ranges for other known quantities like star ratings?\n\nTerminal “adjectives” could also be considered. For example, if the data-generating systems spit out analytically unideal quantities that should be preserved for data lineage purposes, suffixes such as _RAW and _CLEAN might denote version of the same variable in its original and manicured states, respectively.\nPutting it all together\nThis structure now gives us a grammar to compactly name 35 variables in table:\n\nID_{DRIVER | RIDER | TRIP}: Unique identifier for party of the ride\nDT_{ORIG | DEST}: Date at the trip’s start and end, respectively\nTM_{ORIG | DEST}: Timestamp at the trip’s start and end, respectively\nN_TRIP_{PASSENGERS | ORIG | DEST} Count of unique passengers, pick-up points, and drop-off points for the trip\nN_DRIVER_SEATS: Count of passenger seats available in the driver’s car\nAMT_TRIP_{DIST | TIME}: Total trip distance in miles traveled and time taken\nAMT_COST_{TIME | DIST | BASE | FEES | SURGE | TIPS}: Amount of each cost component\nIND_SURGE: Indicator variable if ride caled during surge pricing\nCAT_TRIP_TYPE: Trip type, such as ‘Pool’, ‘Standard’, ‘Elite’\nCAT_RIDER_TYPE: Rider status, such as ‘Basic’, ‘Frequent’, ‘Subscription’\nVAL_{DRIVER | RIDER}_RATING: Average star rating of rider and driver\nADDR_{ORIG | DEST}_{STREET | CITY | STATE | ZIP}: Address components of trip’s start and end\nVAL_{ORIG | DEST}_{LAT | LON}: Latitude and longitude of trip’s start and end\n\nThe fact that we can describe 35 variables in roughly 1/3 the number of rows already speaks to the value of this structure in helping data consumers build a strong mental model to quickly manipulate the data. But now we can demonstrate far greater value.\nTo start, we create a small fake data set using our schema. For simplicity, I simulate 18 of the 35 variables listed above:\n\nhead(data_trips)\n\n  ID_DRIVER ID_RIDER ID_TRIP    DT_ORIG    DT_DEST N_DRIVER_PASSENGERS\n1      1029     1520    7682 2019-08-07 2019-08-07                   2\n2      9294     3461    1490 2019-03-30 2019-03-30                   2\n3      4904     2826    1764 2019-08-27 2019-08-27                   2\n4      6937     8364    1842 2019-05-01 2019-05-01                   2\n5      1377     9223    9227 2019-07-14 2019-07-14                   1\n6      5767     5005    5616 2019-04-14 2019-04-14                   1\n  N_TRIP_ORIG N_TRIP_DEST AMT_TRIP_DIST IND_SURGE VAL_DRIVER_RATING\n1           1           1      32.55653         1          2.385012\n2           1           1      36.49731         0          2.529680\n3           1           1      40.04109         0          4.417074\n4           1           1      31.95087         0          4.312145\n5           1           1      41.82485         1          2.734805\n6           1           1      33.92214         0          4.829972\n  VAL_RIDER_RATING VAL_ORIG_LAT VAL_DEST_LAT VAL_ORIG_LON VAL_DEST_LON\n1         3.123530     41.60704     41.20487     86.72199     98.95445\n2         4.852908     40.55550     40.14035     89.21445    110.76930\n3         1.968365     41.18290     41.76558     92.56702    110.52004\n4         4.074426     40.64971     40.45868     75.06746     72.53508\n5         3.886951     40.10458     40.15908     86.91200     92.70651\n6         3.349862     40.53041     40.81622    118.19481    107.63962\n  CAT_TRIP_TYPE CAT_RIDER_TYPE\n1      Standard          Basic\n2      Standard       Frequent\n3         Elite          Basic\n4          Pool          Basic\n5      Standard   Subscription\n6         Elite          Basic"
  },
  {
    "objectID": "post/column-name-contracts/index.html#data-validation",
    "href": "post/column-name-contracts/index.html#data-validation",
    "title": "Column Names as Contracts",
    "section": "Data Validation",
    "text": "Data Validation\nThe “promises” in variable names aren’t just there for decoration. They can actually help producers publish higher quality data my helping to automate data validation checks. Data quality is context-specific and requires effort on the consumer side, but setting up safeguards in any data pipeline can help detect and eliminate commonplace errors like duplicated or corrupt data.\nOf course, setting up data validation pipelines isn’t the most exciting part of any data engineer’s job. But that’s where R’s pointblank package comes to the rescue with an excellent domain-specific language for common assertive data checks. Combining this syntax with dplyr’s “select helpers” (such as starts_with()), the same validation pipeline could ensure many of the data’s promises are kept with no additional overhead. For example, N_ columns should be strictly non-negative and IND_ columns should always be either 0 or 1.\nThe following example demonstrate the R syntax to write such a pipeline in pointblank, but the package also allows rules to be specified in a standalone YAML file which could further increase portability between projects.\nlibrary(pointblank)\n\nagent &lt;-\n  data_trips %&gt;%\n  create_agent(actions = action_levels(stop_at = 0.001)) %&gt;%\n  col_vals_gte(starts_with(\"N\"), 0) %&gt;%\n  col_vals_gte(starts_with(\"N\"), 0) %&gt;%\n  col_vals_not_null(starts_with(\"IND\")) %&gt;%\n  col_vals_in_set(starts_with(\"IND\"), c(0,1)) %&gt;%\n  col_is_date(starts_with(\"DT\")) %&gt;%\n  col_vals_between(matches(\"_LAT(_|$)\"), 19, 65) %&gt;%\n  col_vals_between(matches(\"_LON(_|$)\"), -162, -68) %&gt;%\n  interrogate()\nagent\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Pointblank Validation\n    \n    \n      [2023-10-12|20:31:46]\ndata frameWARN\n—\nSTOP\n0.00\nNOTIFY\n—\n\n    \n  \n  \n    \n      \n      \n      STEP\n      COLUMNS\n      VALUES\n      TBL\n      EVAL\n      UNITS\n      PASS\n      FAIL\n      W\n      S\n      N\n      EXT\n    \n  \n  \n    \n1\n\n                                                                                                              \n   col_vals_gte()\n\n\n\n  \n    ▮N_DRIVER_PASSENGERS\n  \n\n\n0\n\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n2\n\n                                                                                                              \n   col_vals_gte()\n\n\n\n  \n    ▮N_TRIP_ORIG\n  \n\n\n0\n\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n3\n\n                                                                                                              \n   col_vals_gte()\n\n\n\n  \n    ▮N_TRIP_DEST\n  \n\n\n0\n\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n4\n\n                                                                                                              \n   col_vals_gte()\n\n\n\n  \n    ▮N_DRIVER_PASSENGERS\n  \n\n\n0\n\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n5\n\n                                                                                                              \n   col_vals_gte()\n\n\n\n  \n    ▮N_TRIP_ORIG\n  \n\n\n0\n\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n6\n\n                                                                                                              \n   col_vals_gte()\n\n\n\n  \n    ▮N_TRIP_DEST\n  \n\n\n0\n\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n7\n\n                                                                                                                                      \n   col_vals_not_null()\n\n\n\n  \n    ▮IND_SURGE\n  \n\n\n—\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n8\n\n                                                                                                              \n   col_vals_in_set()\n\n\n\n  \n    ▮IND_SURGE\n  \n\n\n0, 1\n\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n9\n\n                                                                                                                              D                        \n   col_is_date()\n\n\n\n  \n    ▮DT_ORIG\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n10\n\n                                                                                                                              D                        \n   col_is_date()\n\n\n\n  \n    ▮DT_DEST\n  \n\n\n—\n                                                            \n\n✓\n\n1\n11.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n11\n\n                                                                                                              \n   col_vals_between()\n\n\n\n  \n    ▮VAL_ORIG_LAT\n  \n\n\n[19, 65]\n\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n12\n\n                                                                                                              \n   col_vals_between()\n\n\n\n  \n    ▮VAL_DEST_LAT\n  \n\n\n[19, 65]\n\n                                                            \n\n✓\n\n100\n1001.00\n00.00\n—\n\n○\n\n—\n\n—\n\n    \n13\n\n                                                                                                              \n   col_vals_between()\n\n\n\n  \n    ▮VAL_ORIG_LON\n  \n\n\n[−162, −68]\n\n                                                            \n\n✓\n\n100\n00.00\n1001.00\n—\n\n●\n\n—\n\n\n  CSV\n\n\n    \n14\n\n                                                                                                              \n   col_vals_between()\n\n\n\n  \n    ▮VAL_DEST_LON\n  \n\n\n[−162, −68]\n\n                                                            \n\n✓\n\n100\n00.00\n1001.00\n—\n\n●\n\n—\n\n\n  CSV\n\n\n  \n  \n    \n      2023-10-12 20:31:46 CDT\n1.9 s\n2023-10-12 20:31:48 CDT\n    \n  \n  \n\n\n\nIn the example above, just 7 lines of portable table-agnostic code end up creating 14 data validation checks. The results catch two errors. Upon investigation, we might find that our geocoder is incorrectly flipping the sign on longitude!\nOne could also imagine writing a linter or validator of the variables names themselves to check for typos, outliers that don’t follow common stubs, etc."
  },
  {
    "objectID": "post/column-name-contracts/index.html#data-discoverability",
    "href": "post/column-name-contracts/index.html#data-discoverability",
    "title": "Column Names as Contracts",
    "section": "Data Discoverability",
    "text": "Data Discoverability\nOn the user side, a controlled vocabulary makes new data easier to explore. Although is is not and should not be a replacement for a true data dictionary, imagine how relatively easy it is to understand the following variables’ intent and navigate either a searchable tab of visualization of the output.\nTo make some accessible outputs, we can first wrangle the column names into a table of their own.\ncols_trips &lt;- names(data_trips)\ncols_trips_split &lt;- strsplit(cols_trips, split = \"_\")\ncols_components &lt;- data.frame(\n  variable = cols_trips,\n  level1 = vapply(cols_trips_split, FUN = function(x) x[1], FUN.VALUE = character(1)),\n  level2 = vapply(cols_trips_split, FUN = function(x) x[2], FUN.VALUE = character(1)),\n  level3 = vapply(cols_trips_split, FUN = function(x) x[3], FUN.VALUE = character(1))\n)\nhead(cols_components)\n\n             variable level1 level2     level3\n1           ID_DRIVER     ID DRIVER       &lt;NA&gt;\n2            ID_RIDER     ID  RIDER       &lt;NA&gt;\n3             ID_TRIP     ID   TRIP       &lt;NA&gt;\n4             DT_ORIG     DT   ORIG       &lt;NA&gt;\n5             DT_DEST     DT   DEST       &lt;NA&gt;\n6 N_DRIVER_PASSENGERS      N DRIVER PASSENGERS\n\nPart of the metadata, then, could make it particularly easy to search by various stubs – whether that be the measure type (e.g. N or AMT) or the measure subject (e.g. RIDER or DRIVER).\nlibrary(DT)\ndatatable(cols_components,  filter = list(position = 'top', clear = FALSE))\n\n\n\n\nSimilarly, we can use visualization to both validate and explore the available fields. Below, data fields are illustrated in a tree.\nlibrary(collapsibleTree)\ncollapsibleTree(cols_components, \n                hierarchy = paste0(\"level\", 1:3),\n                nodeSize = \"leafCount\"\n                )\n\n\n\n\nDepending on the type of exploraion being done, it might be more convenient to drilldown first by measure subject. collapsibleTree flexibly lets us control this by specifying the hierarchy.\ncollapsibleTree(cols_components, \n                hierarchy = paste0(\"level\", c(2,1,3)),\n                nodeSize = \"leafCount\"\n                )\n\n\n\n\nThese naming conventions are particularly friendly to a “passive search” via an IDE with autocomplete functionality. Simply typing “N_” and pausing or hitting tab might elicit a list of potential options of count variables in the data set.\nMore broadly, driving this standardization opens up interesting possibility for variable-first documentation. As our grammar for describing fields becomes richer and less ambiguous, it’s increasingly possible for users to explore a variable-first web of quantities and work their way back to the appropriate tables which contain them."
  },
  {
    "objectID": "post/column-name-contracts/index.html#data-wrangling",
    "href": "post/column-name-contracts/index.html#data-wrangling",
    "title": "Column Names as Contracts",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nControlled, hierarchical vocabularies also make basic data wrangling pipelines a breeze. By programming on the column names, we can appropriately summarize multiple pieces of data in the most relevant way.\nFor example, the following code uses dplyr’s “select helpers” to sum up count variables where we might reasonably be interested in the total and find the arithmetic average of indicator variables to help us calculate the proportion of occurrences of an event (here, the incidence of surge pricing).\nNote what our controlled vocabulary and the implied “contracts” have given us. We aren’t summing up fields like latitude and longitude which would have no inherent meaning. Conversely, we can confidently calculate proportions which we couldn’t do if there was a chance our indicator variable contained nulls or occasionally used other numbers (e.g. 2) to denote something like the surge severity instead of pure incidence.\nlibrary(dplyr)\n\ndata_trips %&gt;%\n  group_by(CAT_RIDER_TYPE) %&gt;%\n  summarize(\n    across(starts_with(\"N_\"), sum),\n    across(starts_with(\"IND_\"), mean)\n  )\n\n# A tibble: 3 x 5\n  CAT_RIDER_TYPE N_DRIVER_PASSENGERS N_TRIP_ORIG N_TRIP_DEST IND_SURGE\n  &lt;chr&gt;                        &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 Basic                           57          38          38     0.395\n2 Frequent                        42          31          31     0.548\n3 Subscription                    45          31          31     0.645"
  },
  {
    "objectID": "post/column-name-contracts/index.html#addendum-on-other-languages",
    "href": "post/column-name-contracts/index.html#addendum-on-other-languages",
    "title": "Column Names as Contracts",
    "section": "Addendum on Other Languages",
    "text": "Addendum on Other Languages\nThe above examples use a few specific R packages with helpers that specifically operate on column names. However, the value of this approach is language agnostic since most popular languages for data manipulation support character pattern matching and wrangling operations specified by lists of variable names. We will conclude with a few examples.\n\nGenerating SQL\nAlthough SQL is a hard language to “program on”, many programming-friendly tools offer SQL generators. For example, using dbplyr, we can use R to generate SQL code that sums up all of our count variables by rider type without having to type them out manually.\nlibrary(dbplyr)\n\ndf_mem &lt;- memdb_frame(data_trips, .name = \"example_table\")\n\ndf_mem %&gt;%\n  group_by(CAT_RIDER_TYPE) %&gt;%\n  summarize_at(vars(starts_with(\"N_\")), sum, na.rm = TRUE) %&gt;%\n  show_query()\n\n&lt;SQL&gt;\nSELECT `CAT_RIDER_TYPE`, SUM(`N_DRIVER_PASSENGERS`) AS `N_DRIVER_PASSENGERS`, SUM(`N_TRIP_ORIG`) AS `N_TRIP_ORIG`, SUM(`N_TRIP_DEST`) AS `N_TRIP_DEST`\nFROM `example_table`\nGROUP BY `CAT_RIDER_TYPE`\n\n\n\nR - base & data.table\nHowever, we aren’t of course limited just to tidyverse style coding. Similarly concise workflows exists in both base and data.table syntaxes. Suppose we wanted to summarize all numeric variables. First, we can use base::grep to find all column names that begin with N_.\ncols_n &lt;- grep(\"^N_\", names(data_trips), value = TRUE)\nprint(cols_n)\n\n[1] \"N_DRIVER_PASSENGERS\" \"N_TRIP_ORIG\"         \"N_TRIP_DEST\"        \n\nWe can define the variables we want to group by in another vector.\ncols_grp &lt;- c(\"CAT_RIDER_TYPE\")\nThese vectors can be used in aggregation operations such as stats::aggregate:\naggregate(data_trips[cols_n], by = data_trips[cols_grp], FUN = sum)\n\n  CAT_RIDER_TYPE N_DRIVER_PASSENGERS N_TRIP_ORIG N_TRIP_DEST\n1          Basic                  57          38          38\n2       Frequent                  42          31          31\n3   Subscription                  45          31          31\n\nOr with data.table syntax:\nlibrary(data.table)\ndt &lt;- as.data.table(data_trips)\ndt[, lapply(.SD, sum), by = cols_grp, .SDcols = cols_n]\n\n   CAT_RIDER_TYPE N_DRIVER_PASSENGERS N_TRIP_ORIG N_TRIP_DEST\n1:          Basic                  57          38          38\n2:       Frequent                  42          31          31\n3:   Subscription                  45          31          31\n\n\n\npython pandas\nSimilarly, we can use list comprehensions in python to create a list of columns names matching a specific pattern (cols_n). This list and a list to define grouping variables can be passed to pandas’s data manipulation methods.\nimport pandas as pd\ncols_n   = [vbl for vbl in data_trips.columns if vbl[0:2] == 'N_']\ncols_grp = [\"CAT_RIDER_TYPE\"]\ndata_trips.groupby(cols_grp)[cols_n].sum()\n\n                N_DRIVER_PASSENGERS  N_TRIP_ORIG  N_TRIP_DEST\nCAT_RIDER_TYPE                                               \nBasic                            57         38.0         38.0\nFrequent                         42         31.0         31.0\nSubscription                     45         31.0         31.0"
  },
  {
    "objectID": "post/column-name-contracts/index.html#updates",
    "href": "post/column-name-contracts/index.html#updates",
    "title": "Column Names as Contracts",
    "section": "Updates",
    "text": "Updates\n\nConcept Map\nThanks to Greg Wilson for helping illustrate the concepts from this post in a concept map:\n\n\n\nNew Package (Dec 2020)\nSince writing this post, I have released the convo R package to facilitate maintenance and application of controlled vocabularies. Please check it out and let me know what you think!\n\n\nNew Package (April 2021)\nI also released a dbt package called dbtplyr to port dplyr’s useful select-helper semantics to SQL via dbt."
  },
  {
    "objectID": "post/column-name-contracts/index.html#footnotes",
    "href": "post/column-name-contracts/index.html#footnotes",
    "title": "Column Names as Contracts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAgain, vocabularies should span a database – not just an individual dataset, but for simplicity we just talk through a smaller example.↩︎"
  },
  {
    "objectID": "post/causal-data/index.html",
    "href": "post/causal-data/index.html",
    "title": "Industry information management for causal inference",
    "section": "",
    "text": "Data strategy motivated by causal methods\nThis post summarizes the final third of my talk at Data Science Salon NYC in June 2023. Please see the talk details for more content.\nTechniques of observational causal inference are becoming increasingly popular in industry as a complement to experimentation. Causal methods offer the promise of accelerating measurement agendas and facilitating the estimation of previously un-measurable targets by allowing analysts to extract causal insights from “found” data (e.g. observational data collected without specific intent). However, if executed without careful attention to their assumptions and limitations, they can lead to spurious conclusions.\nBoth experimental and observational methods attempt to address the fundamental problem of causal inference: that is, the fact that for a given treatment of interest, we can never “see” the individual-level outcome both for the case when an individual received a treatment and a counterfactual scenario in which for the same individual in the exact same context that treatment was withheld. Some literature casts this as a “missing data” problem.1 Counterfactual data is uncollectable; however, this fundamental missingness can be partially mitigated by diligent collection of other types of quantitative and qualitative information to control for confounding2 and interrogate assumptions.\nIn this post, I argue that industry has unique advantages when using causal techniques over the social science disciplines that originated many foundational methods due to industry’s (theoretically) superior ability to observe and capture relevant supplemental data and context. Examining the implicit assumptions in common causal design patterns motivates the types of proactive enterprise information management – including data, metadata, and knowledge management – that will help preserve the raw inputs that future data scientists will need to effectively deploy causal techniques on historical data and answer questions that our organizations cannot even anticipate today. By casting an intentionally wide net on what information we observationally collect, we increase the likelihood that the future “found” data will have what those analysts need to succeed."
  },
  {
    "objectID": "post/causal-data/index.html#why-industry-needs-causal-inference",
    "href": "post/causal-data/index.html#why-industry-needs-causal-inference",
    "title": "Industry information management for causal inference",
    "section": "Why industry needs causal inference",
    "text": "Why industry needs causal inference\n\nIndustry data science tends to highly value the role of A/B testing and experimentation. However, there are many situations where experimentation is not an optimal approach to learning. Experiments can be infeasible if we worry about the ethics or reputational risk of offering disparate customer treatments; they may be impractical in situations that are hard to randomize or avoid spillover effects; they can be costly to run and configure either in direct or opportunity costs; and, finally, they can just be slow if we wish to measure complex and long-term impacts on customer behaviors (e.g. retention, lifetime value)."
  },
  {
    "objectID": "post/causal-data/index.html#what-causal-methods-require",
    "href": "post/causal-data/index.html#what-causal-methods-require",
    "title": "Industry information management for causal inference",
    "section": "What causal methods require",
    "text": "What causal methods require\n\nThese limitations are one of the reasons why observational causal inference is gaining increasing popularity in industry. Methods of observational causal inference allows us to estimate treatment effects without randomized controlled experimentation by using existing historical data. At the highest level, these methods work by replacing randomization with strategies to exploit other forms of semi-random variation in historical exposures of a population to a treatment. Since this semi-random variation could be susceptible to confounding, observational methods supplement variation with additional data to control for other observable sources of bias in our estimates and contextual assumptions about the data generating process.\nMy previous post on causal design patterns outlines a number of foundational causal methods, but I’ll briefly recap to emphasize the different ways that sources of variation, data, and context are used:\n\nStratification and Inverse Propensity Score Weighting:\n\nExploits “similar” populations of treated and untreated individuals\nAssumes we can observe and control for common causes of the treatment and the outcome\n\nRegression Discontinuity:\n\nExploits a sharp, semi-arbitrary cut-off between treated and untreated individuals\nAssumes that the outcome is continuous with respect to the assigment variable and the assignment mechanism is unknown to individuals (to avoid self-selection)\n\nDifference in Differences:\n\nExploits variation between behavior over time of treated and untreated groups\nAssumes that the treatment assignment is unrelated to expected future outcomes and that the treatment is well-isolated to the treatment group\n\n\nNotably, the assumptions mentioned above are largely untestable statistically (e.g. not like testing for normality or multicolinearity) but rely on knowledge of past strategies and policies that guided differential treatment in historical data.3"
  },
  {
    "objectID": "post/causal-data/index.html#industrys-unique-advantages-deploying-causal-inference",
    "href": "post/causal-data/index.html#industrys-unique-advantages-deploying-causal-inference",
    "title": "Industry information management for causal inference",
    "section": "Industry’s unique advantages deploying causal inference",
    "text": "Industry’s unique advantages deploying causal inference\n\nMany causal methods originated in fields like epidemiology, economics, political science, and other social sciences. In such fields, direct experimentation is often impossible and even first-hand data collection is less common. Often, researchers may have to rely on pre-existing data sources like censuses, surveys, and administrative data (e.g. electronic health records).\nDespite the lineage of these methods, industry has many advantages over traditional research fields in using them because each company controls the entire “universe” in which its customers exist. This should in theory provide a distinct advantage when collecting each of the three “ingredients” that causal methods use to replace randomization:\n\nVariation: We control customer engagement strategies through methods like customer segmentation or models. Subsequent customer treatments are completely known to us but inherently have some arbitrary, judgmental component to exploit\nData: We tend to be able to collect more measurements of our customers both as a snapshot (more variety in fields) and longitudinally (more observations over time) that can be brought into our analyses to control for confounders4, reduce other sources of variation in our estimate, and have additional ‘out of time’ data left over to conduct forms of validation like placebo tests\nContext: We tend to know how past strategies were set-up, how they looked to individuals involved, and why those decisions were made. This can be critical in reasoning whether our assumptions hold\n\nHowever, to convert this theoretical benefit to a practical one requires information management."
  },
  {
    "objectID": "post/causal-data/index.html#data-management-for-causal-inference",
    "href": "post/causal-data/index.html#data-management-for-causal-inference",
    "title": "Industry information management for causal inference",
    "section": "Data management for causal inference",
    "text": "Data management for causal inference\n\nWhile all causal methods will be enhanced with better enterprise information management, it’s easiest to see the motivation by thinking back to specific examples. Causal inference can benefit from better data, metadata, and knowledge management. These are illustrated by propensity score weighting, regression discontinuity, and diff-in-diff respectively.\nIntegrated Data Management\nEarlier, we posited that one advantage that industry has over academia for causal inference is access to richer historical data sources as a higher level of resolution (more measures per individual at more time points). A rich set of customer measures is critical for stratification and propensity score weighting where we attempt to control for selection on observables by balancing populations along dimensions that might be common causes of treatment assignment and outcome. (And, we may also wish to control for other unrelated sources of variation that effect only the outcome to develop more precise estimates.)\nHowever, this is only true if customer data is proactively collected, cleaned, and harmonized across sources in the true spirit of a customer 360 view. Enterprises may collect data about customers from many different operational systems – for example, demographic information provided at registration, digital data on their logins and web activity, campaign data on attempted customer touchpoints and engagement, behavioral or fulfillment data on purchases / subscription renewals / etc. Any of these sources could be useful “observables” that help close confounding pathways in our analyses.\nTo make this data useful and accessible for analysis, it must be proactively integrated into a common source like a data warehouse, well-documented to help future users understand the nuances of each system, harmonized so fields have standard definitions (e.g. common definitions of an “account” and a “customer”), and unified by using techniques like entity resolution to ensure all sources share common identifiers so that they can be merged for analysis.\nMetadata Management\nBeyond those “typical” sources of customer data, our past customer strategies create data beyond the data directly generated by our customers. Metadata about past campaigns such as precise business logic on the different treatments offered (e.g. if sending customers a discount, what algorithmically determined the amount?), the campaign targeting and segmentation (e.g. What historical behaviors were used to segments customers? Was treatment determined by a predictive model?), and launch timing can all be critical to clearly identifying those sources of variation that we wish to exploit. For example, we might know that we once ran an re-engagement campaign to attempt the nudge interaction from customers who didn’t log-in to a website for some amount of time, but knowing whether that campaign was targeting customers &gt;30 days inactive or &gt;45 days inactive impacts our ability to analyze it with a regression discontinuity.\nThis means that we need to treat metadata as first-class data and ensure that it is extracted from operational source systems (or intent docs, config files, etc.), structured in a machine-readable format, and preserved in analytical data stores along with our customer data.\nThe importance of “metadata as data” extends beyond business-as-usual organization strategies. We can also fuel future causal inference with better metadata management of past formal experiments and execution errors.\nAs discussed above, formal experiments may represent a substantial investment in company resources so the data collected from them should be regarded as an asset. Beyond their utility for one-time reads and decisions, experiment designs and results should be carefully catalogued along with the assigned treatment group and the randomization criteria (such as fields characterizing sampling weights as provided in US Census data). This can support future observational analysis of past experiments, including generalizing and transporting results to different populations.\nFurthermore, even mistakes in executing past strategies may become “natural experiments” to help businesses understand scenarios that they might never have prioritized for testing. So, machine-readable incident logs and impacted populations can be useful as well.\nKnowledge Management\nOf course, not all information can be condensed into a nice, machine-readable spreadsheet. Methods like difference-in-differences illustrate how conceptual context can also help us battle-test assumptions like whether the decision-to-treat could have spilled over into the control population or been influenced by an anticipated change in the future outcome. This is the one area where industry may sometimes lag social sciences in information since some population-level treatments like a state law or local ordinance often have documented histories through the legislative process, news coverage, and historical knowledge about their implementation.\nIndustry can catch up on knowledge management by documenting and preserving in a centralized knowledge repository key information about strategic decisions undertaken, the motivating factors, and the anticipated customer experience. Such documents are inevitably created when working on new projects through memos ad decks intended to communicate the business case, intent, and expected customer experience. However, proactively figuring out how to organize and index this information through a classification system and democratize access through centralized knowledge repositories is critical to giving future users entree to this tribal knowledge. Projects like Airbnb’s Knowledge Repository suggest what such a system might look like in practice."
  },
  {
    "objectID": "post/causal-data/index.html#footnotes",
    "href": "post/causal-data/index.html#footnotes",
    "title": "Industry information management for causal inference",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, see https://arxiv.org/abs/1710.10251↩︎\nIf you’ve heard of ‘selection on observables’ in causal literature, richer data means observables!↩︎\nThere are some exceptions to this like placebo tests, bunching checks, etc.↩︎\nNotable, the availability of more data absolutely does not mean that we should simply “dump in” all the data we have. Controlling for certain variables like colliders is counterproductive.↩︎"
  },
  {
    "objectID": "post/abstraction-airbyte/index.html",
    "href": "post/abstraction-airbyte/index.html",
    "title": "Crosspost: The Art of Abstraction in ETL",
    "section": "",
    "text": "I previously shared the first in my three-part series of guest posts on Airbyte’s developer blog about ETL. The first focused on errors in data extraction. The next two focused on the countless, small decisions one makes when loading data, and finally the DataOps burden to keep things up-and-running.\nThis post serves only to serve as a quick reference to those posts:\n\nDodging extraction errors\nMaking sound loading decisions\nKeeping the good things going"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m passionate about working on problems throughout the data space – from strategy and analytics, modeling and data science, and upstream data infrastructure development. I currently work at Capital One where I’ve led teams across those domains, including initiatives to develop better fit-for-purpose models and to reimagine our analytical infrastructure with innersource tools.\nCurrently, I’m most energized by problems related to reproducible research and analytical tooling, data management and metric/feature innovation, and causal inference. In my free time, I enjoy working on open source and pro-bono projects, reviewing (50+!) book proposals for CRC Press, and serving on the editorial board of rOpenSci.\nPreviously, I earned degrees in Mathematics and Mathematical Decision Sciences (Stats / OR) at UNC Chapel Hill. At Carolina, I was a research assistant in the Department of Statistics & Operations Research and focused on testing the impact of Emergency Department patient flow strategies using discrete event simulation.\nWhen I’m not crunching numbers, I enjoy reading, running and weightlifting, and watching college basketball (Go Tar Heels!)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "RecentFavorites\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\ndbt\n\n\nsql\n\n\ndata\n\n\nml\n\n\n\nPlaying with the potential, perils, and design principles of deploying ML models into the analytical database using orbital’s sklearn-to-sql translation, sqlglot, and dbt\n\n\n\n\n\n2025-08-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nworkflow\n\n\nrstats\n\n\npython\n\n\nquarto\n\n\nrmarkdown\n\n\n\nLiterate programming excels at capturing our stream of conscience. Our stream of conscience does not excel at explaining the impact of our work. Notebooks enable some of data scientists’ worst tendencies in technical communication, but Quarto’s embed feature bridges the gap between reproducible research and resonate story-telling.\n\n\n\n\n\n2025-07-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nml\n\n\n\nThe orbital package offers an interface for translating a fitted SciKitLearn pipeline to pure SQL for scaling predictions. In this tech note, I explore how this framework can (mostly) be used for xgboost models, as well, with a bit of wrangling (and a few limitations).\n\n\n\n\n\n2025-07-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\n\nSwitching languages is about switching mindsets - not just syntax. New developments in python data science toolings, like polars and seaborn’s object interface, can capture the ‘feel’ that converts from R/tidyverse love while opening the door to truly pythonic workflows. (Updated from 2025 for new tools).\n\n\n\n\n\n2025-01-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nquarto\n\n\nrmarkdown\n\n\nworkflow\n\n\n\nA quick tech note on Netlify’s managed authentication solution\n\n\n\n\n\n2024-11-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nData teams may struggle to quantify the benefits of good data documentation. But running countless ad hoc validation queries can incur both computational and cognitive cost.\n\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\ntutorial\n\n\n\nGetting comfortable in a new language is more than the packages you use. Syntactic sugar in base python increases the efficiency, and aesthetics of python code in ways that R users may enjoy in packages like glue and purrr. This post collects a miscellaneous grab bag of tools for wrangling, formatting (f-strings), repeating (list comprehensions), faking data, and saving objects (pickle)\n\n\n\n\n\n2024-01-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nData documentation isn’t a box to check; it’s an active member of your team with many jobs-to-be-done. In this cross-post with Select Star, I write about how effective documentation can be your data products’ developer advocate for users, project manager for developers, and chief of staff for data leaders\n\n\n\n\n\n2024-01-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\ntutorial\n\n\n\nIn this follow-up post to Python Rgonomics, we deep dive into some of the advanced data wrangling functionality in python’s polars package to see how it’s powertools like column selectors and nested data structures mirror the best of dplyr and tidyr’s expressive and concise syntax\n\n\n\n\n\n2024-01-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nWriting is thinking; documenting is planning and executing. In this cross-post with Select Star, I write about how teams can produce high-quality and maintainble documentation by smartly structuring planning and development documentation and effeciently recycling them into long-term, user-friendly docs\n\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\n\nSwitching languages is about switching mindsets - not just syntax. New developments in python data science toolings, like polars and seaborn’s object interface, can capture the ‘feel’ that converts from R/tidyverse love while opening the door to truly pythonic workflows\n\n\n\n\n\n2023-12-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\n\nFive highlights and links to select talks\n\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\n\nProactive collection of data to comply or confront assumptions\n\n\n\n\n\n2023-05-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nRounding out my three-part ETL series form Airbyte’s developer blog\n\n\n\n\n\n2023-05-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nCross-post from guest post on Airbyte’s developer blog\n\n\n\n\n\n2023-03-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\ndbt\n\n\ncrosspost\n\n\n\nAfter a prior post on the merits of grouped data quality checks, I demo my newly merged implementation for dbt\n\n\n\n\n\n2023-01-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nsql\n\n\n\nOut-of-memory processing of North Carolina’s voter file with DuckDB and Apache Arrow\n\n\n\n\n\n2022-09-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\nsql\n\n\ndata\n\n\ndata-disasters\n\n\n\nHow we do (or don’t) think about null values and why the polyglot push makes it all the more important\n\n\n\n\n\n2022-09-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\ndbt\n\n\n\nAfter a prior post on the merits of grouped data quality checks, I demo my newly merged implementation for dbt\n\n\n\n\n\n2022-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nshiny\n\n\ndata\n\n\n\nKey issues when adding persistent storage to a Shiny application, featuring {golem} app development and Digital Ocean serving\n\n\n\n\n\n2022-01-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nrmarkdown\n\n\n\nMuch like ice sculpting, applying powertools to absolutely frivolous pursuits\n\n\n\n\n\n2021-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\nWhich of these numbers doesn’t belong? -1, 0, 1, NA. You can’t judge data quality without data context, so our tools should enable as much context as possible.\n\n\n\n\n\n2021-11-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata-disasters\n\n\n\nA personal encounter with ‘intelligent’ data products gone wrong\n\n\n\n\n\n2021-11-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\ndbt\n\n\n\nFollowing up on ‘Embedding Column-Name Contracts… with dbt’ to demo my new dbtplyr package to further streamline the process\n\n\n\n\n\n2021-09-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\ndata\n\n\nelt\n\n\n\nA right-sized solution to automated data monitoring, alerting, and reporting using R (pointblank, projmgr), GitHub (Actions, Pages, issues), and Slack\n\n\n\n\n\n2021-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\nsql\n\n\n\nTricks for modularizing and refactoring your projects SQL/R interface. (Image source techdaily.ca)\n\n\n\n\n\n2021-07-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nelt\n\n\n\nA data consumer’s guide to validating data based on the failure modes data producer’s try to avoid\n\n\n\n\n\n2021-05-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nelt\n\n\npython\n\n\n\nExploring how Playwright‘s headless browser automation (and its friends) can help unite the states’ data\n\n\n\n\n\n2021-05-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nsql\n\n\ndbt\n\n\n\ndbt supercharges SQL with Jinja templating, macros, and testing – all of which can be customized to enforce controlled vocabularies and their implied contracts on a data model\n\n\n\n\n\n2021-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\ntutorial\n\n\n\nAn informal primer to causal analysis designs and data structures\n\n\n\n\n\n2021-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\nresources\n\n\n\nFree books, lectures, blogs, papers, and more for a causal inference crash course\n\n\n\n\n\n2021-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\njtbd\n\n\n\nOn the jobs-to-be-done and design principles for internal tools\n\n\n\n\n\n2021-01-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\ndata\n\n\nsql\n\n\ntutorial\n\n\n\nUsing the tidyverse’s expressive data wrangling vocabulary as a preprocessor for elegant SQL scripts. (Image source techdaily.ca)\n\n\n\n\n\n2021-01-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\ndata\n\n\n\nAn R package for maintaining controlled vocabularies to encode contracts between data producers and consumers\n\n\n\n\n\n2020-12-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nworkflow\n\n\n\nMarketing maintenance work with irrational exuberance\n\n\n\n\n\n2020-09-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\ntutorial\n\n\n\nAn introduction to browser-based interactivity of htmlwidgets – no Shiny server required!\n\n\n\n\n\n2020-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nrstats\n\n\n\nUsing controlled dictionaries for low-touch documentation, validation, and usability of tabular data\n\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshiny\n\n\nrstats\n\n\nworkflow\n\n\ntutorial\n\n\n\nDon’t believe the documentation! Shiny modules aren’t just for advanced users; they might just be a great entry point for development\n\n\n\n\n\n2020-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresources\n\n\nrstats\n\n\n\nReadings and assorted ideas about creating and maintaining low-overhead documentation\n\n\n\n\n\n2020-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrmarkdown\n\n\nrstats\n\n\n\nA few tips and tools for finding the right selectors to style in RMarkdown\n\n\n\n\n\n2020-06-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\n\nA walkthrough of using the projmgr package for GitHub-based project management via R\n\n\n\n\n\n2020-05-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npkgdev\n\n\nworkflow\n\n\nrstats\n\n\nrmarkdown\n\n\n\nA recommended tech stack for implementing RMarkdown Driven Development\n\n\n\n\n\n2020-02-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresources\n\n\nrstats\n\n\n\nCase studies of the impact of R use on organizational culture and collaboration\n\n\n\n\n\n2019-08-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresources\n\n\nworkflow\n\n\n\nAn annotated bibliography of advice for getting started with reproducible research\n\n\n\n\n\n2019-08-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\n\nA walkthrough of a GitHub template for making your own RMarkdown and ggplot2 theme package\n\n\n\n\n\n2019-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspeaking\n\n\nnotes\n\n\n\nConference planning tips to design a good speakers experience\n\n\n\n\n\n2019-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npkgdev\n\n\nrmarkdown\n\n\nrstats\n\n\nworkflow\n\n\n\nA workflow for refactoring one-time analyses to sustainable data products\n\n\n\n\n\n2019-05-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspeaking\n\n\nnotes\n\n\n\nA proposed workflow for methodically developing a good presentations\n\n\n\n\n\n2019-04-20\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\njtbd\n\n\n\nOn the jobs-to-be-done and design principles for internal tools\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\ntutorial\n\n\n\nAn informal primer to causal analysis designs and data structures\n\n\n\n\n\nJan 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nrstats\n\n\n\nUsing controlled dictionaries for low-touch documentation, validation, and usability of tabular data\n\n\n\n\n\nSep 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npkgdev\n\n\nrmarkdown\n\n\nrstats\n\n\nworkflow\n\n\n\nA workflow for refactoring one-time analyses to sustainable data products\n\n\n\n\n\nMay 4, 2019\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nGo to All Posts →"
  },
  {
    "objectID": "index.html#posts",
    "href": "index.html#posts",
    "title": "Welcome!",
    "section": "",
    "text": "RecentFavorites\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\ndbt\n\n\nsql\n\n\ndata\n\n\nml\n\n\n\nPlaying with the potential, perils, and design principles of deploying ML models into the analytical database using orbital’s sklearn-to-sql translation, sqlglot, and dbt\n\n\n\n\n\n2025-08-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nworkflow\n\n\nrstats\n\n\npython\n\n\nquarto\n\n\nrmarkdown\n\n\n\nLiterate programming excels at capturing our stream of conscience. Our stream of conscience does not excel at explaining the impact of our work. Notebooks enable some of data scientists’ worst tendencies in technical communication, but Quarto’s embed feature bridges the gap between reproducible research and resonate story-telling.\n\n\n\n\n\n2025-07-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nml\n\n\n\nThe orbital package offers an interface for translating a fitted SciKitLearn pipeline to pure SQL for scaling predictions. In this tech note, I explore how this framework can (mostly) be used for xgboost models, as well, with a bit of wrangling (and a few limitations).\n\n\n\n\n\n2025-07-19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\n\nSwitching languages is about switching mindsets - not just syntax. New developments in python data science toolings, like polars and seaborn’s object interface, can capture the ‘feel’ that converts from R/tidyverse love while opening the door to truly pythonic workflows. (Updated from 2025 for new tools).\n\n\n\n\n\n2025-01-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nquarto\n\n\nrmarkdown\n\n\nworkflow\n\n\n\nA quick tech note on Netlify’s managed authentication solution\n\n\n\n\n\n2024-11-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nData teams may struggle to quantify the benefits of good data documentation. But running countless ad hoc validation queries can incur both computational and cognitive cost.\n\n\n\n\n\n2024-07-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\ntutorial\n\n\n\nGetting comfortable in a new language is more than the packages you use. Syntactic sugar in base python increases the efficiency, and aesthetics of python code in ways that R users may enjoy in packages like glue and purrr. This post collects a miscellaneous grab bag of tools for wrangling, formatting (f-strings), repeating (list comprehensions), faking data, and saving objects (pickle)\n\n\n\n\n\n2024-01-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nData documentation isn’t a box to check; it’s an active member of your team with many jobs-to-be-done. In this cross-post with Select Star, I write about how effective documentation can be your data products’ developer advocate for users, project manager for developers, and chief of staff for data leaders\n\n\n\n\n\n2024-01-15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\ntutorial\n\n\n\nIn this follow-up post to Python Rgonomics, we deep dive into some of the advanced data wrangling functionality in python’s polars package to see how it’s powertools like column selectors and nested data structures mirror the best of dplyr and tidyr’s expressive and concise syntax\n\n\n\n\n\n2024-01-13\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nWriting is thinking; documenting is planning and executing. In this cross-post with Select Star, I write about how teams can produce high-quality and maintainble documentation by smartly structuring planning and development documentation and effeciently recycling them into long-term, user-friendly docs\n\n\n\n\n\n2024-01-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\n\nSwitching languages is about switching mindsets - not just syntax. New developments in python data science toolings, like polars and seaborn’s object interface, can capture the ‘feel’ that converts from R/tidyverse love while opening the door to truly pythonic workflows\n\n\n\n\n\n2023-12-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\n\nFive highlights and links to select talks\n\n\n\n\n\n2023-11-18\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\n\nProactive collection of data to comply or confront assumptions\n\n\n\n\n\n2023-05-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nRounding out my three-part ETL series form Airbyte’s developer blog\n\n\n\n\n\n2023-05-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nCross-post from guest post on Airbyte’s developer blog\n\n\n\n\n\n2023-03-22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\ndbt\n\n\ncrosspost\n\n\n\nAfter a prior post on the merits of grouped data quality checks, I demo my newly merged implementation for dbt\n\n\n\n\n\n2023-01-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nsql\n\n\n\nOut-of-memory processing of North Carolina’s voter file with DuckDB and Apache Arrow\n\n\n\n\n\n2022-09-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npython\n\n\nsql\n\n\ndata\n\n\ndata-disasters\n\n\n\nHow we do (or don’t) think about null values and why the polyglot push makes it all the more important\n\n\n\n\n\n2022-09-05\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\ndbt\n\n\n\nAfter a prior post on the merits of grouped data quality checks, I demo my newly merged implementation for dbt\n\n\n\n\n\n2022-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nshiny\n\n\ndata\n\n\n\nKey issues when adding persistent storage to a Shiny application, featuring {golem} app development and Digital Ocean serving\n\n\n\n\n\n2022-01-02\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nrmarkdown\n\n\n\nMuch like ice sculpting, applying powertools to absolutely frivolous pursuits\n\n\n\n\n\n2021-12-11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\n\nWhich of these numbers doesn’t belong? -1, 0, 1, NA. You can’t judge data quality without data context, so our tools should enable as much context as possible.\n\n\n\n\n\n2021-11-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata-disasters\n\n\n\nA personal encounter with ‘intelligent’ data products gone wrong\n\n\n\n\n\n2021-11-10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\ndbt\n\n\n\nFollowing up on ‘Embedding Column-Name Contracts… with dbt’ to demo my new dbtplyr package to further streamline the process\n\n\n\n\n\n2021-09-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\ndata\n\n\nelt\n\n\n\nA right-sized solution to automated data monitoring, alerting, and reporting using R (pointblank, projmgr), GitHub (Actions, Pages, issues), and Slack\n\n\n\n\n\n2021-08-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\nsql\n\n\n\nTricks for modularizing and refactoring your projects SQL/R interface. (Image source techdaily.ca)\n\n\n\n\n\n2021-07-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nelt\n\n\n\nA data consumer’s guide to validating data based on the failure modes data producer’s try to avoid\n\n\n\n\n\n2021-05-27\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nelt\n\n\npython\n\n\n\nExploring how Playwright‘s headless browser automation (and its friends) can help unite the states’ data\n\n\n\n\n\n2021-05-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nsql\n\n\ndbt\n\n\n\ndbt supercharges SQL with Jinja templating, macros, and testing – all of which can be customized to enforce controlled vocabularies and their implied contracts on a data model\n\n\n\n\n\n2021-02-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\ntutorial\n\n\n\nAn informal primer to causal analysis designs and data structures\n\n\n\n\n\n2021-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\nresources\n\n\n\nFree books, lectures, blogs, papers, and more for a causal inference crash course\n\n\n\n\n\n2021-01-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\njtbd\n\n\n\nOn the jobs-to-be-done and design principles for internal tools\n\n\n\n\n\n2021-01-21\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\ndata\n\n\nsql\n\n\ntutorial\n\n\n\nUsing the tidyverse’s expressive data wrangling vocabulary as a preprocessor for elegant SQL scripts. (Image source techdaily.ca)\n\n\n\n\n\n2021-01-16\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\ndata\n\n\n\nAn R package for maintaining controlled vocabularies to encode contracts between data producers and consumers\n\n\n\n\n\n2020-12-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nworkflow\n\n\n\nMarketing maintenance work with irrational exuberance\n\n\n\n\n\n2020-09-20\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\ntutorial\n\n\n\nAn introduction to browser-based interactivity of htmlwidgets – no Shiny server required!\n\n\n\n\n\n2020-09-12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nrstats\n\n\n\nUsing controlled dictionaries for low-touch documentation, validation, and usability of tabular data\n\n\n\n\n\n2020-09-06\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshiny\n\n\nrstats\n\n\nworkflow\n\n\ntutorial\n\n\n\nDon’t believe the documentation! Shiny modules aren’t just for advanced users; they might just be a great entry point for development\n\n\n\n\n\n2020-07-26\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresources\n\n\nrstats\n\n\n\nReadings and assorted ideas about creating and maintaining low-overhead documentation\n\n\n\n\n\n2020-07-03\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrmarkdown\n\n\nrstats\n\n\n\nA few tips and tools for finding the right selectors to style in RMarkdown\n\n\n\n\n\n2020-06-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\n\nA walkthrough of using the projmgr package for GitHub-based project management via R\n\n\n\n\n\n2020-05-14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npkgdev\n\n\nworkflow\n\n\nrstats\n\n\nrmarkdown\n\n\n\nA recommended tech stack for implementing RMarkdown Driven Development\n\n\n\n\n\n2020-02-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresources\n\n\nrstats\n\n\n\nCase studies of the impact of R use on organizational culture and collaboration\n\n\n\n\n\n2019-08-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresources\n\n\nworkflow\n\n\n\nAn annotated bibliography of advice for getting started with reproducible research\n\n\n\n\n\n2019-08-30\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\n\nA walkthrough of a GitHub template for making your own RMarkdown and ggplot2 theme package\n\n\n\n\n\n2019-05-25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspeaking\n\n\nnotes\n\n\n\nConference planning tips to design a good speakers experience\n\n\n\n\n\n2019-05-07\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npkgdev\n\n\nrmarkdown\n\n\nrstats\n\n\nworkflow\n\n\n\nA workflow for refactoring one-time analyses to sustainable data products\n\n\n\n\n\n2019-05-04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nspeaking\n\n\nnotes\n\n\n\nA proposed workflow for methodically developing a good presentations\n\n\n\n\n\n2019-04-20\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\njtbd\n\n\n\nOn the jobs-to-be-done and design principles for internal tools\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\ntutorial\n\n\n\nAn informal primer to causal analysis designs and data structures\n\n\n\n\n\nJan 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nrstats\n\n\n\nUsing controlled dictionaries for low-touch documentation, validation, and usability of tabular data\n\n\n\n\n\nSep 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npkgdev\n\n\nrmarkdown\n\n\nrstats\n\n\nworkflow\n\n\n\nA workflow for refactoring one-time analyses to sustainable data products\n\n\n\n\n\nMay 4, 2019\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nGo to All Posts →"
  },
  {
    "objectID": "index.html#talks",
    "href": "index.html#talks",
    "title": "Welcome!",
    "section": "Talks",
    "text": "Talks\n\nRecentFavorites\n\n\n\n\n\n\n\n\n\n\n\n\nCasual Inference Pod - Optimizing Data Workflows with Emily Riederer (Season 6, Episode 8)\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\n\nCasual Inference is a podcast on all things epidemiology, statistics, data science, causal inference, and public health. Sponsored by the American Journal of Epidemiology. As a guest on this episode, I discuss data science communication, the different challenges of causal analysis in industry versus academia, and much more.\n\n\n\n\n\nJun 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA different type of DAG - data pipelines for epidemiology\n\n\n\n\n\n\nworkflow\n\n\ndata\n\n\n\nA tour of data pipeline techniques and tools for use in academia\n\n\n\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython Rgonomics\n\n\n\n\n\n\nworkflow\n\n\npython\n\n\nrstats\n\n\n\nA survey of modern python tooling that “feels good” to R users\n\n\n\n\n\nAug 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nData Downtime Horror Stories Panel\n\n\n\n\n\n\ndata\n\n\n\nPanel discussion with Chad Sanderson and Joe Reis, hosted by Monte Carlo Data, on our thorniest brushes with data downtime, leading data teams to tackle data quality at scale with testing, contracts, observability and monitoring, and more.\n\n\n\n\n\nOct 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nOperationalizing Column-Name Contracts with dbtplyr\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nAn exploration of how data producers and consumers can use column names as interfaces, configuations, and code to improve data quality and discoverability. The second half of the talk demonstrates how to implement these ideas with my dbtplyr dbt package.\n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nScaling Personalized Volunteer Emails\n\n\n\n\n\n\ndata\n\n\nelt\n\n\n\nAn overview of the data stack used to automate over 50,000 personalized emails to voter turnout volunteers using BigQuery, dbt, Census, and MailChimp\n\n\n\n\n\nJun 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Design Patterns\n\n\n\n\n\n\ncausal\n\n\n\nAn overview of basic research design patterns in causal inference, modern extensions, and data management strategies to set up a causal inference initiative for success\n\n\n\n\n\nJun 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nDataFold Data Quality Meet Up\n\n\n\n\n\n\nelt\n\n\ndata\n\n\n\nJoined a panel of speakers to discuss tips and tricks for running dbt at scale\n\n\n\n\n\nMay 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nPosit Data Science Hangout\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\n\nEach week, host Rachael Dempsey invites an accomplished data science leader to talk about their experience and answer questions from the audience. The discussion focuses mainly on the human elements of data science leadership. There’s no sales or marketing fluff, just great insights from inspiring professionals.\n\n\n\n\n\nApr 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluation without Experimentation\n\n\n\n\n\n\ncausal\n\n\n\nAn introduction to inverse propensity of treatment weighting for program evaluation with applications to Two Million Texans’ relational organizing campaign during the 2022 midterms\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTaking Flight with Shiny: a Modules-First Approach\n\n\n\n\n\n\nworkflow\n\n\nrstats\n\n\nshiny\n\n\n\nAn argument for the individual and organization-wide benefits of teaching new developers Shiny with a modules-first paradigm.\n\n\n\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Data (error) Generating Process\n\n\n\n\n\n\ndata\n\n\n\nInterrogating the data generating process to devise better data quality tests.\n\n\n\n\n\nNov 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThe Data Engineering Podcast: Column Names as Contracts\n\n\n\n\n\n\ndata\n\n\n\nDiscussing how column names can serve as a light-weight alternative to data catalogs and contracts and how to implement this approach with dbtplyr\n\n\n\n\n\nJan 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUIUC STAT447 (Data Science Programming) Guest Lecture\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\n\nDiscussing how to move from scripting to tool development, designing tools in enterprise, and navigating diverse data career paths\n\n\n\n\n\nNov 17, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Names as Contracts\n\n\n\n\n\n\ndata\n\n\n\nExploring the benefits of using controlled vocabularies to encode metadata in column names, and demonstrations of implementing this approach with the convo R package or dbt extensions of SQL.\n\n\n\n\n\nFeb 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\noRganization: Design patterns for internal packages\n\n\n\n\n\n\nworkflow\n\n\npkgdev\n\n\nrstats\n\n\n\nAn overview of the unique design challenges and opportunities when building R packages for use inside of a single organization versus open-source. By using the jobs-to-be-done framework, this talk explores how internal packages can be better teammates by following specific design patterns for API design, testing, documentaiton, and more.\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nprojmgr: Managing the human dependencies of your project\n\n\n\n\n\n\nworkflow\n\n\nrstats\n\n\npkgdev\n\n\n\nA lightning talk on key features of the projmgr package which brings enables code-based planning and reporting workflows grounded in GitHub issues and milestones\n\n\n\n\n\nJul 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown Driven Development\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nHow and why to refactor one time analyses in RMarkdown into sustainable data products\n\n\n\n\n\nJan 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nAssorted talks on designing analytical tools and communities for enterprise\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nA variety of related talks to creating innersource culture with R packages and related tools\n\n\n\n\n\nNov 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\ntidycf: Turning analysis on its head by turning cashflows on their side\n\n\n\n\n\n\nworkflow\n\n\npkgdev\n\n\nrstats\n\n\n\nA case study on building an internal R package for customer lifetime value modeling at Capital One and leading broader analyst adoption of open-source tooling and reproducible workflows through a community of practice.\n\n\n\n\n\nNov 1, 2017\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperationalizing Column-Name Contracts with dbtplyr\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nAn exploration of how data producers and consumers can use column names as interfaces, configuations, and code to improve data quality and discoverability. The second half of the talk demonstrates how to implement these ideas with my dbtplyr dbt package.\n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Design Patterns\n\n\n\n\n\n\ncausal\n\n\n\nAn overview of basic research design patterns in causal inference, modern extensions, and data management strategies to set up a causal inference initiative for success\n\n\n\n\n\nJun 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\noRganization: Design patterns for internal packages\n\n\n\n\n\n\nworkflow\n\n\npkgdev\n\n\nrstats\n\n\n\nAn overview of the unique design challenges and opportunities when building R packages for use inside of a single organization versus open-source. By using the jobs-to-be-done framework, this talk explores how internal packages can be better teammates by following specific design patterns for API design, testing, documentaiton, and more.\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown Driven Development\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nHow and why to refactor one time analyses in RMarkdown into sustainable data products\n\n\n\n\n\nJan 30, 2020\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nGo to All Talks →"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Welcome!",
    "section": "Publications",
    "text": "Publications\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal and Associational Language in Observational Health Research: A Systematic Evaluation\n\n\n\n\n\n\ncausal\n\n\n\nWe screened over 1,000 articles from 18 high-profile journals published from 2010-2019 to review the use of causal language to describe results in experimental and observational data. \n\n\n\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n97 Things Every Data Engineer Should Know: Collective Wisdom from the Experts\n\n\n\n\n\n\ndata\n\n\n\nWith this in-depth book, data engineers will learn powerful, real-world best practices for managing data—both big and small. Contributors from companies including Google, Microsoft, IBM, Facebook, Databricks, and GitHub share their experiences and lessons learned on cleaning, prepping, wrangling, and storing data. I contributed 6 of the 97 essays. \n\n\n\n\n\nAug 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nR Markdown Cookbook\n\n\n\n\n\n\nrmarkdown\n\n\nrstats\n\n\n\nThis book is designed to provide a range of examples of how to extend the functionality of your R Markdown documents. As a cookbook, this guide is recommended to new or intermediate R Markdown users who desire to enhance the efficiency of using R Markdown and also explore the power of R Markdown. Read online at https://bookdown.org/yihui/rmarkdown-cookbook/ \n\n\n\n\n\nSep 1, 2020\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\nGo to All Publications →"
  },
  {
    "objectID": "post/abstraction-extraction/index.html",
    "href": "post/abstraction-extraction/index.html",
    "title": "The Art of Abstraction in ETL: Dodging Data Extraction Errors",
    "section": "",
    "text": "Whenever I think about data developer tooling, I always like to take the perspectives of:\n\nUnderstanding what higher-level abstractions that it provides that help eliminate rote work or reduce mental overhead for data teams. In the spirit of my post on the jobs-to-be-done of innersource analysis tools, this can be framed as what ‘jobs’ that tool can be hired to do (and with what level of responsibility and autonomy)\nInterrogating the likely failure modes in the data stack based on the mechanics of the system, in the spirit of my call for hypothesis-driven data quality testing\n\nThese two themes motivated my recent guest post for Airbyte’s developer blog on The Art of Abstraction in ETL: Dodging Data Extraction Errors. In this post, I argue:\n\nCooking a meal versus grocery shopping. Interior decorating versus loading the moving van. Transformation versus Extract-Load. It’s human nature to get excited by flashy outcomes and, consequently, the most proximate processes that evidently created them.\n\n\nThis pattern repeats in the data world. Conferences, blog posts, corporate roadmaps, and even budgets focus on data transformation and the allure of “business insights” that might follow. The steps to extract and load data are sometimes discounted as a trivial exercise of scripting and scheduling a few API calls.\n\n\nHowever, the elegance of Extract-Load is not just the outcome but the execution – the art of things not going wrong. Just as interior decorating cannot salvage a painting damaged in transit or a carefully planned menu cannot be prepared if half of the ingredients are out-of-stock, the Extract-Load steps of data processing have countless pitfalls which can sideline data teams from their ambitious agendas and aspirations.\n\nI then go on to explore common challenges in successfully extracting data from an API and the abstractions that can aid in this process.\nPlease check out the full post on Airbyte’s site! I hope it resonates."
  },
  {
    "objectID": "post/causal-design-patterns/index.html",
    "href": "post/causal-design-patterns/index.html",
    "title": "Causal design patterns for data analysts",
    "section": "",
    "text": "Mnemonic illustrations of CI methods, made with Excalidraw\nSoftware engineers study design patterns1 to help them recognize archetypes, consider approaches with a common language, and reuse tried-and-true architectures. Similarly, statistics has many prototypical analyses, but too often these frameworks are siloed within specific disciplines and clouded by domain-specific language. This makes methods harder to discover and hides their general applicability. Consequently, it can be hard for practitioners outside of these fields to recognize when the problem they are facing fits one of these paradigms.\nObservational causal inference is one such field. The need to understand true causal (versus correlative) effects and to derive meaning and strategy from “found” historical data (as opposed to experimentally “produced” data) is nearly universal, but methods are scattered across epidemiology, economics, political science, and more.\nThis post aims to break down some of those barriers by briefly summarizing common cross-disciplinary design patterns for measuring causal effects from observational data with an emphasis on potential use in industry. For each method, I provide an illustration, overview of the method, summary of the required data structure and key assumptions, and a hypothetical consumer retail example.\nCausal inference is complex and doing it well requires both statistical and domain expertise. Instead of providing all of the details which are already eloquently described in countless free resources, this post humbly aims to advertise these methods so analysts can add them into their mental index and investigate them further when they encounter a relevant question."
  },
  {
    "objectID": "post/causal-design-patterns/index.html#why-ci-in-industry",
    "href": "post/causal-design-patterns/index.html#why-ci-in-industry",
    "title": "Causal design patterns for data analysts",
    "section": "Why CI in Industry?",
    "text": "Why CI in Industry?\nClassic examples of causal questions include “Does smoking cause cancer?” and “Does education cause extra income?” Naively, we might compare outcomes (cancer or income) for those receiving the treatment (smoking and education) and those not. However, the fact that someone chose to smoke or pursue higher education creates selection bias; these behaviors likely correlate to other factors (risk taking, financial support, etc.) which can also impact the outcome.\nIn industry, businesses want to know what causal effect their strategies (e.g. promotional offers) have on customer behavior. Here, of course, business have a major advantage over the examples above because the assignment mechanism into the treatment group (e.g. whom to send a discount code) is known and under their control. They often also have richer “pre-treatment” behavior for each individual (customer) which can help both assess and correct for bias.\nHowever, these advantages don’t make causal inference unnecessary; if anything, they simply make it more possible and more relevant. Good businesses don’t act at random. For example, we market to customers who are likely to be interested in our company and who, therefore, might have been interested even without marketing. When it comes to measuring effectiveness, good business is bad science. Because our treatments are not given at random, comparing the outcomes of treated and untreated groups is confounded and biased towards making us think we are more effective than we actually may be.\nOne antidote to this is true experimentation in which treatment is randomly assigned within the homogenous target population. Experimentation, particularly A/B tests, have become a mainstay of industry data science, so why does observational causal inference matter?\n\nSome situations you cannot test due to ethics, logistics, or reputational risk\nTesting can be expensive. There are direct costs (e.g. testing a marketing promotion) of instituting a policy that might not be effective, implementation costs (e.g. having a tech team implement a new display), and opportunity costs (e.g. holding out a control group and not applying what you hope to be a profitable strategy as broadly as possible)2\nRandomized experimentation is harder than it sounds! Sometimes experiments may not go as planned, but treating the results as observational data may help salvage some information value\nData collection can take time. We may want to read long-term endpoints like customer retention or attrition after many year.3 When we long to read an experiment that wasn’t launched three years ago, historical observational data can help us get a preliminary answer sooner\nSimilarly, we can’t time travel. Some strategies that we’ve fully launched we cannot unwind just to test. If we regret not testing after the fact, we can potential find a quasiexperiment to analyze\nIt’s not either-or but both-and. Due to the financial and temporal costs of experimentation, causal inference can also be a tool to help us better prioritize what experiments are worth running\nEven when you can experiment, understanding observational causal inference can help you better identify biases and design your experiments\n\nBeyond these specific challenges, perhaps the best reason is that there are so many questions that you can answer. As we’ll see, most all of these methods rely on exploiting some arbitrary amount of randomness in whether or not a specific individual or group received a certain treatment. Industry (and life in general) is full of non-random but well-defined (and somewhat arbitrary) policies which make it fertile ground for observational causal inference. Analysts can embark on data search-and-rescue missions and find new uses in reams of historical data that might be otherwise discounted as hopelessly biased or outdated."
  },
  {
    "objectID": "post/causal-design-patterns/index.html#unifying-themes",
    "href": "post/causal-design-patterns/index.html#unifying-themes",
    "title": "Causal design patterns for data analysts",
    "section": "Unifying themes",
    "text": "Unifying themes\nTo illustrate potential applications, this post will provide a brief overview of Stratification, Propensity Score Weighting, Regression Discontinuity, and Difference in Differences with motivating examples from consumer retail. Each of these methods deal with situations where different groups receive different treatments but the assignment of groups was not completely random.\nTo measure a causal effect, we want to somehow consider the potential outcomes and be able to contrast the average outcome under the treatment versus the average outcome under a counterfactual scenario in which similar observations went untreated.\nTo create this counterfactual without true randomization, these methods attempt to exploit different sources of partial random variation4 while avoiding different types of confounding in order to derive a valid inference.\nThe types of partial randomization found in your historical data and the types of biases you are concerned about dictate which methods are applicable. In short:\n\nIf you have significant overlap between “similar” treated and untreated individuals but the treatment was not randomly assigned, stratification or propensity score weighting can help you rebalance your data so that your treated and untreated groups have a more similar distribution of traits and their average outcomes are more comparable\nIf you have disjoint treated and untreated groups partitioned by a sharp cut-off, regression discontinuity allows you to measure the local treatment effect at the juncture between groups\nIf treatments are assigned to different populations, difference-in-differences and event study methods help to compare different groups across multiple time periods."
  },
  {
    "objectID": "post/causal-design-patterns/index.html#stratification",
    "href": "post/causal-design-patterns/index.html#stratification",
    "title": "Causal design patterns for data analysts",
    "section": "Stratification",
    "text": "Stratification\n\nStratification helps us correct for imbalanced weighting of treated and control populations that arise due to a non-random assignment mechanism.\nTLDR: When you have “similar”5 treated and untreated individuals with different distributions on a small number of relevant dimensions, use stratification helps to rebalance these groups to make their average effects more comparable\nMotivating Example:\n\nSuppose we attempted to A/B test “one-click instant checkout” on our Black Friday website and want to measure the effect on total purchase amount6.\nDue to a glitch the code, web users had a 50% chance of being in the treatment group (i.e. seeing the button) but mobile users only had a 30% chance.\nAdditionally, we know that mobile users tend to spend less per order. Thus, the fact that web users are over-represented in the treatment group means that simply comparing treatment versus control outcomes will bias our results and make the causal effect of the button appear higher than it is in actuality.\n\nApproach:\n\nBin (stratify) the population by subgroups based on values of each observation’s characteristics\nCalculate the average treatment effect in each subgroup\nTake the weighted average of the subgroup effects, weighted for the population of interest (e.g. treatment distribution, control distribution, population distribution)\n\nKey Assumptions:\n\nAll common causes of the treatment and the outcome can be captured through the covariates (more mathematically, the outcome and the treatment are independent conditional on the covariates)\nAll observations had some positive probability of being treated. Heuristically, you can think of this as meaning in the image above that there are no major regions where there are only green control observations and no blue treatment observations\nOnly a small number of variables require adjustment (because they impact both the treatment likelihood and the outcome) Otherwise, we are plagued by the curse of dimensionality\n\nExample Application:\n\nAlthough we could run another A/B test, we only had one shot at Black Friday and need to decide whether or not to include this button next year. We can calculate separate treatment effects within web orders versus mobile orders and then weight average these effects based on the overall channel distribution across all orders.\n(Technically with modern web design, we could make separate decisions for mobile and web users, so we might not actually care about the overall treatment effect. This is just an example.)\n\nRelated Methods:\n\nPropensity score weighting (covered next) can be seen as a more advanced form of stratification. Both methods share the goal of making the distribution of treatment and control groups more similar.\n\nTools:\n\nThis method is computationally simple, so it can essentially be done with SQL or any basic data wrangling package like dplyr or pandas so long as the tool can perform aggregations by group"
  },
  {
    "objectID": "post/causal-design-patterns/index.html#propensity-score-weighting",
    "href": "post/causal-design-patterns/index.html#propensity-score-weighting",
    "title": "Causal design patterns for data analysts",
    "section": "Propensity Score Weighting",
    "text": "Propensity Score Weighting\n\nSimilar to stratification, propensity score (think “likelihood of treatment”) weighting helps us correct for systemic differences between treatment and control populations that stem from non-random assignment mechanisms. However, this approach allows us to control for many observable characteristics that influence assignment by reducing all relevant information into a single score on which we balance.7\nTLDR: When you have “similar”8 treated and untreated individuals with different distributions on a larger number of relevant dimensions, propensity score weighting helps to rebalance these groups to make their average effects more comparable\nMotivating Example:\n\nWe sent a marketing promotion text message to all of our customers for whom we have a valid cell phone number and want to know the causal effect on the likelihood to make a purchase in the next month.\nWe did not intentionally leave a control group untreated, but we can observe the untreated response for customers for whom we do not have a valid cell phone number.\nPhone number is an optional field on UI when making a purchase, so there is some randomness between the population; however, we know that those who do not provide a phone number are less frequent shoppers on average but a full spectrum of low-to-high frequency shoppers exists in both groups\nThus, if we simply compare the treated and untreated groups, the promotion will look more effective than it really was because it is being sent to generally more active customers.\n\nApproach:\n\nModel the probability of receiving the treatment (the propensity score) based each observation’s observable characteristics that are relevant both to their treatment assignment and to their outcome.9\nIn observational data, the treatment group’s distribution of propensity scores will generally skew right (tend higher, shown in solid blue) and the control group’s distribution will skew left (tend lower, shown in solid green)\nUse predicted probabilities (propensity scores) to weight the untreated observations to fit the same distribution of treatment likelihood as the control group (shown in dotted green)\nWeights can be constructed in different ways depending on the quantity of interest (average treatment effect of treated, average treatment effect of population, average treatment effect if given to the control population, etc.)\nApply weights when calculating the average outcome in each of the treated and control groups and subtract to find the treatment effect\n\nKey Assumptions:\n\nAll common causes of the treatment and the outcome can be captured through the covariates (more mathematically, the outcome and the treatment are independent conditional on the covariates)\nAll observations had some positive probability10 of being treated. Heuristically, you can think of this as meaning in the image above there are no regions on the line where the treatement or control frequency curves go all the way down to zero\n\nExample Application:\n\nModel the propensity of treatment (or equivalently, having a phone number on record) based on demographics and historical purchase behavior\nDerive weights to calculate the average treatment effect of the treated. Treated observations are left unweighted; for untreated observations, the weight is the ratio of the propensity score over one minus the propensity score11 12\n\nRelated Methods:\n\nStratification is conceptually similar to propensity score weighting since it implicitly calculates the treatment effect on a reweighted sample. There, the reweighting comes after computing localized effects instead of before\nPropensity scores are sometimes also used in matching, but there are arguments against this approach\n\nTools:\n\nWeightIt R package\nEasy to implement with simple stats::glm() as shown in Lucy D’Agostino McGowan’s blog post"
  },
  {
    "objectID": "post/causal-design-patterns/index.html#regression-discontinuity",
    "href": "post/causal-design-patterns/index.html#regression-discontinuity",
    "title": "Causal design patterns for data analysts",
    "section": "Regression Discontinuity",
    "text": "Regression Discontinuity\n\nOften, in real life and particularly in industry, we violate the “positive probability of treatment throughout the covariate space” assumption required by stratification and propensity score weighting. Business and public policy often use strategies that have sharp cut-offs (e.g. customer segmentation based on age and spend) with individuals on either side of this cut-off receiving different treatments. In such cases, we have no relevant observations to re-weight. However, we can apply a regression discontinuity design to understand the local effect of a treatment at the point of discontinuity.\nTLDR: When you have disjoint treated and untreated individuals separated by a sharp cut-off, use the arbitrary variation in treatment assignment for those right above or below the cut-off to measure a local causal effect\nMotivating Example:\n\nCustomers who have not made a purchase in 90 days are sent a “$10 Off Your Next Purchase” coupon.\nWe can use the sharp cut-off in “days since last purchase” to measure the effect of a coupon on spend in the next year.\nWhile it’s implausible to think that customers who haven’t purchased in 10 days are similar to those who have not in 150 days, customers who haven’t purchased in 88 days are likely not substantively different than those who have not purchased in 92 days except for the different treatment\n\nApproach:\n\nA set of individuals either do or do not receive a treatment based on an arbitrary cut-off.\nModel the relationship between the “running” variable (the variable used in the cut-off) and the outcome on both sides of the cut-off. Then, the local treatment effect at the point of the cut-off can be determined by the difference in modeled outcome at this value of the running variable.\nNote that we can only measure the local treatment effect at the cut-off – not the global average treatment effect, as we did with stratification and propensity score weighting\n\nKey Assumptions:\n\nThe assignment rule is unknown to the individuals being observed so it cannot be gamed\nThe outcome of interest can be modeled as a continuous function with respect to the decisioning variable\nWe can fit a reasonably well-specified and simple model between the outcome of interest and the decisioning variable. Since RDD necessarily requires us to use estimates from the very “tails” of our model, overly complex models (e.g. high degree polynomials) can reach bizarre conclusions\n\nExample Application:\n\nWe can model the relationship between “days since last spend” and “spend in the next year” and evaluate the difference in modeled values at the value of 90 for “days since last spend”\nA counter-example that violates the assumption would be advertising “Free Shipping and Returns over $50” and attempting to measure the effect of offering free shipping on future customer loyalty. Why? This cut-off is known to individuals ahead of time and can be gamed. For example, perhaps less loyal are more skeptical of a company’s products and more likely to return, so they might intentionally spend more than $50 to change their classification and gain entrance to the treatment group\nAs a side note, could you have simply A/B tested in this case? Definitely! It would be easy to get a fairer comparison by randomly withholding the coupon from some customers. However, its unlikely that every historical campaign your organization ever ran had a good randomized control. With this approach, you can still extract some useful insights to inform your next steps.\n\nRelated Methods:\n\nFuzzy regression discontinuity allows for cut-off points to be probabilistic instead of absolute\nInstrumental variable methods and two-stage least squares can be thought of as a broader family in which regression discontinuity is a simple example. More broadly, these methods address confounding between a treatment and an outcome by modeling the relationship between a treatment and an instrument which is only related to the outcome through the treatment\n\nTools:\n\nrdd R package for regression discontinuity\nAER R package for instrumental variable methods"
  },
  {
    "objectID": "post/causal-design-patterns/index.html#difference-in-differences",
    "href": "post/causal-design-patterns/index.html#difference-in-differences",
    "title": "Causal design patterns for data analysts",
    "section": "Difference in Differences",
    "text": "Difference in Differences\n\nSo far, we’ve looked at methods that try to make groups as similar before treatment as possible. One way we can evaluate these methods is to see if, as we would expect, their values of the variable of interest before treatment are similar (e.g. spend in the 6 months leading up to a promotion.) However, the data we have available makes that intractably restrictive. Instead of looking for similarity in absolute level, difference-in-differences helps us more flexibly settle for similarity in trajectories over time. That is, instead of comparing treatment and control groups within the same population at the same time, we can compare the relative change across treatment and control populations across time.\nTLDR: When you have group-level treatments or data available, use random variation across populations to compare their overall trends over time\nMotivating Example:\n\nWe want to estimate the effect of a store remodel on visits.\nA remodel affects all potential customers, so this “treatment” cannot be applied at the individual level; in theory, it could be randomized to individual stores, but we do not have the budget for or interest to randomly remodel many stores before there is evidence of a positive effect.\n\nApproach:\n\nIn two separate populations, one receives the treatment and one does not. We believe but-for the treatment the two populations would have similar trends in outcome\nWe can estimate the treatment effect by taking the difference between the post-treatment difference between populations and the pre-treatment difference between populations\nIn effect, this is the same as extrapolating the counterfactual for the treated population in the post-treatment period if it had not received treatment (the dashed line in the image above)\nTechnically, this is implemented as a fixed-effects regression model\n\nKey Assumptions:\n\nThe decision to treat the treatment group was not influenced by the outcome\nIf not for the treatment, the two groups being compared would have parallel trends in the outcome. Note that groups are allowed to have different levels but must have similar trends over time\nThere is no spill-over effect such that treating the treatment group has an effect on the control group\n\nExample Application:\n\nWe can estimate the effect of a store remodel on visits by comparing store traffic before and after the remodel with traffic at a store that did not remodel.\nNote how sensitive this method is to our assumptions:\n\nif the remodel is an expansion and caused by a foreseen increase in traffic, our first assumption is violated and our effect will be overestimated\nif the control we chose is another nearby store in the same town, we could experience spillover effects where more people who would have otherwise gone to the control store decide to go to the treatment store instead. This again would overestimate the effect\n\nAnother counter-example that violates the assumption would be measuring the effect of placing a certain product brand near a store’s check-out on sales and using sales of a different brand of the same product as the control. Why? Since these products are substitutes, the product placement of the treatment group could “spillover” to negatively effect sales of the control\n\nRelated Methods:\n\nVariants exist that relax different assumptions. For example, we may consider cases in which different units receive the treatment at different times, different units have different (heterogenous) treatment effects, the parallel trend assumption only holds after conditioning on covariates, and many more scenarios\nSynthetic control methods can be thought of as an extension of difference-in-differences where the control is a weighted average of a number of different possible controls\nBayesian structural time-series methods relax the “parallel trends” asumptions of difference-in-differences by modeling the relationship between time series (including trend and seasonal components)\n\nTools:\n\ndid R package for difference-in-differences\nSynth R package for synthetic controls\nCausalImpact R package for Bayesian structural time-series"
  },
  {
    "objectID": "post/causal-design-patterns/index.html#implications",
    "href": "post/causal-design-patterns/index.html#implications",
    "title": "Causal design patterns for data analysts",
    "section": "Implications",
    "text": "Implications\n\nIf the examples above have at all intrigued you, there are a few more things to keep in mind. Explaining these design patterns is easy; implementing them when, and only when, relevant is hard. Causal inference requires investment in data management, domain knowledge, and probabilistic reasoning.\nData management is need to ensure that data on past treatments is preserved, discoverable, and sufficiently detailed. All of these methods require rich data with measures of baseline characteristics of each individual being studied and a solid understanding of the treatment they received. This may seem obvious, but it’s easy to neglect to preserve data of sufficient granularity. For example, we might have a database that maps every customer’s account to a system-generated campaign_id denoting some marketing campaign that they participated in; however, unless information about that specific campaign (the specific treatment, the targeting, the timing, etc.) is readily available, this is not terribly useful. Additionally, as in our stratification example, some of the best opportunities for causal inference come from execution errors (or, more gently, “natural experiments”). We may be inclined to forget about these errors and move on, but information on events that did not go as intended can be powerful fodder for future analysis.\nDomain knowledge is essential to validating the assumptions. Unlike other forms of inference (e.g. a basic linear regression), many of the assumptions we discussed for the methods above cannot be computational or visually assessed (e.g. not like the quintessential residual or QQ plots). Instead, assumptions rely largely and careful attention to detail combined with intuition and background knowledge from one’s domain. This means causal inference should necessarily be a human-in-the-loop activity.\nFinally, a solid grasp of probabilistic reasoning and understanding of these methods is also critical. As many of the resources I link below discuss at length, it’s easy to do causal inference wrong. For example, attempting to control for the wrong variables can sometimes induce correlations and cause biases instead of eliminating them.13"
  },
  {
    "objectID": "post/causal-design-patterns/index.html#learn-more",
    "href": "post/causal-design-patterns/index.html#learn-more",
    "title": "Causal design patterns for data analysts",
    "section": "Learn More",
    "text": "Learn More\nThe point of this post is not to teach any one method of causal inference but to help raise awareness for basic causal questions, data requirements, and analysis designs which one might be able to use in the wild. There’s a plethora of fantastic resources available to learn more about the specific implementation of these or other methods. Please check out my companion resource roundup post for links to many free books, courses, talks, tutorials, and more."
  },
  {
    "objectID": "post/causal-design-patterns/index.html#footnotes",
    "href": "post/causal-design-patterns/index.html#footnotes",
    "title": "Causal design patterns for data analysts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA concept popularized by the 1994 book Design Patterns: Elements of Reusable Object-Oriented Software. See more on Wikipedia↩︎\nRegarding opportunity costs, however, multi-armed bandits can help balance this opportunity cost by deciding when to “explore versus exploit”↩︎\nSpecifically, we should consider these endpoints. Notoriously, short-term tests can be superficially rosy due to a novelty effect↩︎\nYou may see this called “exogeneity” in the economics literature↩︎\nWhat is “similarity”? Without going into too much detail, we’re specifically concerned here about characteristics of an individual that affect both their likelihood to receive the treatment of interest and affect the outcome of interest↩︎\nOn one hand, we might think this would reduce abandoned baskets; on the other hand, it might decrease browsing↩︎\nHeuristically, you can think of it as a type of “dimensionality reduction” based on relevance instead of variance.↩︎\nSee previous footnote on “similarity”.↩︎\nNote that we are modeling whether or not they receive treatment - not if they respond to it. This may seem unintuitive since we know whether or not they received treatment, but you can intuitively think of it as modeling how similar various untreated observation are to treated ones↩︎\nThis is often called the positivity assumption↩︎\nIntuitively, you can think of this as canceling out the probability of being untreated (the actual state) and replacing it with the probability of receiving treatment (the target state) in the same way one converts 60 inches to feet by multiplying by (1 foot / 12 inches).↩︎\nI promised I wouldn’t get into the math and formulas in the post, but here’s a short numerical illustration of how this works. Suppose we have two groups A and B. A has 30 treated individuals and 20 untreated individuals, so the true propensity score is 60% (30/50). B has 10 treated individuals and 40 untreated individuals so the true propensity score is 20% (10/50). Thus, Group A accounts for 75% (30/40) of the treated group and 33% (20/60) of the untreated group. If the distinction between Group A and Group B suggests different performance on the outcome of interest, this imbalance would skew our comparison between the treated and untreated. Following our formula, the weight to apply to the untreated Group A population is 3/2 (0.6/0.4) and the weight to apply to the untreated Group B population is 1/4 (0.2/0.8). When we weight average our untreated group then, the weight on Group A in the untreated group is (20 * 3/2) / ( 20*3/2 + 40*1/4) = 30 / 40 = 3/4 = 75%. And voila! Now the distribution is the same as in the treated group. In reality, we would apply propensity scores instead to situations with multiple and continuous factors effecting the propensity score (not just “Groups A and B”), but a univariate and discrete example can make it easier to see what is happening.↩︎\nFor more here, look for references to “collider bias” in the linked resources, such as this section of Causal Inference: the Mixtape.↩︎"
  },
  {
    "objectID": "post/convo-dbt/index.html",
    "href": "post/convo-dbt/index.html",
    "title": "Embedding column-name contracts in data pipelines with dbt",
    "section": "",
    "text": "In my post Column Names as Contracts, I explore how using controlled vocabularies to name fields in a data table can create performance contracts between data producers and data consumers1. In short, I argue that field names can encode metadata2 and illustrate with R and python how these names can be used to improve data documentation, wrangling, and validation.\nHowever, demonstrations with R and python are biased towards the needs of data consumers. These popular data analysis tools provide handy, high-level interfaces for programmatically operating on columns. For example, dplyr’s select helpers make it easy to quickly manipulate all columns whose names match given patterns. For example, suppose I know that all variables beginning with IND_ are binary and non-null so I may sum them to get a count or average them to get a valid proportion. I can succinctly write:\nsummarize_at(my_data,\n             .vars = vars(starts_with(\"IND\")),\n             .funs = list(sum, mean))\nIn contrast, SQL remains a mainstay for data producers – both for use in traditional relational databases and SQL interfaces for modern large-scale data processing engines like Spark. As a very high-level and declarative language, SQL variants generally don’t offer a control flow (e.g. for loops, if statements) or programmatic control which would allow for column operations that are similar to the one shown above. That is, one might have to manually write:\nselect \n  mean(ind_a), \n  mean(ind_b), \n  sum(ind_a), \n  sum(ind_b)\nfrom my_data\nBut that is tedious, static (would not automatically adapt to the addition of more indicator variables), and error-prone (easy to miss or mistype a variable).\nAlthough SQL itself is relatively inflexible, recent tools have added a layer of “programmability” on top of SQL which affords far more flexibility and customization. In this post, I’ll demonstrate how one such tool, dbt, can help data producers consistently apply controlled vocabularies when defining, manipulating, and testing tables for analytical users.\n(In fact, after writing this post, I’ve also begun experimenting with a dbt package, dbt_dplyr that brings dplyr’s select-helper semantics to SQL.)"
  },
  {
    "objectID": "post/convo-dbt/index.html#a-brief-intro-to-dbt",
    "href": "post/convo-dbt/index.html#a-brief-intro-to-dbt",
    "title": "Embedding column-name contracts in data pipelines with dbt",
    "section": "A brief intro to dbt",
    "text": "A brief intro to dbt\ndbt (Data Build Tool) “applies the principles of software engineering to analytics code”. Specifically, it encourages data producers to write modular, atomic SQL SELECT statements in separate files (as opposed to the use of CTEs or subqueries) from which dbt derives a DAG and orchestrates the execution on your database of choice3. Further, it enables the ability to write more programmatic (with control flow) SQL templates with Jinja2 which dbt compiles to standard SQL files before executing.\nFor the purposes of implementing a controlled vocabulary, key advantages of this approach include:\n\nTemplating with if statements and for loops\nDynamic insertion of local variables4\nAutomated testing of each modular SQL unit\nCode sharing with tests and macros exportable in a package framework\n\nAdditional slick (but tangential for this post) dbt features include:\n\nThe ability to switch between dev and production schemas\nEasy toggling between views, tables, and inserts for the same base logic\nAutomatic generation of a static website documenting data lineage, metadata, and test results (the featured image above is a screenshot from the created website)\nOrchestration of SQL statements in the DAG\nHooks for rote database management tasks like adding indices and keys or granting access\n\nFor a general overview to dbt, check out the introductory tutorial on their website, the dbt101 presentation from their recent Coalesce conference5, or the interview with one of their founders on the Data Engineering Today podcast.\nIn this post, I’ll demonstrate how three features of dbt can support the use of controlled vocabulary column naming by:\n\nCreating variable names that adhere to conventions with Jinja templating\nOperating on subgroups of columns created by custom macros to enforce contracts\nValidating subgroups of columns to ensure adherence to contracts with custom tests"
  },
  {
    "objectID": "post/convo-dbt/index.html#scenario-covid-forecast-model-monitoring",
    "href": "post/convo-dbt/index.html#scenario-covid-forecast-model-monitoring",
    "title": "Embedding column-name contracts in data pipelines with dbt",
    "section": "Scenario: COVID Forecast Model Monitoring",
    "text": "Scenario: COVID Forecast Model Monitoring\nThe full example code for this project is available on GitHub.\nTo illustrate these concepts, imagine we are tasked with monitoring the performance of a county-level COVID forecasting model using data similar to datasets available through Google BigQuery public dataset program. We might want to continually log forecasted versus actual observations to ask questions like:\n\nDoes the forecast perform well?\nHow far in advance does the forecast become reliable?\nHow does performance vary across counties?\nIs the performance acceptable in particularly sensitive counties, such as those with known health professional shortages?\n\nBefore we go further, a few caveats:\n\nI am not a COVID expert nor do I pretend to be. This is not a post about how one should monitor a COVID model. This is just an understandable, hypothetical example with data in a publicly available database6\nI do not attempt to demonstrate the best way to evaluate a forecasting model or a holistic approach to model monitoring. Again, this is just a hypothetical motivation to illustrate data management techniques\nThis may seem like significant over-engineering for the problem at hand. Once again, this is just an example\n\nNow, back to work.\n\nControlled Vocabulary\nTo operationalize this analytical goal, we might start out by defining our controlled vocabulary with relevant concepts and contracts.\nUnits of measurement:\n\nID: Unique identifier of entity with no other semantic meaning\n\nNon-null\n\nN: Count\n\nInteger\nNon-null\n\nDT: Date\n\nDate format\n\nIND: Binary indicator\n\nValues of 0 or 1\nNon-null\n\nPROP: Proportion\n\nNumeric\nBounded between 0 and 1\n\nPCT: Percent\n\nNumeric\nUnlike PROP, not bounded (e.g. think “percent error”)\n\nCD: System-generated character\n\nNon-null\n\nNM: Human-readable name\n\nUnits of observation:\n\nCOUNTY: US County\nSTATE: US State\nCASE: Realized case (in practice, we would give this a more specific definition. What defines a case? What sort of confirmation is required? Is the event recorded on the date or realization or the date of reporting?)\nHOSP: Realized hospitalization (same note as above)\nDEATH: Realized death (same note as above)\n\nDescriptors:\n\nACTL: Actual observed value\nPRED: Predicted value\nHPSA: Health Professional Shortage Area (county-level measure)\n\n\n\nData Sources and Flow\nOur goal is to end up with a model_monitor table with one record per observation date and county (same as the actual table). Using the grammar above, we may define the variables we intend to include in our final table:\n\nCD_(COUNTY|STATE): Unique county/state identifier (from Census Bureau FIPS codes)\nNM_(COUNTY|STATE): Human-readable county/state names-\nDT_COUNTY: The date a county’s values are observed\nN_(CASE|HOSP|DEATH)_(ACTL|PRED)_(07|14|21|28): The actual or predicted number of cases, hospitalizations, or deaths (and, for predictions only, the value of these predictions at 7, 14, 21, and 28 days prior to the day being forecasted)\nIND_COUNTY_HPSA: Indicator of whether county is considered a shortage area\nPROP_COUNTY_HPSA: Proportion of population that is underserved in a designated shortage area\n\nWe will source these fields from four tables:\n\nactual table\n\nsourced from bigquery-public-data.covid19_jhu_csse.summary\none record per observation date x county\nfields for county code, observation date, realized number of cases and deaths\n\nprediction table\n\nsourced from bigquery-public-data.covid19_public_forecasts.county_28d_historical\none record per date prediction was made x data being predicted x county (initially)\nfields for county code, observation date, prediction date, predicted number of cases and deaths\nwe transform to one record per observation date x county with observations at different time lags represented as separate fields\n\nhpsa table\n\nsourced from bigquery-public-data.sdoh_hrsa_shortage_areas.hpsa_primary_care\n(after some wrangling on our end) one record per county for counties identified as having a shortage\nfields for the county code, date of designation, proportion of county under-served\n\nfips table7\n\nsourced from bigquery-public-data.census_utility.fips_codes_all\n(after some wrangling) one record per county for each county in the 50 US states\nfields for FIPS code (Census Bureau county identifiers), state name, county name\n\n\nFor a conceptual mental map, once all the wrangling and cleaning is done for each of the tables above, we might have psuedocode for the final table that looks something like this.\n\nselect *\nfrom \n  actual \n    left join\n  predictions using (cd_county, dt_county)\n    left join\n  hpsa using (cd_county)\n    left join\n  fips using (cd_county)\n\nBut as we’re about to see, dbt allows us to get a bit more complex and elegant."
  },
  {
    "objectID": "post/convo-dbt/index.html#variable-creation-with-jinja-templating",
    "href": "post/convo-dbt/index.html#variable-creation-with-jinja-templating",
    "title": "Embedding column-name contracts in data pipelines with dbt",
    "section": "Variable Creation with Jinja Templating",
    "text": "Variable Creation with Jinja Templating\ndbt makes it easy to create typo-free variable names that adhere to our controlled vocabulary by using the Jinja templating language.8 Jinja brings traditional control-flow elements like conditional statements and loops to make SQL more programmatic. When dbt is executed with dbt run, it first renders this Jinja to standard SQL before sending the query to the database.\nTemplates, and specifically loops, help write more concise and proof-readable SQL code when deriving a large number of variables with similar logic. For example, below we collapse the raw prediction data (which is represented as one record for each county x each day being prediction x each day a prediction was made) to one record for each county and each day being predicted with different columns containing the numeric value of each prediction of cases, hospitalizations, and deaths at lags (defined in the dbt_project.yml configuration file) of 7, 14, 21, and 28 days prior to the date being predicted.\nOrdinarily, deriving these 12 variables (3 measures x 4 lags) would pose significant room for typos in either the code or the variable names, but in this script, the Jinja template of n_case_pred_{{l}} ensures consistency.\n\n{{\n    config(\n        materialized='incremental',\n        unique_key= 'id'\n    )\n}}\n\nselect\n  county_fips_code || ' ' || forecast_date as id,\n  county_fips_code as cd_county,\n  forecast_date as dt_county,\n  {% for l in var('lags') %}\n    max(if(date_diff(prediction_date, forecast_date, day) = {{l}}, \n         round(100*new_confirmed, 0), null)) as n_case_pred_{{l}},\n    max(if(date_diff(prediction_date, forecast_date, day) = {{l}}, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_{{l}},\n    max(if(date_diff(prediction_date, forecast_date, day) = {{l}}, \n         round(100*new_deaths, 0), null)) as n_death_pred_{{l}}\n  {% if not loop.last %},{% endif %}\n  {% endfor %}\nfrom {{ source('bqpred', 'pred') }}\nwhere \n  cast(left(county_fips_code, 2) as int64) between 1 and 56 and\n  forecast_date &lt;= current_date()\n  {% if is_incremental() %}\n  and forecast_date &gt;= (\n    select dateadd(day, -7, max(dt_county)) from {{this}}\n  )\n  {% endif %}\ngroup by 1,2,3\n\nThis script renders to the following:\n\nselect\n  county_fips_code || ' ' || forecast_date as id,\n  county_fips_code as cd_county,\n  forecast_date as dt_county,\n  \n    max(if(date_diff(prediction_date, forecast_date, day) = 07, \n         round(100*new_confirmed, 0), null)) as n_case_pred_07,\n    max(if(date_diff(prediction_date, forecast_date, day) = 07, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_07,\n    max(if(date_diff(prediction_date, forecast_date, day) = 07, \n         round(100*new_deaths, 0), null)) as n_death_pred_07\n  ,\n  \n    max(if(date_diff(prediction_date, forecast_date, day) = 14, \n         round(100*new_confirmed, 0), null)) as n_case_pred_14,\n    max(if(date_diff(prediction_date, forecast_date, day) = 14, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_14,\n    max(if(date_diff(prediction_date, forecast_date, day) = 14, \n         round(100*new_deaths, 0), null)) as n_death_pred_14\n  ,\n  \n    max(if(date_diff(prediction_date, forecast_date, day) = 21, \n         round(100*new_confirmed, 0), null)) as n_case_pred_21,\n    max(if(date_diff(prediction_date, forecast_date, day) = 21, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_21,\n    max(if(date_diff(prediction_date, forecast_date, day) = 21, \n         round(100*new_deaths, 0), null)) as n_death_pred_21\n  ,\n  \n    max(if(date_diff(prediction_date, forecast_date, day) = 28, \n         round(100*new_confirmed, 0), null)) as n_case_pred_28,\n    max(if(date_diff(prediction_date, forecast_date, day) = 28, \n         round(100*hospitalized_patients, 0), null)) as n_hosp_pred_28,\n    max(if(date_diff(prediction_date, forecast_date, day) = 28, \n         round(100*new_deaths, 0), null)) as n_death_pred_28\n  \n  \nfrom `bigquery-public-data`.`covid19_public_forecasts`.`county_28d_historical`\nwhere \n  cast(left(county_fips_code, 2) as int64) between 1 and 56 and\n  forecast_date &lt;= current_date()\n  \ngroup by 1,2,3\n\nThis script and the other three that derive our base tables (actual, prediction, fips, and hpsa) can be found in the models directory of the repo. After they are individually created, they are combined into the model_monitor_staging table in the relatively uninteresting script:\n\n{{\n    config(\n        materialized='incremental',\n        unique_key='id'\n    )\n}}\n\nselect\n  actual.*,\n  prediction.* except (cd_county, dt_county, id),\n  fips.* except (cd_county),\n  hspa.* except (cd_county)\nfrom\n  {{ ref('actual') }} as actual\n  inner join\n  {{ ref('prediction') }} as prediction\n  using (dt_county, cd_county)\n  left join\n  {{ ref('fips') }} as fips\n  using (cd_county)\n  left join\n  {{ ref('hpsa') }} as hspa\n  using (cd_county)\n{% if is_incremental() %}\nwhere dt_county &gt;= (\n  select dateadd(day, -7, max(dt_county)) from {{this}}\n  )\n{% endif %}"
  },
  {
    "objectID": "post/convo-dbt/index.html#variable-manipulation-with-regex-macros",
    "href": "post/convo-dbt/index.html#variable-manipulation-with-regex-macros",
    "title": "Embedding column-name contracts in data pipelines with dbt",
    "section": "Variable Manipulation with Regex Macros",
    "text": "Variable Manipulation with Regex Macros\nOf course, it’s not enough to adhere to controlled vocabulary naming. If the actual contracts implied in those names are not upheld, the process is meaningless (or, worse, dangerous). When preparing our final table, we want to explicitly enforce as many of the vocabulary’s promises to be met as possible. This means, for example, ensuring all variables prefixed with n are really integers, dt are truly dates (and not just similarly formatted strings), and ind variables are actually never-null.\nThis time, we again use Jinja templating along with another dbt feature: custom macros. The final script in our pipeline (model_monitor) uses custom macros get_column_names() to determine all of the column names in the staging table and get_matches() to subset this list for variable names which match regular expressions corresponding to different prefixes.\nThen, we iterate over each of these lists to apply certain treatments to each set of columns such as casting cols_n and cols_dt variables to int64 and date respectively, rounding cols_prop variables to three decimal places, and coalescing cols_ind variables to be 0 if null.9\n\n{{\n    config(\n        materialized='incremental',\n        unique_key='id',\n        partition_by={\n          \"field\": \"dt_county\",\n          \"data_type\": \"date\",\n          \"granularity\": \"month\"\n        }\n    )\n}}\n\n{% set cols = get_column_names( ref('model_monitor_staging') ) %}\n{% set cols_n = get_matches(cols, '^n_.*') %}\n{% set cols_dt = get_matches(cols, '^dt_.*') %}\n{% set cols_prop = get_matches(cols, '^prop_.*') %}\n{% set cols_ind = get_matches(cols, '^ind_.*') %}\n{% set cols_oth = cols\n   | reject('in', cols_n)\n   | reject('in', cols_dt)\n   | reject('in', cols_prop)\n   | reject('in', cols_ind) %}\n\nselect\n    \n   {%- for c in cols_oth %}\n   {{c}},\n   {% endfor -%}\n   {%- for c in cols_n %} \n     cast({{c}} as int64) as {{c}}, \n   {% endfor %}\n   {%- for c in cols_dt %} \n     date({{c}}) as {{c}}, \n   {% endfor -%}\n   {%- for c in cols_prop %} \n     round({{c}}, 3) as {{c}}, \n   {% endfor -%}\n   {%- for c in cols_ind %} \n     coalesce({{c}}, 0) as {{c}} \n     {% if not loop.last %},{% endif %} \n   {% endfor -%}\n   \nfrom {{ ref('model_monitor_staging') }}\n\n{% if is_incremental() %}\nwhere dt_county &gt;= (\n  select dateadd(day, -7, max(dt_county)) from {{this}}\n  )\n{% endif %}\n\nNote how abstract this query template is. In fact, it completely avoids referencing specific variables in our table.10 If we should decide to go back and add more fields (for example, actual and predicted recoveries) into our upstream models, they will receive the correct post-processing and validation as long as they are named appropriately.\nFor a peak under the hood, here’s how those two macros work.\nFirst, get_column_names() simply queries the databases’ built in INFORMATION_SCHEMA11 to collect all column names of a given table. In the case of the model_monitor.sql script, the table provided is the staging table (model_monitor_staging) which was made in the previous step.\n\n{% macro get_column_names(relation) %}\n\n{% set relation_query %}\nselect column_name\nFROM {{relation.database}}.{{relation.schema}}.INFORMATION_SCHEMA.COLUMNS\nWHERE table_name = '{{relation.identifier}}';\n{% endset %}\n\n{% set results = run_query(relation_query) %}\n\n{% if execute %}\n{# Return the first column #}\n{% set results_list = results.columns[0].values() %}\n{% else %}\n{% set results_list = [] %}\n{% endif %}\n\n{{ return(results_list) }}\n\n{% endmacro %}\n\nNext, the get_matches() macro simply iterates through a list of characters (such as the column names obtained in the previous step) and appends only those that match our regex to the final list that is returned.12 13 (Thanks to David Sanchez on the dbt Slack community for helping me figure out how to call the re library from within Jinja.)\n\n{% macro get_matches(input_list, regex) %}\n\n{% set results_list = [] %}\n{% for l in input_list %}\n    {% if modules.re.match(regex, l, modules.re.IGNORECASE) %}\n        {{ results_list.append(l) or \"\" }}\n    {% endif %}\n{% endfor %}\n\n{{ return(results_list) }}\n\n{% endmacro %}\n\nThese macros live in the macros/ directory of the repository."
  },
  {
    "objectID": "post/convo-dbt/index.html#data-validation-with-custom-tests",
    "href": "post/convo-dbt/index.html#data-validation-with-custom-tests",
    "title": "Embedding column-name contracts in data pipelines with dbt",
    "section": "Data Validation with Custom Tests",
    "text": "Data Validation with Custom Tests\nOf course, not every contract can be made by force without risk of corrupting data. For any that we cannot enforce in their creation, we must rigorously test.\ndbt’s testing framework allows for testing any data model in the project – not just the final table. This is very useful to intercept errors as soon as they happen instead of trying to backtrack from bad output many steps later. Some tests are built-in but others can be custom written as SQL SELECT statements.\nBuilt-in tests for properties of individual columns include unique, not_null, and relationship14. These can be implemented in the schema.yml configuration file under the tests key-value pair for each relevant column, and can sometimes be shared across models with the YAML & and * (as shown below with the same basetest checks being applied to the actual and prediction data models) which allows for naming and repeating blocks (think copy-paste). However, even with a relatively small number of tests and columns, its cumbersome and easy to overlook a column.\n\nversion: 2\n\nsources:\n  - name: bqhspa\n    description: HRSA designated shortage areas\n    database: bigquery-public-data\n    schema: sdoh_hrsa_shortage_areas\n    tables:\n      - name: hpsa\n        identifier: hpsa_primary_care\n  - name: bqcensus\n    description: &gt; \n      Census Bureau mapping of FIPS codes to county and state names\n    database: bigquery-public-data\n    schema: census_utility\n    tables:\n      - name: fips\n        identifier: fips_codes_all\n  - name: bqjhu\n    description: &gt; \n      Daily COVID case and death statistics by county \n      from the Johns Hopkins University CSSE\n    database: bigquery-public-data\n    schema: covid19_jhu_csse\n    tables:\n      - name: actual\n        identifier: summary    \n  - name: bqpred\n    description: Forecasted case and death statistics\n    database: bigquery-public-data\n    schema: covid19_public_forecasts\n    tables:\n      - name: pred\n        identifier: county_28d_historical   \n\nmodels:\n  - name: actual\n    description: &gt;\n      Actual COVID cases and deaths by county\n    columns: &basetest\n      - name: id\n        tests:\n          - unique\n          - not_null\n      - name: cd_county\n        tests:\n          - relationships:\n              to: ref('fips')\n              field: cd_county\n  - name: prediction\n    description: &gt; \n      Predicted COVID cases and deaths by county\n    columns: *basetest\n  - name: hpsa\n    description: &gt;\n      Counties designated as healthcare shortage areas\n    columns:\n      - name: cd_county\n        tests:\n          - unique\n          - not_null\n          - relationships:\n              to: ref('fips')\n              field: cd_county\n  - name: fips\n    description: &gt; \n      Mapping of county and state names from FIPS codes\n    columns:\n      - name: cd_county\n        tests:\n          - unique\n          - not_null \n  - name: model_monitor_staging\n    description: &gt;\n      Staging table to combine different data sources\n  - name: model_monitor\n    description: &gt;\n      Final model monitoring table with one row per county x observed day\n    columns:\n      - name: id\n        test:\n        - unique\n        - not_null\n      - name: ind_county_hpsa\n        tests:\n        - not_null\n        - accepted_values:\n            values: [0,1]  \n            quote: false   \n      - name: prop_county_hpsa\n        tests:\n          - dbt_utils.not_null_where:\n              where: \"ind_county_hpsa = 1\"\n\nInstead, developers may also define custom tests as SQL SELECT statements which returns only records that fail the test. Like data models, tests may also use Jinja and macros. This allows us to abstract some of our data validation tests to target all variables with a specific naming convention (and, thus, performance contract) at any arbitrary point in the pipeline.\nFor example, in the model_monitor data model shown in the last section, we explicitly cast all variables that start with n to be integers. However, before we do this, we should probably ensure that these fields are truly “integer-like”; otherwise, if we are casting values that have unexpected fractional components, we are simply masking inaccurate data.\nThe following test checks whether the n variables in the model_monitor_staging table (before casting) are sufficiently “integer like”. It first retrieves all fields in this tables, next subsets all field names only to those with n prefixes, and finally uses Jinja to create a SQL script with separate WHERE conditions to check if the absolute difference between each n variable and its value after being cast to an integer is ever greater than 0.01 (which would imply a violation.)\n\n{% set cols = get_column_names( ref('model_monitor_staging') ) %}\n{% set cols_n = get_matches(cols, '^n_.*') %}\n\nselect *   \nfrom {{ ref('model_monitor_staging') }}\nwhere\n   {%- for c in cols_n %} abs({{c}} - cast({{c}} as int64)) &gt; 0.01 or \n   {% endfor %}\n   FALSE\n\nWe can apply the same trick to testing more conditions on the final table. For example, the following test checks whether every prop variable is truly bounded between 0 and 1 (by returning any times where this is not the case.)\n\n{% set cols = get_column_names( ref('model_monitor') ) %}\n{% set cols_n = get_matches(cols, '^prop_.*') %}\n\nselect *   \nfrom {{ ref('model_monitor') }}\nwhere\n   {%- for c in cols_n %} ({{c}} &lt; 0 or {{c}} &gt; 1) or \n   {% endfor %}\n   FALSE\n   \n\nFinally, we may also use tests to ensure our naming conventions are upheld. The following script once again calls the INFORMATION_SCHEMA table (as did our get_column_names() macro) to obtain a table with one record for each column name in the final table. It next uses the regexp_extract() SQL function with capturing groups to create separate columns (l1, l2, l3) for each underscore-delimited section of the naming. Finally, the WHERE conditions filter the output for any stubs that do not match the convention.\n\nwith cols as (\nselect \n  column_name, \n  regexp_extract(lower(column_name), '^[a-z]+') as l1,\n  regexp_extract(lower(column_name), '^[a-z]+_([a-z]+)') as l2,\n  regexp_extract(lower(column_name), '^[a-z]+_[a-z]+_([a-z]+)') as l3\nfrom \n  {{ ref('model_monitor').database }}.\n    {{ ref('model_monitor').schema }}.\n      INFORMATION_SCHEMA.COLUMNS\nwhere table_name = '{{ ref('model_monitor').identifier }}'\n)\n\nselect *\nfrom cols \nwhere \n  l1 not in ('id', 'cd', 'n', 'nm', 'prop', 'pct', 'dt', 'ind') or \n  l2 not in ('county', 'state', 'case', 'hosp', 'death') or \n  l3 not in ('hpsa','pred', 'actl')\n\nWe could further extend the script above and impose a hierarchy on our controlled vocabulary by adding additional conditions to the WHERE clause. For example, since the HPSA stub only makes sense as a suffix to COUNTY (e.g. there’s no such thing as a health professional shortage area case or death), we could add the additional condition or (l3 = 'hpsa' and not l2 = 'county').\nSimilarly, we can query the INFORMATION_SCHEMA to validate that each column has its implied data type.\n\nwith cols_type as (\nselect distinct \n  regexp_extract(lower(column_name), '^[a-z]+') as stub,\n  data_type\nfrom \n  {{ ref('model_monitor').database }}.\n    {{ ref('model_monitor').schema }}.\n      INFORMATION_SCHEMA.COLUMNS\nwhere table_name = '{{ ref('model_monitor').identifier }}'\n)\n\nselect * \nfrom cols_type\nwhere \n    (stub in ('id', 'cd', 'nm') and not data_type = 'STRING') or \n    (stub in ('n', 'ind') and not data_type = 'INT64') or \n    (stub in ('prop', 'pct') and not data_type = 'FLOAT64') or\n    (stub = 'dt' and not data_type = 'DATE')\n\nAs with our model_monitor.sql data model, the beauty of these tests is that they have abstracted away the column names themselves. So, they will continue to test all of the correct pieces of intent regardless of whether columns are added or removed from the table. Like macros, these could also be put into a package so that the same tests could be applied to all tables in a database.\nThe code for these tests, and a few more similar examples, are located in the tests/ directory of the repository. They can be run on the command line with the dbt test command."
  },
  {
    "objectID": "post/convo-dbt/index.html#sample-output",
    "href": "post/convo-dbt/index.html#sample-output",
    "title": "Embedding column-name contracts in data pipelines with dbt",
    "section": "Sample Output",
    "text": "Sample Output\nTo conclude, I show a few top rows of output from the final model monitoring table:\n\nselect * \nfrom dbt_emily.model_monitor\nlimit 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncd_county\ndt_county\nid\ncd_state\nnm_county\nnm_state\nn_case_actl\nn_death_actl\nn_case_pred_07\nn_hosp_pred_07\nn_death_pred_07\nn_case_pred_14\nn_hosp_pred_14\nn_death_pred_14\nn_case_pred_21\nn_hosp_pred_21\nn_death_pred_21\nn_case_pred_28\nn_hosp_pred_28\nn_death_pred_28\ndt_county_hpsa\nprop_county_hpsa\nind_county_hpsa\n\n\n\n\n01001\n2021-08-15\n2021-08-15 01:00:01\n01\nAutauga County\nAlabama\n8025\n114\n1900\n1355\n8\n1634\n1537\n9\n1426\n1561\n9\n1260\n1492\n8\nNA\nNA\n0\n\n\n01001\n2021-01-02\n2021-01-02 01:00:01\n01\nAutauga County\nAlabama\n4268\n50\n2323\n2215\n29\n1768\n1942\n26\n1434\n1625\n22\n1214\n1333\n18\nNA\nNA\n0\n\n\n01001\n2021-06-07\n2021-06-07 01:00:01\n01\nAutauga County\nAlabama\n7206\n113\n758\n514\n14\n483\n466\n13\n308\n425\n12\n196\n385\n10\nNA\nNA\n0\n\n\n01001\n2020-11-24\n2020-11-24 01:00:01\n01\nAutauga County\nAlabama\n2661\n39\n2668\n1253\n14\n2939\n1375\n15\n3200\n1510\n17\n3461\n1652\n18\nNA\nNA\n0\n\n\n01001\n2021-08-22\n2021-08-22 01:00:01\n01\nAutauga County\nAlabama\n8311\n115\n1833\n2429\n13\n1680\n2740\n14\n1561\n2871\n14\n1461\n2877\n14\nNA\nNA\n0"
  },
  {
    "objectID": "post/convo-dbt/index.html#bonus---analysis-prep-with-jinja-templates",
    "href": "post/convo-dbt/index.html#bonus---analysis-prep-with-jinja-templates",
    "title": "Embedding column-name contracts in data pipelines with dbt",
    "section": "Bonus - Analysis Prep with Jinja Templates",
    "text": "Bonus - Analysis Prep with Jinja Templates\nAlthough this post primarily focuses on uses of dbt to help data producers apply controlled vocabularies, dbt also provides an interesting framework for transitioning projects to data consumers with the use of their Analyses feature. Analyses are additional SQL script templates that are not sent to the database to produce tables or views.Instead, running dbt compile simply renders these scripts for use in analyses or BI tools.\nFor example of an “analysis”, and as another example of templating in action, the following script uses our published table to compute the percent difference between actual observations and each prediction.\n\nselect\n\n  {%- for l in var('lags') %}\n    {%- for m in ['case', 'death'] %}\n      case \n        when n_{{m}}_actl = 0 then null \n        else round( (n_{{m}}_actl - n_{{m}}_pred_{{l}}) / n_{{m}}_actl, 4)\n      end as pctdiff_{{m}}_pred_{{l}} ,  \n    {% endfor %}\n  {% endfor %}\n  \n  mm.*\n  \nfrom {{ ref('model_monitor') }} as mm\n\nIt compiles to:\n\nselect\n      case \n        when n_case_actl = 0 then null \n        else round( (n_case_actl - n_case_pred_07) / n_case_actl, 4)\n      end as pctdiff_case_pred_07 ,  \n    \n      case \n        when n_death_actl = 0 then null \n        else round( (n_death_actl - n_death_pred_07) / n_death_actl, 4)\n      end as pctdiff_death_pred_07 ,  \n    \n  \n      case \n        when n_case_actl = 0 then null \n        else round( (n_case_actl - n_case_pred_14) / n_case_actl, 4)\n      end as pctdiff_case_pred_14 ,  \n    \n      case \n        when n_death_actl = 0 then null \n        else round( (n_death_actl - n_death_pred_14) / n_death_actl, 4)\n      end as pctdiff_death_pred_14 ,  \n    \n  \n      case \n        when n_case_actl = 0 then null \n        else round( (n_case_actl - n_case_pred_21) / n_case_actl, 4)\n      end as pctdiff_case_pred_21 ,  \n    \n      case \n        when n_death_actl = 0 then null \n        else round( (n_death_actl - n_death_pred_21) / n_death_actl, 4)\n      end as pctdiff_death_pred_21 ,  \n    \n  \n      case \n        when n_case_actl = 0 then null \n        else round( (n_case_actl - n_case_pred_28) / n_case_actl, 4)\n      end as pctdiff_case_pred_28 ,  \n    \n      case \n        when n_death_actl = 0 then null \n        else round( (n_death_actl - n_death_pred_28) / n_death_actl, 4)\n      end as pctdiff_death_pred_28 ,  \n    \n  \n  \n  mm.*\n  \nfrom `sonorous-wharf-302611`.`dbt_emily`.`model_monitor` as mm"
  },
  {
    "objectID": "post/convo-dbt/index.html#footnotes",
    "href": "post/convo-dbt/index.html#footnotes",
    "title": "Embedding column-name contracts in data pipelines with dbt",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that, in many cases, the distinction between a data producer and consumer is transient and somewhat arbitrary. In many cases, the same person can be both. Here, I use the terms mostly to differentiate the goal of a specific step of work. By “data producer”, I mean someone engaged in the act of wrangling source data into a form suitable for analysis; by “data consumer”, I mean someone actually using that wrangled data for reporting, analysis, visualization, modeling, etc.↩︎\nAs one example – not a prescription for how all such vocabularies should work – one might define that all counts start with N_ and are non-negative integers; all identified start with ID_ and are non-null↩︎\ndbt has adapters for most major databases and engines including Amazon Redshift, Snowflake, and Apache Spark. An up-to-date list is available here↩︎\nSome but not all databases natively support local variables, but dbt’s approach works equally well with those that do not↩︎\nOne excellent feature of this project is the impressive amount of onboarding and documentation materials↩︎\nIn fact, many COVID models were unduly criticized because their purpose was not strictly to have the most accurate forecast possible.↩︎\nTechnically, this table should be static, so the same information could be included with dbt’s Seeds feature↩︎\nFor another exploration of using Jinja templating to generate SQL, check out this nice blog post from Stitch Fix↩︎\nOrdinarily, we would want to be careful setting null values to 0. We would not want to lie and imply the existence of missing data to nominally uphold a contract. However, this is the correct approach here. Our indicator variables in this case come from tables which only contain the 1 or “presence” values (e.g. the hpsa relation which provides ind_county_hpsa only has records for counties which are shortage areas) so this is a safe approach.↩︎\nIn fact, this could also be a macro, as I introduce before, and shipped in a package to apply across all data models in an analytical database. To make the narrative of this example easier to follow, I leave it as a standard query model.↩︎\nAn automatically created table containing metadata such as field names and types for each table in a database↩︎\nFor those interested in the nitty gritty details, we must loop here because Jinja does not allow the more compact python list comprehensions. Additionally, Jinja only allows the python append method in display brackets {{}} so the or '' is a trick to silence the output, per this site.↩︎\nNote that if you have installed dbt previously, this solution might not work for you. The python re library for regular expressions was not enabled inside dbt’s Jinja until the recent release of v0.19.0 ↩︎\nThe add-on package dbt-utils contains many more common tests such as unique_combination, not_null_where, etc.↩︎"
  },
  {
    "objectID": "post/convo-intro/index.html",
    "href": "post/convo-intro/index.html",
    "title": "Introducing the {convo} package",
    "section": "",
    "text": "Back in September, I wrote about how controlled vocabularies can help form contracts between data producers and consumers. In short, I argued that aligning on an ontology of stub names for use naming variables in a dataset can improve data documentation, validation, and wrangling with minimal overhead.\nHowever, all of these benefits assume absolute consistency in the use of the controlled vocabulary. As soon as typos creep into variable names or fields violate the supposed data validation checks that their stubs promise, these vocabularies become more of a liability than an asset by luring data consumers into complacency.\nI’m pleased to announced the experimental convo package to enable the definition and application of controlled vocabularies. In this post, I briefly describe the key features. Please see the package website for full documentation.\nlibrary(convo)"
  },
  {
    "objectID": "post/convo-intro/index.html#defining-your-vocabulary",
    "href": "post/convo-intro/index.html#defining-your-vocabulary",
    "title": "Introducing the {convo} package",
    "section": "Defining your vocabulary",
    "text": "Defining your vocabulary\nconvo uses a YAML specification to define controlled vocabularies. Stubs are defined at each level which can optionally take on additional fields such as desc (a human-readable description), valid (which specifies pointblank-style data validation checks), and rename (which specifies how variable names should change under certain computations).\n\nfilepath &lt;- system.file(\"\", \"ex-convo.yml\", package = \"convo\")\ncat(readLines(filepath), sep = \"\\n\")\n\nlevel1:\n  ID:\n    desc: Unique identifier\n    valid:\n      - col_vals_not_null()\n      - col_is_numeric()\n      - col_vals_between(1000, 99999)\n  IND:\n    desc: Binary indicator\n    valid:\n      - col_is_numeric()\n      - col_vals_in_set(c(0,1))\n    rename:\n      - when: SUM\n        then: 'N'\n      - when: AVG\n        then: P\n  AMT:\n    desc: Non-negative, summable quantity\n    valid:\n      - col_is_numeric()\n      - col_vals_gte(0)\n  VAL:\n    desc: Value\n    valid:\n      - col_is_numeric()\n    rename:\n      - when: AVG\n        then: VALAV\n  CAT:\n    desc: Category\n    valid:\n      - col_is_character()\n  CD:\n    desc: System-generated code\n    valid:\n      - col_is_character()\n  DT:\n    desc: Calendar date in YYYY-MM-DD format\n    valid:\n      - col_is_date()\nlevel2:\n  A:\n    desc: Type A\n  C:\n    desc: Type C\n  D:\n    desc: Type D\nlevel3:\n  \"\\\\d{4}\": []\n\n\nWe can read this into R and retrieve a brief summary. Note that in this case the third-level stub allows for a regular expression to be used.\n\nconvo &lt;- read_convo(filepath)\nprint(convo)\n\nLevel 1\n- ID\n- IND\n- AMT\n- VAL\n- CAT\n- CD\n- DT\nLevel 2\n- A\n- C\n- D\nLevel 3\n- \\d{4}\n\n\nAlternatively, you may define a convo as a simple R list object (as shown when bad_convo is defined in the following two examples.)"
  },
  {
    "objectID": "post/convo-intro/index.html#assessing-vocabulary-quality",
    "href": "post/convo-intro/index.html#assessing-vocabulary-quality",
    "title": "Introducing the {convo} package",
    "section": "Assessing vocabulary quality",
    "text": "Assessing vocabulary quality\nGood features of a vocabulary stubs include monosemy (having only one meaning) and unique (being the only thing to mean that thing). Functions pivot_convo() and cluster_convo() help us spot deviations from these two properties. To illustrate these functions, I’ll use a different convo than above since that one exhibits both monosemy and uniqueness already.\npivot_convo() allows us to obtain all of the level indices at which each stub appears. When the repeats_only argument is set to the default value TRUE, this function only returns stubs that exist at multiple levels, thus violating monsemy. For example, this function could help us realize that we had used the stub “CAT” to refer both to a categorical variable and an animal.\n\nbad_convo &lt;- list(c(\"IND\", \"AMT\", \"CAT\"), c(\"DOG\", \"CAT\"))\npivot_convo(bad_convo)\n\n$CAT\n[1] 1 2\n\n\nSimilarly, cluster_convo() attempts to catch errors in uniqueness by clustering stubs based on string similarity. This can highlight similar but distinct stubs, which might arise when a common word or concept is abbreviated in different ways. In the following example, “ACCOUNT”, “ACCT”, and “ACCNT” are closely clustered in the second level, which might help us realize that all three are intended to represent a customer’s account.\n\nbad_convo &lt;- list(c(\"IND\", \"IS\", \"AMT\", \"AMOUNT\", \"CAT\", \"CD\"),\n              c(\"ACCOUNT\", \"ACCT\", \"ACCNT\", \"PROSPECT\", \"CUSTOMER\"))\nclusts &lt;- cluster_convo(bad_convo)\nplot(clusts[[2]])"
  },
  {
    "objectID": "post/convo-intro/index.html#evaluating-variable-names",
    "href": "post/convo-intro/index.html#evaluating-variable-names",
    "title": "Introducing the {convo} package",
    "section": "Evaluating variable names",
    "text": "Evaluating variable names\nHaving defined a convo, we can next use it to evaluate variable names. The evaluate_convo() function accepts a convo object and a set of names in a vector. It returns any variable names that violate the controlled vocabulary, listed at the specific level in which the violation occurs.\n\ncol_names &lt;- c(\"ID_A\", \"IND_A\", \"XYZ_D\", \"AMT_B\", \"AMT_Q\", \"ID_A_1234\", \"ID_A_12\")\nevaluate_convo(convo, col_names, sep = \"_\")\n\nLevel 1\n- XYZ_D\nLevel 2\n- AMT_B\n- AMT_Q\nLevel 3\n- ID_A_12\n\n\nIf a large number of violations occur, it might be more useful to directly retrieve all of the stubs existing in variable names that are not part of the convo. To do this, we can use set operators available in the compare_convo() function to examing the unions, intersections, and set differences between our controlled vocabulary and our variable names. Doing so might inspire new candidate stubs that ought to be included in our controlled vocabulary.\n\nconvo_colnames &lt;- parse_stubs(col_names)\ncompare_convo(convo_colnames, convo, fx = \"setdiff\")\n\nLevel 1\n- XYZ\nLevel 2\n- B\n- Q\nLevel 3\n- 12\n\n\nIf desired, newly uncovered stubs can be added to the convo object in R with the add_convo_stub() function:\n\nconvo2 &lt;- add_convo_stub(convo, level = 2, stub = \"B\", desc = \"Type B\")\nconvo2 \n\nLevel 1\n- ID\n- IND\n- AMT\n- VAL\n- CAT\n- CD\n- DT\nLevel 2\n- A\n- C\n- D\n- B\nLevel 3\n- \\d{4}\n\n\nCurrently, there is not support for editing the YAML specification via R function. New stubs would need to be added manually. However, a completely new YAML file can be created with the write_convo() function. This is particularly useful if you are creating a controlled vocabulary for the first time based on an existing set of variables names. First, you may parse them with parse_stubs() to create a minimal controlled vocabulary (stubs without descriptions, validation checks, etc.) and then you may write this to a draft YAML file for further customization."
  },
  {
    "objectID": "post/convo-intro/index.html#validating-data-fields",
    "href": "post/convo-intro/index.html#validating-data-fields",
    "title": "Introducing the {convo} package",
    "section": "Validating data fields",
    "text": "Validating data fields\nThe validation checks specified with pointblank verbs in your YAML file can be used to create either a pointblank agent or a pointblank YAML file which can be used to consistently apply all of the promised data checks.\nThe pointblank YAML file may be created with the write_pb():\n\nwrite_pb(convo, c(\"IND_A\", \"AMT_B\"), filename = \"convo-validation.yml\")\n\n\ncat(readLines(\"convo-validation.yml\"), sep = \"\\n\")\n\nread_fn: ~setNames(as.data.frame(matrix(1, ncol = 2)), c(\"IND_A\", \"AMT_B\"))\ntbl_name: ~\nlabel: '[2021-02-09|05:57:05]'\nlocale: en\nsteps:\n- col_is_numeric:\n    columns: matches(\"^([A-Za-z]_){0}IND\")\n- col_vals_in_set:\n    columns: matches(\"^([A-Za-z]_){0}IND\")\n    set:\n    - 0.0\n    - 1.0\n- col_is_numeric:\n    columns: matches(\"^([A-Za-z]_){0}AMT\")\n- col_vals_gte:\n    columns: matches(\"^([A-Za-z]_){0}AMT\")\n    value: 0.0\n\n\nAlternatively, a validation agent can be created directly with create_pb_agent():\n\ndata_to_validate &lt;- data.frame(IND_A = 1, IND_B = 5, DT_B = as.Date(\"2020-01-01\"))\nagent &lt;- create_pb_agent(convo, data_to_validate)\npointblank::interrogate(agent)"
  },
  {
    "objectID": "post/convo-intro/index.html#document-fiels-and-vocabularies",
    "href": "post/convo-intro/index.html#document-fiels-and-vocabularies",
    "title": "Introducing the {convo} package",
    "section": "Document fiels and vocabularies",
    "text": "Document fiels and vocabularies\nconvo also offers preliminary support for documentation.\nBasic data dictionaries may be created with describe_names() which attempts to create definitions for fields based on a user-provided glue string and YAML-specified stub definitions.\n\nvars &lt;- c(\"AMT_A_2019\", \"IND_C_2020\")\ndesc_df &lt;- describe_names(vars, convo, desc_str = \"{level1} of {level2} in given year\")\nDT::datatable(desc_df)\n\n\nAlternatively, the entire controlled vocabulary may be put into a dictionary.\n\ndesc_df &lt;- describe_convo(convo, include_valid = TRUE, for_DT = TRUE)\nDT::datatable(desc_df, escape = FALSE)\n\n\n(The tables actually look much nicer when displayed with the full power of DT, which also allows for interactive filtering and sorting. Unfortunately, the Javascript behind DT causes a weird conflict with my static site generator weird interactions with my blog theme, so I just show screenshots here.)"
  },
  {
    "objectID": "post/convo-intro/index.html#open-issues",
    "href": "post/convo-intro/index.html#open-issues",
    "title": "Introducing the {convo} package",
    "section": "Open issues",
    "text": "Open issues\nconvo is still very experimental and there are many open questions. Currently, I’m debating many aspects of convo specification including:\n\nWhat other formats should be allowed for defining a controlled vocabulary? Should there be a spreadsheet/CSV-based format? More support for constructing the object in R directly?\nCurrently, the separators between levels are specified in the function calls.\n\nShould this be part of the convo object instead?\nShould there be support for varying selectors at different levels (e.g. this would generalize better to using convo to validate file names with / delimiting directories and subdirectories and _ or - used in parts of file names)\n\nconvo assumes prefix-based schemes with names start and “grow” from the beginning. Should suffix-based scheme be supported?\n\nOne on hand, this provides significantly more flexibility\nOn the other hand, I do strongly believe there are advantages to prefixed-based names (e.g. autocomplete, related concepts clustering when sorted) and any additional flexibility will make the initial specification increasingly gnarly for users\n\nShould specification allow truly hierarchical naming structures where allowed stubs at level n+1 vary by the stub at level n?\nShould it be possible to mark some levels are required? Currently, no levels may be “skipped” but if five levels are specified, the software permits derived names of lengths fewer or greater than 5 (so long as any existing levels 1-5 follow the format)\nWould it be useful to be able to programmatically edit the YAML file specification within R? What is the use case for this?\nCurrently, the describe function family is rather primitive. I hope to make this more aesthetic or integrate more deeply with pointblank\n\nIf you are interested, please take the package for a spin and do not hesitate to get in touch about these issues or any other ideas you have! Seeing more use cases beyond my own helps me understand which of these ideas add value versus unneccesary bloat and confusion."
  },
  {
    "objectID": "post/data-discovery-ad-hoc/index.html",
    "href": "post/data-discovery-ad-hoc/index.html",
    "title": "Crosspost: Data discovery doesn’t belong in ad hoc queries",
    "section": "",
    "text": "Credible documentation is the best tool for working with data. Short of that, labor (and computational) intensive validation may be required. Recently, I had the opportunity to expand on these ideas in a cross-post with Select Star. I explore how a “good” data analyst can interrogate a dataset with expensive queries and, more importantly, how best-in-class data products eliminate the need for this.\nMy post is reproduced below.\nIn the current environment of decreasing headcount and rising cloud costs, the benefits of data management are more objective and tangible than ever. Done well, data management can reduce the cognitive and computational costs of working with enterprise-scale data.\nAnalysts often jump into new-to-them tables to answer business questions. Without a robust data platform, this constant novelty leads analysts down one of two paths. Either they boldly gamble that they have found intuitive and relevant data or they painstakingly hypothesize and validate assumptions for each new table. The latter approach leads to more trustworthy outcomes, but it comes at the cost of human capital and computational power.\nConsider an analyst at an e-commerce company asking the question “How many invoices did we generate for fulfilled orders to Ohio in June?” while navigating unfamiliar tables. In this post, we explore prototypical queries analysts might have to run to validate a new-to-them table. Many of these are “expensive” queries requiring full table scans. Next, we’ll examine how a data discovery platform can obviate this effort.\nThe impact of this inefficiency may range from a minor papercut to a major cost sink depending on the sizes of your analyst community, historical enterprise data, and warehouse."
  },
  {
    "objectID": "post/data-discovery-ad-hoc/index.html#preventable-data-discovery-queries",
    "href": "post/data-discovery-ad-hoc/index.html#preventable-data-discovery-queries",
    "title": "Crosspost: Data discovery doesn’t belong in ad hoc queries",
    "section": "6 Preventable Data Discovery Queries",
    "text": "6 Preventable Data Discovery Queries\n\n1. What columns are in the table?\nWithout a good data catalog, analysts will first need to check what fields exist in a table. While there may be lower cost ways to do this like looking at a pre-rendered preview (ala BigQuery), using a DESCRIBE statement (ala Spark), or limiting their query to the first few rows, some analysts may default to requesting all the data.\nselect *\nfrom invoices;\n\n\n2. Is the table still live and updating?\nAfter establishing that a table has potentially useful information, analysts should next wonder if the data is still live and updating. First they might check a date field to see if the table seems “fresh”.\nselect max(order_date) \nfrom invoices;\nBut, of course, tables often have multiple date fields. For example, an e-commerce invoice table might have fields for both the date an order was placed and the date the record was last modified. So, analysts may guess-and-check a few of these fields to determine table freshness.\nselect max(updated_date) \nfrom invoices;\nAfter identifying the correct field, there’s still a question of refresh cadence. Are records added hourly? Daily? Monthly? Lacking system-level metrics and metadata on the upstream table freshness, analysts are still left in the dark. So, once again, they can check empirically by looking at the frequency of the date field.\nselect max(updated_date), count(1) as n\nfrom invoices\ngroup by 1;\n\n\n3. What is the grain of the table?\nNow that the table is confirmed to be usable, the question becomes how to use it. Specifically, to credibly query and join the table, analysts next must determine its grain. Often, they start with a guess informed by the business context and data modeling conventions, such as assuming an invoice table is unique by order_id.\nselect count(1) as n, count(distinct order_id)\nfrom invoices;\n‍However, if they learn that order_id has a different cardinality then the number of records, they must ask why. So, once again, they scan the full table to find examples of records with shared order_id values.\nselect *\nfrom invoices\nqualify count(1) over (partition by order_id) &gt; 1\norder by order_id\nlimit 10;\nEyeballing the results of this query, the analysts might notice that the same order_id value can coincide with different ship_id values, as a separate invoice is generated for each part of an order when a subset of items is shipped. With this new hypothesis, the analyst iterates on the validation of the grain.\nselect count(1) as n, count(distinct order_id, ship_id)\nfrom invoices;\n\n\n4. What values can categorical variables take?\nThe prior questions all involved table structure. Only now can an analyst finally begin to investigate the table’s content. A first step might be to understand the valid values for categorical variables. For example, if our analyst wanted to ensure only completed orders were queried, they might inspect the potential values of the order_status_id field to determine which values to include in a filter.\nselect distinct order_status_id\nfrom invoices;\nThey’ll likely repeat this process for many categorical variables of interest. Since our analyst is interested in shipments specifically to Ohio, they might also inspect the cardinality of the ship_state field to ensure they correctly format the identifier.\nselect distinct ship_state\nfrom invoices;\n\n\n5. Do numeric columns have nulls or ‘sentinel’ values to encode nulls?\nSimilarly, analysts may wish to audit other variables for null handling or sentinel values by inspecting column-level statistics.\nselect distinct ship_state\nfrom invoices;\n\n\n6. Is the data stored with partitioning or clustering keys?\nInefficient queries aren’t only a symptom of ad hoc data validation. More complex and reused logic may also be written wastefully when table metadata like partitioning and clustering keys is not available to analysts. For example, an analyst might be able to construct a reasonable query filtering either on a shipment date or an order date, but if only one of these is a partitioning or clustering key, different queries could have substantial performance differences."
  },
  {
    "objectID": "post/data-discovery-ad-hoc/index.html#understanding-your-data-without-relying-on-queries",
    "href": "post/data-discovery-ad-hoc/index.html#understanding-your-data-without-relying-on-queries",
    "title": "Crosspost: Data discovery doesn’t belong in ad hoc queries",
    "section": "Understanding Your Data Without Relying on Queries",
    "text": "Understanding Your Data Without Relying on Queries\nAnalysts absolutely should ask themselves these types of questions when working with new data. However, it should not be analysts’ job to individually answer these questions by running SQL queries. Instead, best-in-class data documentation can provide critical information through a data catalog like Select Star.\n\n1. What columns are in the table? And do we need a table?\nComprehensive search across all of an organization’s assets can help users quickly identify the right resources based on table names, field names, or data descriptions. Even better, search can incorporate observed tribal knowledge of table popularity and common querying patterns to prioritize the most relevant results. Moreover, when search also includes downstream data products like pre-built reports and dashboards, analysts might sometimes find an answer to their question exists off the shelf.\n\n\n2. Is the table still live and updating? And are its own sources current?\nData is not a static artifact so metadata should not be either. After analysts identify a candidate table, they should have access to real-time operational information like table usage, table size, refresh date, and upstream dependencies to help confirm whether the table is a reliable resource.\nIdeally, analysts can interrogate not just the freshness of a final table but also its dependencies by exploring the table’s data lineage.\n\n\n3. What is the grain of the table? And how does it relate to others?\nTable grain should be clearly documented at the table level and emphasized in the data dictionary via references to primary and foreign keys. Beyond basic documentation, entity-relationship (ER) diagrams will help analysts gain a richer mental model of grains of how they can use these primary-foreign key relationships to link tables to craft information with the desired grain and fields. Alternatively, they can glean this information from the wisdom of the crowds if they have access to how others have queried and joined the data previously.\n\n\n4. What values can categorical variables take? Do numeric columns have nulls or ‘sentinel’ values to encode nulls?\nInformation about proper expectations and handling of categorical and null values may be published as field definitions, pointed to lookup tables, implied in data tests, or illustrated in past queries. To drive consistency and offload redundant work from data producers, such field definitions can be propagated from upstream tables.\n\n\n‍5. Is the data stored with partitioning or clustering keys?\nAnalysts cannot write efficient code if they don’t know where the efficiency gains lie. Table-level documentation should clearly highlight the use of clustering or partitioning files so analysts can use the most impactful variables in filters and joins. Here, consistency of documentation is paramount; analysts may not always be incented to care about query efficiency, so if this information is hard to find or rarely available, they can be easily dissuaded from looking.\nBeyond a poor user experience, poor data discoverability creates inefficiency and added cost. Even if you don’t have large scale historical data or broad data user communities today, slow queries and tedious work still detract from data team productivity while introducing context-switching and chaos. By focusing on improving data discoverability, you can streamline workflows and enhance the overall efficiency of your data operations."
  },
  {
    "objectID": "post/data-valid-lightweight/index.html",
    "href": "post/data-valid-lightweight/index.html",
    "title": "A lightweight data validation ecosystem with R, GitHub, and Slack",
    "section": "",
    "text": "Data quality monitoring is an essential part of any data analysis or business intelligence workflow. As such, an increasing number of promising tools1 have emerged as part of the Modern Data Stack to offer better orchestration, testing, and reporting.\nAlthough I’m very excited about the developments in this space, I realize that emerging products may not be the best fit for every organization. Enterprise tools can be financial costly, and, more broadly, even free and open-source offerings bring costs in the time and risks associated with vetting tools for security, training associates, and committing to potential lock-in of building workflows around these tools. Additionally, data end-users may not always have the ability to get far enough “upstream” in the production process of their data to make these tools make sense.\n“Right-sizing” technology to the problem at hand is a critical task. A “best” solution with the most polished, professional, exciting product isn’t always the best fit for your needs. Trade-offs must be made between feature completeness and fit-for-purpose. In other words, sometimes its more important for technology to be “good enough”.2\nAdditionally, in an embarassment of riches of developer tools, sometimes the number of tools (no matter how good) we have to work with can become a burden. Personally, I like to leverage a core set of tools like Slack and GitHub for as many of their strengths when possible instead of allowing a creep of many different project-specific communication and project management tools.\nWith all of that in mind, in this post I explore a lightweight approach to a data quality workflow using a minimal set of tools that are likely already part of many team’s data stacks: R, GitHub, and Slack. This approach may be far from perfect, but I believe it provides a lot of “bang for the buck” by enabling scheduling data quality monitoring, instantaneous alerting, and workflow management at little-to-no incremental overhead.\nThe full code for this demo is available in my emilyriederer/data-validation-demo repo on GitHub."
  },
  {
    "objectID": "post/data-valid-lightweight/index.html#overall-workflow",
    "href": "post/data-valid-lightweight/index.html#overall-workflow",
    "title": "A lightweight data validation ecosystem with R, GitHub, and Slack",
    "section": "Overall Workflow",
    "text": "Overall Workflow\nTo think about right-sizing, it’s first useful to think about what features from some of the “hot” data quality monitoring products make them so appealing. Key features and tradeoffs include:\n\nAlways-on monitoring: Monitoring any time data is loaded or changed (or preferably before – such as dbt’s dev schemas and Great Expectation’s in-pipeline integration)\nReporting: Dashboards or tools to review outputs of data validation\nAlerting: Proactive logging and alerting of failures of data validation checks\nIntegration in data production process: As alluded to in the “always-on” point, the more validation is not just a passive activity but part of data production itself the better (e.g. preventing bad data from being loaded)\n\nThis approach makes some tradeoffs. It’s not orchestrated or trigger-based but can be scheduled to run on a regular basis. It’s also loosely-coupled with data production, but as we will see it can still support a better GitHub-based workflow for seeing issues through to resolution.\nThe basic idea of this workflow is to recreate as many of these strengths as possibly by maximally leveraging the strengths of existing tools. We use each for what its already good at, including:\n\nR:\n\nData validation with the pointblank package can be run directly or “outsourced” upstream to run in-place in a database (if that is where your data lives)\nValidation failures are logged as GitHub issues using the projmgr package\nA more aesthetic version of data quality reporting output is produced by running the above steps by rendering an R Markdown document to HTML\n\nGitHub: Serves as the central nervous system for execution, project management, and reporting\n\nActions: Reruns the pointblank checks on a regular basis and updates an RMarkdown-based website\nPages: Hosts the resultings RMarkdown-generated HTML for accessible data quality reporting\nIssues: Record data quality errors caught by pointblank. This provides an easy platform to assign owners, discuss issues, and track progress. With detailed labels, closed issues can also serve as a way to catalog past errors and identify trends or needed areas of improvement (where repeat failures occur)\n\nSlack: Integrates with GitHub to provide alerts on new issues on a Slack channel. Individual teams or team members can use Slack’s controls to determine how they receive notifications (e.g. email, mobile notification, etc.) for time-sensitive issues\n\nIntrigued? Next we’ll step through the technical details."
  },
  {
    "objectID": "post/data-valid-lightweight/index.html#detailed-implementation",
    "href": "post/data-valid-lightweight/index.html#detailed-implementation",
    "title": "A lightweight data validation ecosystem with R, GitHub, and Slack",
    "section": "Detailed Implementation",
    "text": "Detailed Implementation\nThis workflow revolves around a single main R Markdown document. The full version can be found on GitHub, and we will step through key components of the code and its interaction with the GitHub and Slack platforms below.\n\nValidating data (with pointblank)\n\nThe first key step is setting up validation with pointblank. Here, I show a minimal example which uses a very small toy dataset. However, pointblank can also connect to a number of remote datasources like databases and run these checks on the data in-place3 The following example just runs a few checks for data ranges, nulls, and duplicates although a wide array of premade and customizable checks are available.\nOut of the box, we can produce an aesthetic table of validation results.\n\ntbl &lt;- data.frame(x = c(1, 2, 3, 3), y = c(1, 1, 2, 2))\nact &lt;- action_levels(warn_at = 0.01, notify_at = 0.01, stop_at = NULL)\ntable_name &lt;- \"my_table\"\nagent &lt;-\n  create_agent(tbl, actions = act) %&gt;%\n  col_vals_between(vars(x), 1, 2) %&gt;%\n  col_vals_not_null(vars(x)) %&gt;%\n  rows_distinct(vars(x,y))\nres &lt;- interrogate(agent)\nres\n\n\n\nPosting results as GitHub issues (with projmgr)\n\nBeyond pointblank’s aesthetic output, we can also extract an underlying dataframe with all of the check information include which columns were included in the check, a human-readable description of the check, and the failure rate.\n\nout &lt;- \n  res$validation_set %&gt;%\n  filter(warn) %&gt;%\n  select(columns_expr, brief, column, n, n_failed, f_failed) \n\nWith this information, we can use projmgr to connect to a GitHub repository4.\n\nrepo &lt;- create_repo_ref(\"emilyriederer\", \"data-validation-demo\")\n\nThe full data wrangling steps are shown in the R Markdown, but after light data wrangling of the output dataset (out) to convert validation results into a title, description, and labels, we can post these issues to our repository.\n\nissue_numbers &lt;- pmap(issues_df, \n                      possibly(~post_issue(ref = repo, ...), otherwise = \"\")\n                      )\n\nThis creates the two issues shown above with labels for each table and variable.\nThe full R Markdown also shows how this collection of issues can also be pulled back into the resulting report to provide context on the status of each issue such as whether it has been assigned to an owner and the number of comments it has.\n\n\nRunning on GitHub Actions\nOf course, monitoring isn’t useful if it doesn’t run and detect new errors at a reasonable cadence. One way to run this report regularly is using GitHub Actions. With a simple config file, we are able to schedule a daily cron job. This job:\n\nExposes the GITHUB personal access token we need for projmgr to be able to write issues to our repository5\nSets up R and pandoc to be able to knit an R Markdown\nInstalls needed packages\nRenders the R Markdown to the file docs/index.html (Why this name? See the next step)\nPushes the results back to the repo\n\n\non:\n  schedule:\n    - cron: \"30 4 * * 3\"\n  push:\n    branches:\n      - master\n\njobs:\n  render:\n    name: Render my document\n    runs-on: macOS-latest\n    steps:\n      - name: Create and populate .Renviron file\n        run: |\n          echo GITHUB_PAT=\"$GH_PAT\" &gt;&gt; ~/.Renviron\n        shell: bash\n        env:\n          GH_PAT: ${{secrets.GH_PAT}}\n      - uses: actions/checkout@v2\n      - uses: r-lib/actions/setup-r@v1\n      - uses: r-lib/actions/setup-pandoc@v1\n      - uses: r-lib/actions/setup-tinytex@v1\n      - name: Install rmarkdown\n        run: Rscript -e 'install.packages(c(\"pointblank\", \"projmgr\", \"dplyr\", \"purrr\", \"glue\", \"rmarkdown\", \"knitr\"))'\n      - name: Render my document to all types\n        run: Rscript -e 'rmarkdown::render(\"data-valid-pipe.Rmd\", output_file = \"index.html\", output_dir = \"docs\")'\n      - name: Commit results\n        run: |\n          git add --force docs\n          git commit -m 'Rerun validation checks' || echo \"No changes to commit\"\n          git push origin || echo \"No changes to commit\"\n\n\n\nPublishing on GitHub Pages\nNow that we’ve created an HTML report from our R Markdown, we can easily host it on GitHub Pages by going to our repo’s Settings &gt; Pages and selecting as a Source the main or master branch and, specifically, the docs folder. GitHub will then provide a URL to our pages where the docs/index.html file serves as the main page.\nIn the case of my repo emilyriederer/data-validation-demo, the URL is https://emilyriederer.github.io/data-validation-demo/.\n\n\nSetting up Slack notifications\n\nOf course, no one wants to go check one more report every single day. While you can watch a repository on GitHub and receive emails about new issues, you might prefer not to fill up your own inbox or have more control over how you manage these notifications. In your team is already using Slack, GitHub + Slack integration offers a great alternative.\nYou can consider making a dedicated Slack channel for data issues and automate instant Slack notifications when any new issues are opened. First, as described in the link above, install GitHub integration for Slack. Then, the following commands (typed simply as if you are writing a message on the Slack channel) connect to your GitHub repo and unsubscribe from all notifications except for issues.\n\n/invite @github\n/github subscribe your-org/your-repo\n/github unsubscribe your-org/your-repo pulls commits releases deployments\n\nSlack messages can both allow teams to customize how and when they are notified about emerging issues across different devices. This also allows a space for “meta” discussions, such as who is equipped to handle an issue, before someone is assigned and the conversation moves to GitHub itself."
  },
  {
    "objectID": "post/data-valid-lightweight/index.html#trade-offs",
    "href": "post/data-valid-lightweight/index.html#trade-offs",
    "title": "A lightweight data validation ecosystem with R, GitHub, and Slack",
    "section": "Trade Offs",
    "text": "Trade Offs\nThere’s always a fine line between exploiting the synergies of different tools or creating an incoherent Rube Goldberg machine with rough edges and new problems. However, different solutions are best suited for different organizations, teams, and data needs. I’m very excited about all of the emerging data quality tools and platforms, and for large enterprises I suspect that may be the way to go. However, if you’re looking for scaling up your data management practices with minimal new tools, infrastructure, or tech debt, I hope this set of powerful but lightweight tools can be a first step in a good direction."
  },
  {
    "objectID": "post/data-valid-lightweight/index.html#footnotes",
    "href": "post/data-valid-lightweight/index.html#footnotes",
    "title": "A lightweight data validation ecosystem with R, GitHub, and Slack",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJust to name a few: dbt, datafold, Soda, Great Expectations, and Monte Carlo↩︎\nWith love and admiration, I borrow this phrase from the excellent paper “Good Enough Practices in Scientific Computing”: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510↩︎\nLogic is translated to SQL via dbplyr.↩︎\nAssuming you have your personal access token set per the documentation: https://emilyriederer.github.io/projmgr/articles/github-pat.html↩︎\nThis assumes that within GitHub, the PAT is defined as a secret called GH_PAT. Curiously, GitHub does not allow secrets that start with the word “GITHUB”. Who knew? Additionally, depending on the privacy level of your repository, you might not need a PAT to make issues and could skip this step.↩︎"
  },
  {
    "objectID": "post/docs-personas/index.html",
    "href": "post/docs-personas/index.html",
    "title": "Crosspost: Why You Need Data Documentation in 2024",
    "section": "",
    "text": "We’ve all worked with poorly documented dataset, and we all know it isn’t pretty. However, it’s surprisingly easy for teams to continue to fall into “documentation debt” and deprioritize this foundational work in favor of flashy new projects. These tradeoff discussions may become even more painful in 2024 as teams are continually asked to do more with less.\nRecently, I had the opportunity to articulate some of the underappreciated benefits of data documentation in a cross-post with Select Star. This builds on my prior post showing that documentation can be strategically created throughout the data development process. To make the case for taking those “raw” documentation resources to a polished final form, I return to the jobs-to-be-done framework that I’ve previously employed to talk about the value of innersource packages. In this perspective, documentation is like hiring an extra resource (or more!) to your team.\nSome of the jobs discussed are:\n\nDeveloper Advocacy and Product Evangelism for users\n\nUsers think data doesn’t exist if they can’t find it, they think data is broken if they misinterpret it\nDocumentation is both a “user interface” to make data usage easy and a bulwark against confusion and frustration\n\nProducct and Project Management for developers\n\nData intent can “drift” over time\nAs teams evolve and collaborate, this risks initial intent getting lost and poluted (after all, what really is a “customer”?)\nDocumentation serves as a contract and coach for one or more teams to force clarity and consistency of intent\n\nChief of Staff oversight for data leaders\n\nLeaders face increasing demands in data governance: navigating changing privacy regulations, fighting decaying data quality, and discerning their next strategic investments\nDocumentation is their command center to understand what data assets exists and where to better spot risks and opportunities\n\n\nIf you or your team works on data documentation, I’d love to hear what other “jobs” you have found that data documentation performs in your organization."
  },
  {
    "objectID": "post/featured.html",
    "href": "post/featured.html",
    "title": "Top Posts",
    "section": "",
    "text": "Causal design patterns for data analysts\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\ntutorial\n\n\n\nAn informal primer to causal analysis designs and data structures\n\n\n\n\n\nJan 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a team of internal R packages\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\njtbd\n\n\n\nOn the jobs-to-be-done and design principles for internal tools\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Names as Contracts\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nrstats\n\n\n\nUsing controlled dictionaries for low-touch documentation, validation, and usability of tabular data\n\n\n\n\n\nSep 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown Driven Development (RmdDD)\n\n\n\n\n\n\npkgdev\n\n\nrmarkdown\n\n\nrstats\n\n\nworkflow\n\n\n\nA workflow for refactoring one-time analyses to sustainable data products\n\n\n\n\n\nMay 4, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/grouping-data-quality-crosspost/index.html",
    "href": "post/grouping-data-quality-crosspost/index.html",
    "title": "Crosspost: Power up your data quality with grouped checks",
    "section": "",
    "text": "Photo credit to Greyson Joralemon on Unsplash\n\n\nI’ve written previously about the unreasonably effectiveness of grouping in data quality checks and implementing such checks in dbt. To publicize my merged pull request for this feature in dbt-utils package, I summarized my thinking on the topic on dbt’s Developer Blog.\nCheck out the post here."
  },
  {
    "objectID": "post/index.html",
    "href": "post/index.html",
    "title": "All Posts",
    "section": "",
    "text": "MLOrbs?: MLOps in the database with orbital and dbt\n\n\n\n\n\n\nrstats\n\n\npython\n\n\ndbt\n\n\nsql\n\n\ndata\n\n\nml\n\n\n\nPlaying with the potential, perils, and design principles of deploying ML models into the analytical database using orbital’s sklearn-to-sql translation, sqlglot, and dbt\n\n\n\n\n\nAug 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow Quarto embed fixes data science storytelling\n\n\n\n\n\n\nworkflow\n\n\nrstats\n\n\npython\n\n\nquarto\n\n\nrmarkdown\n\n\n\nLiterate programming excels at capturing our stream of conscience. Our stream of conscience does not excel at explaining the impact of our work. Notebooks enable some of data scientists’ worst tendencies in technical communication, but Quarto’s embed feature bridges the gap between reproducible research and resonate story-telling.\n\n\n\n\n\nJul 27, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nIn my orbit: hacking orbital’s ML-to-SQL for xgboost\n\n\n\n\n\n\npython\n\n\nml\n\n\n\nThe orbital package offers an interface for translating a fitted SciKitLearn pipeline to pure SQL for scaling predictions. In this tech note, I explore how this framework can (mostly) be used for xgboost models, as well, with a bit of wrangling (and a few limitations).\n\n\n\n\n\nJul 19, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPython Rgonomics - 2025 Update\n\n\n\n\n\n\nrstats\n\n\npython\n\n\n\nSwitching languages is about switching mindsets - not just syntax. New developments in python data science toolings, like polars and seaborn’s object interface, can capture the ‘feel’ that converts from R/tidyverse love while opening the door to truly pythonic workflows. (Updated from 2025 for new tools).\n\n\n\n\n\nJan 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRole-Based Access Control for Quarto sites with Netlify Identity\n\n\n\n\n\n\nquarto\n\n\nrmarkdown\n\n\nworkflow\n\n\n\nA quick tech note on Netlify’s managed authentication solution\n\n\n\n\n\nNov 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCrosspost: Data discovery doesn’t belong in ad hoc queries\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nData teams may struggle to quantify the benefits of good data documentation. But running countless ad hoc validation queries can incur both computational and cognitive cost.\n\n\n\n\n\nJul 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBase Python Rgonomic Patterns\n\n\n\n\n\n\nrstats\n\n\npython\n\n\ntutorial\n\n\n\nGetting comfortable in a new language is more than the packages you use. Syntactic sugar in base python increases the efficiency, and aesthetics of python code in ways that R users may enjoy in packages like glue and purrr. This post collects a miscellaneous grab bag of tools for wrangling, formatting (f-strings), repeating (list comprehensions), faking data, and saving objects (pickle)\n\n\n\n\n\nJan 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCrosspost: Why You Need Data Documentation in 2024\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nData documentation isn’t a box to check; it’s an active member of your team with many jobs-to-be-done. In this cross-post with Select Star, I write about how effective documentation can be your data products’ developer advocate for users, project manager for developers, and chief of staff for data leaders\n\n\n\n\n\nJan 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\npolars’ Rgonomic Patterns\n\n\n\n\n\n\nrstats\n\n\npython\n\n\ntutorial\n\n\n\nIn this follow-up post to Python Rgonomics, we deep dive into some of the advanced data wrangling functionality in python’s polars package to see how it’s powertools like column selectors and nested data structures mirror the best of dplyr and tidyr’s expressive and concise syntax\n\n\n\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCrosspost: Why you’re closer to data documentation than you think\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nWriting is thinking; documenting is planning and executing. In this cross-post with Select Star, I write about how teams can produce high-quality and maintainble documentation by smartly structuring planning and development documentation and effeciently recycling them into long-term, user-friendly docs\n\n\n\n\n\nJan 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPython Rgonomics\n\n\n\n\n\n\nrstats\n\n\npython\n\n\n\nSwitching languages is about switching mindsets - not just syntax. New developments in python data science toolings, like polars and seaborn’s object interface, can capture the ‘feel’ that converts from R/tidyverse love while opening the door to truly pythonic workflows\n\n\n\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nBig ideas from the 2023 Causal Data Science Meeting\n\n\n\n\n\n\ncausal\n\n\n\nFive highlights and links to select talks\n\n\n\n\n\nNov 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIndustry information management for causal inference\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\n\nProactive collection of data to comply or confront assumptions\n\n\n\n\n\nMay 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCrosspost: The Art of Abstraction in ETL\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nRounding out my three-part ETL series form Airbyte’s developer blog\n\n\n\n\n\nMay 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Art of Abstraction in ETL: Dodging Data Extraction Errors\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nelt\n\n\ncrosspost\n\n\n\nCross-post from guest post on Airbyte’s developer blog\n\n\n\n\n\nMar 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCrosspost: Power up your data quality with grouped checks\n\n\n\n\n\n\ndata\n\n\ndbt\n\n\ncrosspost\n\n\n\nAfter a prior post on the merits of grouped data quality checks, I demo my newly merged implementation for dbt\n\n\n\n\n\nJan 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nGoin’ to Carolina in my mind (or on my hard drive)\n\n\n\n\n\n\ndata\n\n\nsql\n\n\n\nOut-of-memory processing of North Carolina’s voter file with DuckDB and Apache Arrow\n\n\n\n\n\nSep 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nOh, I’m sure it’s probably nothing\n\n\n\n\n\n\nrstats\n\n\npython\n\n\nsql\n\n\ndata\n\n\ndata-disasters\n\n\n\nHow we do (or don’t) think about null values and why the polyglot push makes it all the more important\n\n\n\n\n\nSep 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate: grouped data quality check PR merged to dbt-utils\n\n\n\n\n\n\ndata\n\n\ndbt\n\n\n\nAfter a prior post on the merits of grouped data quality checks, I demo my newly merged implementation for dbt\n\n\n\n\n\nAug 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUsing databases with Shiny\n\n\n\n\n\n\nrstats\n\n\nshiny\n\n\ndata\n\n\n\nKey issues when adding persistent storage to a Shiny application, featuring {golem} app development and Digital Ocean serving\n\n\n\n\n\nJan 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Make R Markdown Snow\n\n\n\n\n\n\nrstats\n\n\nrmarkdown\n\n\n\nMuch like ice sculpting, applying powertools to absolutely frivolous pursuits\n\n\n\n\n\nDec 11, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMake grouping a first-class citizen in data quality checks\n\n\n\n\n\n\ndata\n\n\n\nWhich of these numbers doesn’t belong? -1, 0, 1, NA. You can’t judge data quality without data context, so our tools should enable as much context as possible.\n\n\n\n\n\nNov 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWhy machine learning hates vegetables\n\n\n\n\n\n\ndata-disasters\n\n\n\nA personal encounter with ‘intelligent’ data products gone wrong\n\n\n\n\n\nNov 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate: column-name contracts with dbtplyr\n\n\n\n\n\n\nrstats\n\n\ndbt\n\n\n\nFollowing up on ‘Embedding Column-Name Contracts… with dbt’ to demo my new dbtplyr package to further streamline the process\n\n\n\n\n\nSep 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nA lightweight data validation ecosystem with R, GitHub, and Slack\n\n\n\n\n\n\nrstats\n\n\ndata\n\n\nelt\n\n\n\nA right-sized solution to automated data monitoring, alerting, and reporting using R (pointblank, projmgr), GitHub (Actions, Pages, issues), and Slack\n\n\n\n\n\nAug 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflows for querying databases via R\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\nsql\n\n\n\nTricks for modularizing and refactoring your projects SQL/R interface. (Image source techdaily.ca)\n\n\n\n\n\nJul 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the data (error) generating processes for data validation\n\n\n\n\n\n\ndata\n\n\nelt\n\n\n\nA data consumer’s guide to validating data based on the failure modes data producer’s try to avoid\n\n\n\n\n\nMay 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nA Tale of Six States: Flexible data extraction with scraping and browser automation\n\n\n\n\n\n\ndata\n\n\nelt\n\n\npython\n\n\n\nExploring how Playwright‘s headless browser automation (and its friends) can help unite the states’ data\n\n\n\n\n\nMay 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nEmbedding column-name contracts in data pipelines with dbt\n\n\n\n\n\n\ndata\n\n\nsql\n\n\ndbt\n\n\n\ndbt supercharges SQL with Jinja templating, macros, and testing – all of which can be customized to enforce controlled vocabularies and their implied contracts on a data model\n\n\n\n\n\nFeb 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nCausal design patterns for data analysts\n\n\n\n\n\n\ncausal\n\n\ndata\n\n\ntutorial\n\n\n\nAn informal primer to causal analysis designs and data structures\n\n\n\n\n\nJan 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nResource Round-Up: Causal Inference\n\n\n\n\n\n\ncausal\n\n\nresources\n\n\n\nFree books, lectures, blogs, papers, and more for a causal inference crash course\n\n\n\n\n\nJan 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a team of internal R packages\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\njtbd\n\n\n\nOn the jobs-to-be-done and design principles for internal tools\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nGenerating SQL with {dbplyr} and sqlfluff\n\n\n\n\n\n\nrstats\n\n\ndata\n\n\nsql\n\n\ntutorial\n\n\n\nUsing the tidyverse’s expressive data wrangling vocabulary as a preprocessor for elegant SQL scripts. (Image source techdaily.ca)\n\n\n\n\n\nJan 16, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing the {convo} package\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\ndata\n\n\n\nAn R package for maintaining controlled vocabularies to encode contracts between data producers and consumers\n\n\n\n\n\nDec 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSticker-driven maintenance\n\n\n\n\n\n\nworkflow\n\n\n\nMarketing maintenance work with irrational exuberance\n\n\n\n\n\nSep 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\ncrosstalk: Dynamic filtering for R Markdown\n\n\n\n\n\n\nrstats\n\n\ntutorial\n\n\n\nAn introduction to browser-based interactivity of htmlwidgets – no Shiny server required!\n\n\n\n\n\nSep 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nColumn Names as Contracts\n\n\n\n\n\n\ndata\n\n\nworkflow\n\n\nrstats\n\n\n\nUsing controlled dictionaries for low-touch documentation, validation, and usability of tabular data\n\n\n\n\n\nSep 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nA beginner’s guide to Shiny modules\n\n\n\n\n\n\nshiny\n\n\nrstats\n\n\nworkflow\n\n\ntutorial\n\n\n\nDon’t believe the documentation! Shiny modules aren’t just for advanced users; they might just be a great entry point for development\n\n\n\n\n\nJul 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nResource Round-Up: Latent and Lasting Documentation\n\n\n\n\n\n\nresources\n\n\nrstats\n\n\n\nReadings and assorted ideas about creating and maintaining low-overhead documentation\n\n\n\n\n\nJul 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown CSS Selector Tips\n\n\n\n\n\n\nrmarkdown\n\n\nrstats\n\n\n\nA few tips and tools for finding the right selectors to style in RMarkdown\n\n\n\n\n\nJun 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nprojmgr: Managing the human dependencies of your projects\n\n\n\n\n\n\nrstats\n\n\nworkflow\n\n\n\nA walkthrough of using the projmgr package for GitHub-based project management via R\n\n\n\n\n\nMay 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown Driven Development: the Technical Appendix\n\n\n\n\n\n\npkgdev\n\n\nworkflow\n\n\nrstats\n\n\nrmarkdown\n\n\n\nA recommended tech stack for implementing RMarkdown Driven Development\n\n\n\n\n\nFeb 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nResource Round-Up: R in Industry Edition\n\n\n\n\n\n\nresources\n\n\nrstats\n\n\n\nCase studies of the impact of R use on organizational culture and collaboration\n\n\n\n\n\nAug 30, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nResource Round-Up: Reproducible Research Edition\n\n\n\n\n\n\nresources\n\n\nworkflow\n\n\n\nAn annotated bibliography of advice for getting started with reproducible research\n\n\n\n\n\nAug 30, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRtistic: A package-by-numbers repo\n\n\n\n\n\n\nrstats\n\n\npkgdev\n\n\n\nA walkthrough of a GitHub template for making your own RMarkdown and ggplot2 theme package\n\n\n\n\n\nMay 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on supporting conference speakers\n\n\n\n\n\n\nspeaking\n\n\nnotes\n\n\n\nConference planning tips to design a good speakers experience\n\n\n\n\n\nMay 7, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown Driven Development (RmdDD)\n\n\n\n\n\n\npkgdev\n\n\nrmarkdown\n\n\nrstats\n\n\nworkflow\n\n\n\nA workflow for refactoring one-time analyses to sustainable data products\n\n\n\n\n\nMay 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on preparing a tech talk\n\n\n\n\n\n\nspeaking\n\n\nnotes\n\n\n\nA proposed workflow for methodically developing a good presentations\n\n\n\n\n\nApr 20, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/ml-vegetables/index.html",
    "href": "post/ml-vegetables/index.html",
    "title": "Why machine learning hates vegetables",
    "section": "",
    "text": "The collapse of Zillow Offers last week reignited the dialogue about good and bad uses of machine learning in industry. Low-value or counterproductive applications are all too common, and they can pose a range of risks from financial loss (in the case of Zillow) to consumer harm. This discourse inspired me to document a pandemic-era personal experience with bad and potentially harmful (but not to me specifically) machine learning.\nIn this post, I will briefly overview some unexpected “automated advice” that a fitness app began to give me, explore one potential cause grounded in the use of weak evaluation metrics, and reflect on the merits (or lack their of) of the intended use case. This serves as just one more example in the ever-growing corpus of cautionary tales of mindless machine learning results when out-of-the-box algorithms are applied to real-world problems without careful consideration for how algorithm objectives and domain objectives may vary."
  },
  {
    "objectID": "post/ml-vegetables/index.html#what-happened",
    "href": "post/ml-vegetables/index.html#what-happened",
    "title": "Why machine learning hates vegetables",
    "section": "What happened?",
    "text": "What happened?\nEarly on in the pandemic, I started to engage in some personal data collection, including using a running app, a nutrition app, and a time-tracking app. This was motivated in equal if not greater part by a desire for a five minute daily respite from doom-scrolling Twitter than envisioning any specific project or usage for said data. However, there were two unexpected benefits.\nFirst, it gave me an entirely new level of appreciation for what unreliable narrators individuals (like me!) are with self-reported data. Personal manual data entry is exceedingly uninteresting. I had zero incentive to lie – no one would ever see this data but me! – but nevertheless my reporting quality quickly devolved. Specifically, working with the nutrition app was incredibly boring so I started taking easy “shortcuts” which eroded any semblance of data quality.\nAs an example that will become quite relevant shortly, it was far easier to copy the same thing from day-to-day than look up different foods. As a result, foods became metonyms for one another; any vegetable I ate was “encoded” as green beans and cauliflower because those had ended up on this list one day and I just kept copying them. Specifically, the important thing to know is that I had a few items logged every single day in the same quantities.\nSecond, and related to the first, I came to learn that the app I was using was attempting to apply some sort of “intelligence” (by which, we cannot emphasize enough, means absolutely nothing more than automation) to make recommendations based on my sketchy inputs. Most curiously, on many occasions, the app seemed convinced that I should stop eating vegetables and offered pop-up warnings of a “Negative pattern detected”:\n\n\nUpon tapping these curious warnings and venturing down the rabbit hole, it explained “We’ve noticed that on days you incorporate {{food}}, your total calories tend to be higher”.\n\nUhh… okay?"
  },
  {
    "objectID": "post/ml-vegetables/index.html#what-caused-it",
    "href": "post/ml-vegetables/index.html#what-caused-it",
    "title": "Why machine learning hates vegetables",
    "section": "What caused it?",
    "text": "What caused it?\nNaturally, I became intrigued was sort of algorithm-gone-wrong was convinced my life would be better off without vegetables. I may not ever know the exact technology underlying this feature, but there were some clues.\nI first contemplated the most naive way possible that one might identify such relationships. My first thought was somehow correlating the amount of different foods with total daily calories. This seemed like a reasonable culprit since correlation comes up by or before Chapter 3 in introductory statistics and is perhaps best known for masquerading as causation. However, my sloppy, lazy data collection helped rule out this possibility quickly because I knew that there was zero variance in the amounts or incidences of my vegetables entries.1\nThis fact of zero variance made me suspect that the actual quantities of the targeted items were not a factor in the method used. That is, it seemed likely that the approach being used was the result of some type of categorical data analysis. My next guess was that the suggestions were the result of association rules, which are statements of the form “the presence of x (item or set of items) tends to imply the presence of y (item or set of items” and learned by examining frequencies within sets of items. The ranking of “interesting” associate rules is governed by a range of heuristic evaluation metrics, so this seemed like a likely culprit.\n\nAssociation rules and interestingness metrics\nAssociation rule mining attempts to find patterns of co-occurence in a set of categorical data. That is, it looks for patterns that suggest that the incidence of some x (or set of x1, x2, etc.) suggests the incidence of some y (denoted by something like {x} =&gt; {y}). The textbook example is market basket analysis in which someone might analyze items purchase together to uncover relationships such as “people who buy pepper tend to buy salt”.\nFinding such candidate rules is mostly a matter of arithmetic, and algorithms that implement these methods are distinguished by the elegant ways they handle the computational complexity moreso than any nuanced statistics.2\nOnce candidate rules are determined, they can be evaluated for “interesting-ness” by a number of different metrics. For a rule such as {x} =&gt; {y} few examples are:\n\nSupport3: # observations with {x,y} / # of observations\nConfidence: support({x,y}) / support({x}) = # observations with {x,y} / # observations with {x}\nLift: support({x,y}) / support({x}) * support({y}) = # observations with {x,y} / expected # observations if x and y independent\n\nDepending on your background, you might notice some analogy between support, confidence, and lift to an incidence proportion, a conditional probability, and a correlation, respectively.\nAs is true of any summary metric, all of these measures lose something important while attempting to compress information. For example:\n\nSupport reflects commonly occurring things. For example, the number of days my internet traffic includes both Twitter and the Weather is incredibly high not because my usage of these two are intrinsically linked, but simply because both are daily occurrences.\nConfidence partially controls for this, but as we will see in an example, is still very sensitive to the incidence of {y}\n\n\n\nA demo\nComing back to the problem at hand (“Why does ML hate vegetables?”), let’s look at a brief example using the arules R package.\nFirst, we create a sample dataset, log. Here, x occurs in every entry (much like beans or cauliflower), hi or lo serve as a discretized “target” of interest, and z and w co-occur exclusively with hi and lo respectively. If this were a reasonable task at all (more on that in a minute), ideally z and w would be selected over x since they are more informative of hi and lo.\n\nlibrary(arules)\n\n# create fake observations -----\nlog &lt;-\n  list(\n  c(\"x\", \"a\", \"z\", \"hi\"),\n  c(\"x\", \"b\", \"z\", \"hi\"),\n  c(\"x\", \"c\", \"z\", \"hi\"),\n  c(\"x\", \"d\", \"z\", \"hi\"),\n  c(\"x\", \"e\", \"z\", \"hi\"),\n  c(\"x\", \"f\", \"w\", \"lo\"),\n  c(\"x\", \"g\", \"w\", \"lo\"),\n  c(\"x\", \"h\", \"w\", \"lo\"),\n  c(\"x\", \"i\", \"w\", \"lo\"),\n  c(\"x\", \"j\", \"w\", \"lo\")\n  )\n\nNext, we seek out rules that contain \"hi\" on the right-hand side. All rules provided have the same support, but the confidence and lift of {z} =&gt; {hi} is higher than {x} =&gt; {hi}. So that seems promising, right?\n\n# learn association rules ----\nrules &lt;- apriori(as(log, \"transactions\"),\n                 parameter  = list(support = 0.5, confidence = 0.5),\n                 appearance = list(rhs = \"hi\"),\n                 control    = list(verbose = FALSE)\n                 )\ninspect(rules)\n\n    lhs      rhs  support confidence coverage lift count\n[1] {}    =&gt; {hi} 0.5     0.5        1.0      1    5    \n[2] {z}   =&gt; {hi} 0.5     1.0        0.5      2    5    \n[3] {x}   =&gt; {hi} 0.5     0.5        1.0      1    5    \n[4] {x,z} =&gt; {hi} 0.5     1.0        0.5      2    5    \n\n\nNot quite so fast. In our initial log, hi and lo occur the same number of times. What if instead, we create an imbalanced log where the “target” of interest4 occurs 2/3 of the time, we can observe how our metrics change.5\n\nlog_imbalance &lt;- c(log, log[1:5])\nrules &lt;- apriori(as(log_imbalance, \"transactions\"),\n                 parameter  = list(support = 0.5, confidence = 0.5),\n                 appearance = list(rhs = \"hi\"),\n                 control    = list(verbose = FALSE)\n                 )\ninspect(rules)\n\n    lhs      rhs  support   confidence coverage  lift count\n[1] {}    =&gt; {hi} 0.6666667 0.6666667  1.0000000 1.0  10   \n[2] {z}   =&gt; {hi} 0.6666667 1.0000000  0.6666667 1.5  10   \n[3] {x}   =&gt; {hi} 0.6666667 0.6666667  1.0000000 1.0  10   \n[4] {x,z} =&gt; {hi} 0.6666667 1.0000000  0.6666667 1.5  10   \n\n\nSupport is still consistent across all rules. Confidence, however, is boosted from 0.5 to 0.66. Why? Because confidence is # observations where x and hi appear / # observations where x appears. Since x appears in every single entry, this is equivalent to # observations hi appears / # observations, or the incidence of \"hi\".\nSo, one conceivable reason that these rules might have surfaced on my health app might be if an approach like association rule mining were used with a hardcoded rule such as confidence &gt; 0.8 means interesting which would tend to simply pick out very common items to long as the “target” (of discretized high-calorie days) were sufficiently high."
  },
  {
    "objectID": "post/ml-vegetables/index.html#key-takeaways",
    "href": "post/ml-vegetables/index.html#key-takeaways",
    "title": "Why machine learning hates vegetables",
    "section": "Key Takeaways",
    "text": "Key Takeaways\nI don’t pretend to know what the exact methodology underlying this rule is. While a small part of me was intrigued enough to consider making a fresh account and plugging in more adversarial data, but given that this discovery was caused by a significant amount of apathy towards the application, I wasn’t about to solve the case with a greater level of precision than I had at the outset. Nevertheless, I think this is yet another interesting drop in the ocean of cautionary tales about careless use of algorithms to provide “automated intelligence”.\nWhile my discussion above focused specifically on evaluation metrics, in reality, this was but one of many traps of mindless machine learning illustrated by this problem. In all, there are any issues at play including processing data, aligning algorithms and metrics to the right domain-specific problem, and framing useful questions to answer in the first place. I’ll close with a few more thoughts on each.\nDichotimization loses information. I cannot imagine how these results would have occurred if the actual quantities or variation of my entries was taken into consideration. I have to imagine this system treats all inputs on a binary, yes-no basis. In countless cases, this starves analytical tools of meaningful information.\nWould the system work better if I wasn’t a lazy, disengaged user? No doubt. But does that matter? If systems are to be automated, they need to be resilient to bad inputs (whether that be due to adversarial intent, data quality, confused users, etc.) Plus, there’s no reason the data I was entering might not have been real even if it wasn’t in my case.\nDo out-of-the-box algorithms work? Maybe, kind-of, sometimes? But probably not? Without exception, most algorithms are absolutely phenomenal are performing the specific tasks that their implicit or explicit optimization function is designed for. However, without careful thought and tuning, it is highly questionable how often the question the algorithm is answering is the question that matters to you. Which leads us to…\nMetrics are value judgments. Any metric that is used either for optimization or evaluation of an algorithm implicitly has very, very strong opinions on what matters. The most basic example of this is simple RMSE6 which embeds a value judgment that many small errors are better than few huge errors. Simply using a canonical, textbook choice (like “confidence”) without thinking about how it applies to a specific problem is dangerous.\nWould this problem ever be destined for success? Questionable. I have no data to prove this, but my guess would be that days with unusually high caloric intake and characterized by more variety than other days – like cake on a birthday, candy on Halloween, wings on a Superbowl Sunday. It’s debatable whether rules finding common co-occurences would ever help diagnose a useful pattern.\nWould the conclusion ever by actionable? Suppose the algorithm did work, so what? First we would have to define what “work” means, but consider in our example it meant that the algorithm only proposed the rule {z} =&gt; {hi} and not {x} =&gt; {hi}. How would I possibly use that information? It doesn’t make sense that this is a causal claim. Does eating one thing cause higher caloric intake on a day? Probably not, unless that specific item is high in calories (at which point your algorithm is “select food from log order by calories desc”). How would users ever apply this information?\nShould ML be giving automated health advice at all? No. In this case, the results are laughable and clearly not useful. But what about systems where humans have less ability to intuit what is good and bad advice. If instead of vegetables and calories, what if a similar algorithm were used to suggest interactions between foods and high/low blood sugar days to a diabetic patient? Maybe this is a strawman, but in general, automating high-stakes things implies the need for high-stakes rigor and governance.\nIn closing, algorithms are rarely answering the exact question you think you are asking, and they bear close interrogation. Oh, and also, keep eating your vegetables."
  },
  {
    "objectID": "post/ml-vegetables/index.html#footnotes",
    "href": "post/ml-vegetables/index.html#footnotes",
    "title": "Why machine learning hates vegetables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is also possible that some day I forgot to add something so there is variance created by a small number of outliers. Of course, that’s an equally interesting but different story of “things algorithms need to handle.”↩︎\nSpecifically, the challenge come in needing to count frequencies across a combinatorial exploring of groupings, and different algorithms rely on different data structures and pruning methods to tame the problem.↩︎\nSupport is actually a measure of the simpler concept of a “frequent pattern” for which there is no “direction” between x and y↩︎\nI keep putting scare-quotes around “target” since we are bastardizing an unsupervised method for a supervised problem↩︎\nOr try to reason this out for yourself!↩︎\nRoot mean-squared error↩︎"
  },
  {
    "objectID": "post/orbital-xgb/index.html",
    "href": "post/orbital-xgb/index.html",
    "title": "In my orbit: hacking orbital’s ML-to-SQL for xgboost",
    "section": "",
    "text": "Posit’s recently-announced project orbital translates fitted SciKitLearn pipelines to SQL for easy prediction scoring at scale. This project has many exciting applications to deploy models for batch prediction with near-zero dependencies or custom infrastructure and have scores accessible to operatilize from their data warehouse.\nAs soon as I heard about the project, I was eager to test it out. However, much of my recent work is in pure xgboost and neither xgboost’s learning API nor the scikit-learn compatible XGBClassifier() and inherently supported by orbital. This post describes a number of workarounds to get orbital working with xgboost. This mostly works, so we’ll also cover the known limitations.\nJust want the code? The source notebook for this post is linked throughout and available to run end-to-end. I’m also stashing this and other ongoing explorations of wins, snags, and workflows with orbital in this repo.\n(Separately, I’m planning to write about my current test-drive of orbital, possible applicatons/workflows, and current pitfalls. It would have been imminently logical to write that post first. However, I saw others requesting xgboost support for orbital on LinkedIn and began a conversation, so I wanted to pull forward this post.)"
  },
  {
    "objectID": "post/orbital-xgb/index.html#step-by-step-guide",
    "href": "post/orbital-xgb/index.html#step-by-step-guide",
    "title": "In my orbit: hacking orbital’s ML-to-SQL for xgboost",
    "section": "Step-by-Step Guide",
    "text": "Step-by-Step Guide\nPreparing an xgboost model for use in orbital requires a number of transformations. Specifically, this quick “tech note” will cover:\n\nConverting a trained xgboost model into an XGBClassifier\nAdding a pre-trained classifier to a scikit-learn pipeline\nEnabling XGBClassifier translation from onnxmltools for orbital\nGetting final SQL\nValidating our results after this hop-scotch game of transformations\n\nExecuting this sucessfully requires dealing with a handful of rough edges, largely driven by onnxmltools:\n\nonnxmltools requires variables names of format f{number}\nxgboost and XGBClassifier must use base_score of 0.5 (no longer the default!)\norbital seems to complain if the pipeline does not include at least one column transformation\nXGBClassifier converter must be registered from onnxmltools\norbital’s parse function must be overwritten to hard-code the ONNX version for compatibility\nin rare cases, final predictions vary due to different floating point logic in python and SQL (&lt;0.1% of our test cases)\n\nAs we go, we’ll see how to address each of these challenges.\nFirst, we’ll grab some sample data to work with:\n\n\n# make data dataset\nX_train, y_train = make_classification(10000, random_state = 102)\nX_train = X_train.round(3)\n\n# get column names for use in pipeline\n## onnxmltools forces these to be formatted as \"f&lt;number&gt;\"\nn_cols = len(X_train[0])\nnm_cols = [f\"f{i}\" for i in range(n_cols)]\nfeat_dict = {c:orbital.types.DoubleColumnType() for c in nm_cols}\n\nSource: End-to-end notebook\n\nConverting xgboost model to an XGBClassifier pipeline\nxgboost provides two interfaces: a native learning API and a scikit-learn compatible API. The learning API is sometimes favored for performance advantages. However, since orbital can only work with scikit-learn pipelines, it’s necessary to move to a compatible API.\nThe strategy here is to fit an xgboost model (assuming that’s what you wanted to do in the first place), initialize a XGBClassifier, and set its attributes. Then, we can directly put our trained XGBClassifier into the a pipeline.\n\n\n\n\n\n\nBase Score/Magin\n\n\n\nCurrently, we must use a base_score of 0.5 for training xgboost and set the same value for the XGBClassifier. Current versions of xgboost pick smarter values by default, but currently orbital (or perhaps onnxmltools) does not know how to correctly incorporate other base margins into SQL, resulting in incorrect predictions.\nThis is probably currently the biggest weakness of this overall approach because it’s the only blocker where the fix requires fundamentally changing a modeling decision.\n\n\n\n\n# train with xgb learning api\n## keeping parameters super simple so it trains fast and easy to compare\n## important: this only works for now with base_score=0.5 \n## this is the default assumed by orbital's logic, and I haven't figured out how to convince it otherwise\ndtrain = xgb.DMatrix(X_train, y_train, feature_names = nm_cols)\nparams = {'max_depth':2, \n          'objective':'binary:logistic', \n          'base_score':0.5, \n          'seed':504}\nmodel = xgb.train(params, num_boost_round = 1, dtrain = dtrain)\npreds_xgb = model.predict(xgb.DMatrix(X_train, feature_names = nm_cols))\n\n# convert back to skl interface & rebuild needed metadata\nclf = xgb.XGBClassifier()\nclf._Booster = model\nclf.n_classes_ = 2\nclf.base_score = 0.5\npreds_skl = clf.predict_proba(X_train)[:,-1]\n\n# validate that the results are the same\nprint(f\"xgb and skl match: {np.all(np.isclose(preds_xgb, preds_skl))}\")\n\n# add to skl pipeline\nppl = Pipeline([(\"gbm\", clf)])\npreds_ppl = ppl.predict_proba(X_train)[:,-1]\n\n# validate that the results are the same\nprint(f\"xgb and ppl match: {np.all(np.isclose(preds_xgb, preds_ppl))}\")\n\nxgb and skl match: True\nxgb and ppl match: True\nxgb and skl match: True\nxgb and ppl match: True\n\n\nSource: End-to-end notebook\nWe see all three approaches produce the same predictions.\nUnfortunately, things aren’t quite that simple.\n\n\n\n\n\n\nAdd multiple pipeline steps\n\n\n\norbital seems to complain if it does not have at least one column-transformation pipeline step. I’ve yet to figure out exactly why, but in the meantime it’s no-cost to make a “fake” step that changes no columns.\n\n\nBelow, I remake the pipeline with a column transformer, ask it to apply to an empty list of variables, and request the rest (i.e. all of them) be passed through untouched.\n\n\n# now we actually make a slightly more complicated pipeline\n# orbital seems unhappy if there isn't at least one preprocessing step,\n# so we make one that processes no variables and passes through the rest\npipeline = Pipeline([\n    (\"pre\", ColumnTransformer([], remainder=\"passthrough\")),\n])\npipeline.fit(X_train)\npipeline.steps.append((\"gbm\", clf))\npreds_ppl2 = pipeline.predict_proba(X_train)[:,1]\nprint(f\"xgb and ppl2 matches: {np.all(np.isclose(preds_xgb, preds_ppl2))}\")\n\nxgb and ppl2 matches: True\nxgb and ppl2 matches: True\n\n\nSource: End-to-end notebook\nAgain, we see this “null” step does not change our predictions.\n\n\nEnabling onnxmltools for XGBClassifier conversion\norbital depends on skl2onnx which implements a smaller set of model types. onnxmltools offers many additional model converters. However, for skl2onnx to correctly find and apply these converters, they must be registered.\n\n\n# `options` copied straight from `onnxmltools` docs\nupdate_registered_converter(\n    XGBClassifier,\n    \"XGBoostXGBClassifier\",\n    calculate_linear_classifier_output_shapes,\n    convert_xgboost,\n    options={\"nocl\": [True, False], \n             \"zipmap\": [True, False, \"columns\"], \n            },\n)\n\nSource: End-to-end notebook\nHowever, there’s another nuance here. We all know the challenges of python package versioning, but both skl2onnx and onnxmltools also require coordinating on a version of the ONNX spec’s version as a universal way to represent model objects. The skl2onnx function that allows us to request a version is wrapped in orbital without the ability to pass in parameters. So, we must override that function.\n\n\n\n\n\n\nOverride orbital’s parse_pipeline()\n\n\n\nThis is required to set an ONNX version compatible between skl2onnx and onnxmltools. This is a lightweight function and not a class method, so we can just steal the code from the orbital package, modify it, and call it for ourselves. There is no need to monkeypatch.\n\n\n\n\ndef parse_pipeline_local(\n    pipeline: Pipeline, features: orbital.types.FeaturesTypes\n) -&gt; orbital.ast.ParsedPipeline:\n\n    onnx_model = skl2onnx.to_onnx(\n        pipeline,\n        initial_types=[\n            (fname, ftype._to_onnxtype())\n            for fname, ftype in features.items()\n            if not ftype.is_passthrough\n        ],\n        target_opset={\"\": 15,'ai.onnx.ml':3}\n    )\n    return orbital.ast.ParsedPipeline._from_onnx_model(onnx_model, features)\n\nSource: End-to-end notebook\n\n\nRun orbital!\nIf you’ve made it this far, you’ll be happy to know the next step is straightforward. We can now run orbital to generate the SQL representation of our model prediction logic.\n\n\n# translate into an Orbital Pipeline\norbital_pipeline = parse_pipeline_local(pipeline, features=feat_dict)\nsql_raw = orbital.export_sql(\"DATA_TABLE\", orbital_pipeline, dialect=\"duckdb\")\n\nSource: End-to-end notebook\n\n\nValidate results\nSo, after all that, did we get the right result? One way we can confirm (especially because we kept the initial xgboost model very simple) is to compare the visual of our tree with the resulting SQL.\nHere’s the tree grown by xgboost:\n\n\n\n\n\n\n\n\n\n\nSource: End-to-end notebook\nHere’s the SQL developed by orbital:\n\n\n\nSELECT\n  1 / (\n    EXP(\n      -CASE\n        WHEN \"t0\".\"f4\" &lt; -0.04800000041723251\n        THEN CASE\n          WHEN \"t0\".\"f4\" &lt; -0.8119999766349792\n          THEN -0.5087512135505676\n          ELSE -0.21405750513076782\n        END\n        ELSE CASE\n          WHEN \"t0\".\"f18\" &lt; -0.4269999861717224\n          THEN -0.3149999976158142\n          ELSE 0.5008015036582947\n        END\n      END\n    ) + 1\n  ) AS \"pred\",\n  \"f1\"\nFROM \"DATA_TABLE\" AS \"t0\"\nSELECT\n  1 / (\n    EXP(\n      -CASE\n        WHEN \"t0\".\"f4\" &lt; -0.04800000041723251\n        THEN CASE\n          WHEN \"t0\".\"f4\" &lt; -0.8119999766349792\n          THEN -0.5087512135505676\n          ELSE -0.21405750513076782\n        END\n        ELSE CASE\n          WHEN \"t0\".\"f18\" &lt; -0.4269999861717224\n          THEN -0.3149999976158142\n          ELSE 0.5008015036582947\n        END\n      END\n    ) + 1\n  ) AS \"pred\",\n  \"f1\"\nFROM \"DATA_TABLE\" AS \"t0\"\n\n\nSource: End-to-end notebook\nThese appear to match!\nHowever, if we go to use the results, we find that there are some non-equal predictions.\n\n\nDATA_TABLE = pd.DataFrame(X_train, columns = nm_cols)\ndb_preds = duckdb.sql(sql_mod).df()\npreds_orb = db_preds['pred']\nprint(f\"xgb and orb match: {np.all(np.isclose(preds_xgb, preds_orb))}\")\n\nxgb and orb match: False\nxgb and orb match: False\n\n\nSource: End-to-end notebook\n\n\n\n\n\n\nFloating point math\n\n\n\nPredictions may differ slightly across platforms due to floating point precision. Below, we see 5 of 10K predictions were non-equal. We can pull out the values of f4 and f18 for those 5 records (the only variables used in the model) and compare them to either the SQL or the flowchart. All 5 misses lie right at the cutpoint for one of the nodes.\n\n\n\n\n# isolate and size misses\nmisses = np.where(~np.isclose(preds_xgb, preds_orb))\nprint(f'Different predictions (N): {len(misses[0])}')\nprint(f'Different predictions (P): {len(misses[0]) / len(X_train):.4f}')\n\n# pull out f4 and f18; notice that all discrepancies lie exactly at the splitting points\nX_train[misses][:,[4,18]]\n\nDifferent predictions (N): 5\nDifferent predictions (P): 0.0005\nDifferent predictions (N): 5\nDifferent predictions (P): 0.0005\n\n\narray([[-0.812, -0.515],\n       [-0.812, -0.739],\n       [ 1.715, -0.427],\n       [ 0.025, -0.427],\n       [ 2.119, -0.427]])\n\n\nSource: End-to-end notebook"
  },
  {
    "objectID": "post/projmgr/index.html",
    "href": "post/projmgr/index.html",
    "title": "projmgr: Managing the human dependencies of your projects",
    "section": "",
    "text": "Many tools and packages aim to eliminate the pain and uncertainty of technical project management. For example, git, make, Docker, renv, and drake are just a few existing tools that enable collaboration, manage softwatre dependencies, and promote reproducibility. However, there is no analogous gold standard for managing the most time-consuming and unpredictable depencies in data analysis work: our fellow humans.\nThis was my initial motivation for developing the projmgr R package. Earlier this year, I was delighted to receive an invitation to speak about this package at UseR!2020, but unfortunately the conference was rightly cancelled due to COVID-19 risk.\nHowever, as the pandemic has pushed data teams towards remote work, the challenges of effective team communication and project management have become even more acute. Without the passive context sharing that comes through collocation, it’s easier for misunderstandings to arise either because stakeholders were misaligned from the outset or because their vision of a project independently drifts in different directions.\nOvercommunication is key, but this in itself taxes productivity. Communication with our collaborators and customers is often spread across Zoom, email, Slack, GitHub, and sometimes third-party project management tools like Jira or Trello. Switching between these different software tools and frames of mind knocks analysts out of their flow and detracts from getting work done.\nprojmgr offers a solution: an opinionated interface for conducting end-to-end project management with R, using GitHub issues and milestones as the backend. Key features of this package include bulk generation of GitHub issues and milestones from a YAML project plan and automated creation of status updates with user-friendly text summaries and plots.\nIn this post, I summarize my would-have-been talk to highlight features of the package that are particularly useful in the current climate."
  },
  {
    "objectID": "post/projmgr/index.html#project-management-goals",
    "href": "post/projmgr/index.html#project-management-goals",
    "title": "projmgr: Managing the human dependencies of your projects",
    "section": "Project Management Goals",
    "text": "Project Management Goals\nComparing how we manage tools sheds some light on what is needed in a technical project management tool. Good practices from software engineering include:\n\nEstablishing clear expectations both for our code (with unit and integration tests) and the environment (with Dockerfiles and dependency management)\nEnsuring new development aligns with the objective with version control and continuous integration\nBroadcasting updates through multiple mechanisms such as commit messages, NEWS.md files, semantic versioning, and (in the case of R in particular) CRAN’s reverse dependency checks\n\nIn contrast, in project management:\n\nExpectations can be ambiguous as we communicate with fluid, flexible language\nPriorities can change due to misunderstandings or private context (and without so much as a commit message to warn us!)\nProgress is not always observable without active effort from the performing team\n\nAll of these issues can be mitigated through proactive communication. However, any time spent making update decks or writing emails is time not spent on the next data product, model, or analysis. The beauty of many of our software developer tools is these features are largely automated and integrated into our workflows with minimal friction or time cost.\nIn the rest of this post, I will demonstrate how projmgr can be used to tackle these three challenges by: making a plan, assessing priorities, and sharing updates."
  },
  {
    "objectID": "post/projmgr/index.html#making-a-plan",
    "href": "post/projmgr/index.html#making-a-plan",
    "title": "projmgr: Managing the human dependencies of your projects",
    "section": "Making a Plan",
    "text": "Making a Plan\nThe first step to project management is making a plan and getting buy-in from all relevant stakeholders, including target piees of work to deliver on different timelines. projmgr allows you to easily articulate a project plan in human-readable YAML and then seamlessly bulk-upload a set of GitHub issues and milestones.\nFor example, suppose in a data analysis, we write out the following plan and save it to the file plan.yml:\n\n- title: Data cleaning and validation\n  description: &gt;\n    We will conduct data quality checks,\n    resolve issues with data quality, and\n    document this process\n  due_on: 2018-12-31T12:59:59Z\n  issue:\n    - title: Define data quality standards\n      body: List out decision rules to check data quality\n      assignees: [emilyriederer]\n      labels: [a, b, c]\n    - title: Assess data quality\n      body: Use assertthat to test decision rules on dataset\n      labels: [low]\n    - title: Resolve data quality issues\n      body: Conduct needed research to resolve any issues\n\n- title: Exploratory data analysis\n  description: &gt;\n    Create basic statistics and views to better\n    understand dataset and relationships\n  issue:\n    - title: Summary statistics\n      body: Calculate summary statistics\n    - title: Visualizations\n      body: Create univariate and bivariate plots\n\nThis format is fairly human-readable, accessible, and easy to edit. It can be shared with a team to get alignment.\nOnce a plan has been made, projmgr can read this into R as follows:\n\nlibrary(projmgr)\nplan &lt;- read_plan(\"plan.yml\")\n\nTo ensure our plan was read in correctly, we can print a quick summary:\n\nplan\n\nPlan: \n1. Data cleaning and validation (3 issues) \n2. Exploratory data analysis (2 issues) \n\n\nNext, we can create a connection to a GitHub repo and push our plan to the repo:\n\nrepo &lt;- create_repo_ref(repo_owner = 'emilyriederer', repo_name = 'experigit')\npost_plan(repo, plan)\n\nThis results in the creation of the following set of issues and milestones:\n\n\n\nScreenshot of GitHub milestones page\n\n\nFrom there, we can continue to open, close, and comment on our issues as we would with any others.\nSimilarly, with read_todo() and post_todo(), you can send additional issues to your repository that aren’t nested under milestones."
  },
  {
    "objectID": "post/projmgr/index.html#analyzing-priorities",
    "href": "post/projmgr/index.html#analyzing-priorities",
    "title": "projmgr: Managing the human dependencies of your projects",
    "section": "Analyzing Priorities",
    "text": "Analyzing Priorities\nOf course, the fun thing about plans is that plans can change. GitHub issues provide a flexible interface for projects to be dynamic and always open for commentary, feedback, requests, and new ideas. Beyond the issues created in our plan, we can use projmgr to understand organically occuring GitHub issues and to potentially incorporate them into our projects.\nprojmgr facilitates this by looking at issue metadata. Specifically, it can filter, extract, and pivot any type of custom label names. For example, let’s consider the RForwards/tasks repository.\n\nforwards &lt;- create_repo_ref(\"forwards\", \"tasks\")\nissues &lt;- get_issues(forwards) %&gt;% parse_issues()\n\nThe labels_name column contains lists of entries. One common use of labels in this repo is to denote the team responsible for completing a task, denoted by the tag \"{name}-team\".\n\nhead(issues[, c('labels_name', 'number', 'title')])\n\n               labels_name number\n1              survey-team     41\n2 help wanted, survey-team     40\n3                              35\n4                              33\n5                              32\n6                              31\n                                                                             title\n1                                                       useR! 2018 survey analysis\n2                                        Create new Community section on Data page\n3                       Guidelines on Ableist language in Talks and Presentations.\n4                                             Rainbow R : LGBT+ in the R Community\n5 Inviting R community and event organizers from Africa and Asia to the RUG slack.\n6                                                      Joint event with Trans*Code\n\n\n\nunique(unlist(issues$labels_name))\n\n[1] \"survey-team\"       \"help wanted\"       \"conferences-team\" \n[4] \"admin\"             \"on-ramps-team\"     \"branding\"         \n[7] \"teaching-team\"     \"social-media-team\" \"community-team\"   \n\n\nThe listcol_filter() function lets us filter our data only to the isues relevant to a certain list column entry. For example, the data currently contains 26 issues. But we can filter down to those only assigned to any team or a specific team.\n\nnrow(issues)\n\n[1] 26\n\n# any issue with a label ending in 'team'\nlistcol_filter(issues, \"labels_name\", matches = \"-team$\", is_regex = TRUE) %&gt;% nrow()\n\n[1] 14\n\n# any issue with a label matching 'teaching-team'\nlistcol_filter(issues, \"labels_name\", matches = \"teaching-team\") %&gt;% nrow()\n\n[1] 2\n\n\nAlternatively, we can create new columns by extracting data from our labels. For example, we can create a team column in our dataset by extracting the labels ending in \"-team\".\n\nissues[, c('labels_name', 'number', 'title')] %&gt;%\n  listcol_extract(\"labels_name\", regex = \"-team$\") %&gt;%\n  head()\n\n               labels_name number\n1              survey-team     41\n2 help wanted, survey-team     40\n3                              35\n4                              33\n5                              32\n6                              31\n                                                                             title\n1                                                       useR! 2018 survey analysis\n2                                        Create new Community section on Data page\n3                       Guidelines on Ableist language in Talks and Presentations.\n4                                             Rainbow R : LGBT+ in the R Community\n5 Inviting R community and event organizers from Africa and Asia to the RUG slack.\n6                                                      Joint event with Trans*Code\n    team\n1 survey\n2 survey\n3   &lt;NA&gt;\n4   &lt;NA&gt;\n5   &lt;NA&gt;\n6   &lt;NA&gt;\n\n\nFinally, the listcol_pivot() helped function identifies all labels matching a regex, extract all the “values” from the key-value pair, and pivots these into boolean columns. For example, the following code makes a widened dataframe with a separate column for each team. TRUE denotes the fact that that team is responsible for that issue.\n\nissues_by_team &lt;-\nissues[, c('labels_name', 'number', 'title')] %&gt;%\n  listcol_pivot(\"labels_name\", \n                regex = \"-team$\",\n                transform_fx = function(x) sub(\"-team\", \"\", x), \n                delete_orig = TRUE) \nhead(issues_by_team)\n\n  number\n1     41\n2     40\n3     35\n4     33\n5     32\n6     31\n                                                                             title\n1                                                       useR! 2018 survey analysis\n2                                        Create new Community section on Data page\n3                       Guidelines on Ableist language in Talks and Presentations.\n4                                             Rainbow R : LGBT+ in the R Community\n5 Inviting R community and event organizers from Africa and Asia to the RUG slack.\n6                                                      Joint event with Trans*Code\n  survey conferences on-ramps teaching social-media community\n1   TRUE       FALSE    FALSE    FALSE        FALSE     FALSE\n2   TRUE       FALSE    FALSE    FALSE        FALSE     FALSE\n3  FALSE       FALSE    FALSE    FALSE        FALSE     FALSE\n4  FALSE       FALSE    FALSE    FALSE        FALSE     FALSE\n5  FALSE       FALSE    FALSE    FALSE        FALSE     FALSE\n6  FALSE       FALSE    FALSE    FALSE        FALSE     FALSE\n\n\nThis has many convenient use-cases, including being able to quickly see the number falling into each category.\n\ncolSums(issues_by_team[, -c(1,2)])\n\n      survey  conferences     on-ramps     teaching social-media    community \n           4            1            3            2            2            2 \n\n\nOther examples of metadata that could be included as issue labels include subjective assessments of priority and difficulty (e.g. 'Priority: 1, 'Difficulty: Hard), the part of a project effected (e.g. stage:eda, stage:tuning), and much more. The only limit is the maintainer’s willingness to adhere to labelling conventions."
  },
  {
    "objectID": "post/projmgr/index.html#reporting-progress",
    "href": "post/projmgr/index.html#reporting-progress",
    "title": "projmgr: Managing the human dependencies of your projects",
    "section": "Reporting Progress",
    "text": "Reporting Progress\nOnce we are managing our project in GitHub, projmgr offers multiple ways to summarize progress.\nWe can retrieve issues from any repository with get_issues() (which returns the full results from the GitHub API) and parse_issues() (which structures particularly relevant columns into a dataframe). For example:\n\nissues &lt;- repo %&gt;% get_issues() %&gt;% parse_issues()\n\nWe can use the report_progress() family of functions to share an update on the status of each issue, now linked back to the GitHub reposiitory.\n\nreport_progress(issues)\n\n\n Data cleaning and validation  ( 100 % Complete - 3 / 3 )\n\n\n☑ Resolve data quality issues\n\n\n☑ Assess data quality\n\n\n☑ Define data quality standards\n\n\n Exploratory data analysis  ( 50 % Complete - 1 / 2 )\n\n\n☐ Visualizations\n\n\n☑ Summary statistics\n\n\n\n\n\nBeyond simple text summaries, we can also share a number of visualizations such as Gantt charts or taskboards. To demonstrate this, I’ll switch to an example of a more complex set of issues, hypothetically pulled from GitHub and stored to as a data.frame in a variable called pkg_issues.\nUsing HTML and CSS grid, report_taskboard() function creates an aesethetic and interactive views of your work.\n\nreport_taskboard(pkg_issues, in_progress_when = is_assigned_to(\"emilyriederer\"), hover = TRUE)\n\n  Not StartedIn ProgressDone  Extend report() support to LaTeX     Add get for events data     Add parse for events data    API mocking for testing     Validate YAML read from read fxs     Add browse and help functions    Write vignette to demo visualization      Add metadata function examples to vignettes    Add JSON support for plan / todo     \n\n\nreport_taskboard() includes many different options including linking back to each individual issue and changing the colors of each column.\nNot only is this visual summary more pleasant, but it also enables better insights for stakeholders by allowing you to flexibly share what work is being actively worked on at the moment (beyond simple open / closed status). This is controlled by the in_progress_when option and a number of helper function-factories to allow you to semantically described what constitutes progress. The above example uses the is_labeled_with() option. Other options include:\n\nls('package:projmgr')[grep(\"^is_*\", ls('package:projmgr'))]\n\n [1] \"is_assigned\"       \"is_assigned_to\"    \"is_created_before\"\n [4] \"is_due\"            \"is_due_before\"     \"is_in_a_milestone\"\n [7] \"is_in_milestone\"   \"is_labeled\"        \"is_labeled_with\"  \n[10] \"is_part_closed\""
  },
  {
    "objectID": "post/projmgr/index.html#want-to-learn-more",
    "href": "post/projmgr/index.html#want-to-learn-more",
    "title": "projmgr: Managing the human dependencies of your projects",
    "section": "Want to Learn More?",
    "text": "Want to Learn More?\nTo learn more and explore other use cases and code flows, please check out the articles on the projmgr website. This showcases other features such as using projmgr to send emails1, coordinate hackathons, track KPIs, documenting conversations, and more.\nThis project is still under active development, and I welcome any ideas of new features via GitHub."
  },
  {
    "objectID": "post/projmgr/index.html#footnotes",
    "href": "post/projmgr/index.html#footnotes",
    "title": "projmgr: Managing the human dependencies of your projects",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI have yet to test this with blastula, but I hope to update the vignette with that in mind↩︎"
  },
  {
    "objectID": "post/py-rgo-2025/index.html",
    "href": "post/py-rgo-2025/index.html",
    "title": "Python Rgonomics - 2025 Update",
    "section": "",
    "text": "Photo credit to the inimitable Allison Horst\nAbout a year ago, I wrote the original version of Python Rgonomics to help fellow former R users who were entering into the world of python. The general point of the article was that new python tooling (e.g. polars versus pandas) has evolved to a point where there are tools that remain truly performant and pythonic while still having a more similar user experience for those coming from the R world. I also discussed this at posit::conf(2025).\nIronically, the thesis held so true that it condemned my first 2024 post on the topic. 2024 saw the release of a few game-changing tools that further streamline and simplify the python workflow. This post provides an updated set of recommendations. Specifically, it highlights:\nIt is important to have a stable stack and not always jump to the next bright, shiny object; however, as I’ve watched these projects evolve throughout 2024, I feel confident to say they are not just a flash in the pan.\nuv is supported by the Charlie Marsh’s Astral, which formerly made ruff to consolidate a number of code quality tools. Astral’s commitment to open source, the careful design, and the incredible performance becnhmarks of uv speak for itself. Similarly, Positron is backed by the reliable Posit PBC (formerly RStudio) as an open source extension of Code OSS (which is also the open-source skeleton for Microsoft’s VS Code).\nThe rest of this post is reproduced in full with relevant updates so it reads end-to-end instead of referencing the changes from old to new recommendations."
  },
  {
    "objectID": "post/py-rgo-2025/index.html#now-lets-get-started",
    "href": "post/py-rgo-2025/index.html#now-lets-get-started",
    "title": "Python Rgonomics - 2025 Update",
    "section": "Now let’s get started",
    "text": "Now let’s get started\nThe “expert-novice” duality is an uncomfortable part of switching between languages like R and python. Learning a new language is easily enough done; programming 101 concepts like truth tables and control flow translate seamlessly. But ergonomics of a language do not. The tips and tricks we learn to be hyper productive in a primary language are comfortable, familiar, elegant, and effective. They just feel good. Working in a new language, developers often face a choice between forcing their favored workflows into a new tool where they may not “fit”, writing technically correct yet plodding code to get the job done, or approaching a new language as a true beginner to learn it’s “feel” from the ground up.\nFortunately, some of these higher-level paradigms have begun to bleed across languages, enriching previously isolated tribes with the and enabling developers to take their advanced skillsets with them across languages. For any R users who aim to upskill in python in 2024, recent tools and versions of old favorites have made strides in converging the R and python data science stacks. In this post, I will overview some recommended tools that are both truly pythonic while capturing the comfort and familiarity of some favorite R packages of the tidyverse variety.1"
  },
  {
    "objectID": "post/py-rgo-2025/index.html#what-this-post-is-not",
    "href": "post/py-rgo-2025/index.html#what-this-post-is-not",
    "title": "Python Rgonomics - 2025 Update",
    "section": "What this post is not",
    "text": "What this post is not\nJust to be clear:\n\nThis is not a post about why python is better than R so R users should switch all their work to python\nThis is not a post about why R is better than python so R semantics and conventions should be forced into python\nThis is not a post about why python users are better than R users so R users need coddling\nThis is not a post about why R users are better than python users and have superior tastes for their toolkit\nThis is not a post about why these python tools are the only good tools and others are bad tools\n\nIf you told me you liked the New York’s Museum of Metropolitan Art, I might say that you might also like Chicago’s Art Institute. That doesn’t mean you should only go to the museum in Chicago or that you should never go to the Louvre in Paris. That’s not how recommendations (by human or recsys) work. This is an “opinionated” post in the sense that “I like this” and not opinionated in the sense that “you must do this”."
  },
  {
    "objectID": "post/py-rgo-2025/index.html#on-picking-tools",
    "href": "post/py-rgo-2025/index.html#on-picking-tools",
    "title": "Python Rgonomics - 2025 Update",
    "section": "On picking tools",
    "text": "On picking tools\nThe tools I highlight below tend to have two competing features:\n\nThey have aspects of their workflow and ergonomics that should feel very comfortable to users of favored R tools\nThey should be independently accepted, successful, and well-maintained python projects with the true pythonic spirit\n\nThe former is important because otherwise there’s nothing tailored about these recommendations; the latter is important so users actually engage with the python language and community instead of dabbling around in its more peripheral edges. In short, these two principles exclude tools that are direct ports between languages with that as their sole or main benefit.2\nFor example, siuba and plotnine were written with the direct intent of mirroring R syntax. They have seen some success and adoption, but more niche tools come with liabilities. With smaller user-bases, they tend to lack in the pace of development, community support, prior art, StackOverflow questions, blog posts, conference talks, discussions, others to collaborate with, cache in a portfolio, etc. Instead of enjoying the ergonomics of an old language or embracing the challenge of learning a new one, ports can sometimes force developers to invest energy into a “secret third thing” of learning tools that isolate them from both communities and facing inevitable snags by themselves.\nWhen in Rome, do as the Romans do – but if you’re coming from the U.S. that doesn’t mean you can’t bring a universal adapter that can help charge your devices in European outlets."
  },
  {
    "objectID": "post/py-rgo-2025/index.html#the-stack",
    "href": "post/py-rgo-2025/index.html#the-stack",
    "title": "Python Rgonomics - 2025 Update",
    "section": "The stack",
    "text": "The stack\nWIth that preamble out of the way, below are a few recommendations for the most ergonomic tools for getting set up, conducting core data analysis, and communication results.\nTo preview these recommendations:\nSet Up\n\nInstallation: uv\nIDE:\n\nVS Code, or\nPositron\n\n\nAnalysis\n\nWrangling: polars\nVisualization: seaborn\n\nCommunication\n\nTables: Great Tables\nNotebooks: Quarto\n\nMiscellaneous\n\nEnvironment Management: uv\nCode Quality: ruff\n\n\nFor setting up\nThe first hurdle is often getting started – both in terms of installing the tools you’ll need and getting into a comfortable IDE to run them.\n\nInstallation: R keeps installation simple; there’s one way to do it so you do and it’s done3. But before python converts can print(\"hello world\"), they face a range of options (system Python, Python installer UI, Anaconda, Miniconda, etc.) each with its own kinks. These decisions are made harder in Python since projects tend to have stronger dependencies of the language, requiring one to switch between versions. Fortunately, uv now makes this task easy with many different commands for:\n\nInstalling one or more specific versions: uv python install &lt;version, constraints, etc.&gt;\nListing all available installations: uv python list\nReturning path of python executables: uv python find\nSpinning up a quick REPL with a temporary python version and packages: e.g. uv run --python 3.12 --with pandas python\n\nIntegrated Development Environment: Once R is install, R users are typically off to the races with the intuitive RStudio IDE which helps them get immediately hands-on with the REPL. With the UI divided into quadrants, users can write an R script, run it to see results in the console, conceptualize what the program “knows” with the variable explorer, and navigate files through a file explorer. Once again, python is not lacking in IDE options, but users are confronted with yet another decision point before they even get started. Pycharm, Sublime, Spyder, Eclipse, Atom, Neovim, oh my! For python, I’d recommend either VS Code or Positron, which are both extensions of Code OSS.\n\nVS Code is an industry standard tool for software development. This means it has a rich set of features for coding, debugging, navigating large projects, etc. It’s rich extension ecosystem also means that most major tools (e.g. Quarto, git, linters and stylers, etc.) have nice add-ons so, like RStudio, you can customize your platform to perform many side-tasks in plaintext or with the support of extra UI components.4\nPositron is a newer entrant from Posit PBC (formerly RStudio). It streamlines the offerings of VS Code to center the features most useful for data analysis. Positron may feel easier to go from zero-to-one. It does a great job finding and consistently using the right versions of R, python, Quarto, etc. and prioritizes many of the IDE elements that make RStudio wonderful for working with data (e.g. object preview pane). Additionally, most VS Code extensions will work in Positron; however, Positron cannot use extensions that rely on Microsoft’s PyLance meaning some realtime linting and error detection tools like ErrorLens do not work out-of-the-box. Ultimately, your comfort navigating VS Code and your mix of dev versus data work may determine which is best for you.\n\n\n\n\nFor data analysis\nAs data practitioners know, we’ll spend most of our time on cleaning and wrangling. As such, R users may struggle particularly to abandon their favorite tools for exploratory data analysis like dplyr and ggplot2. Fans of those packages often appreciate how their functional paradigm helps achieve a “flow state”. Precise syntax may differ, but new developments in the python wrangling stack provide increasingly close analogs to some of these beloved Rgonomics.\n\nData Wrangling: (See my separate post on polars)Although pandas is undoubtedly the best-known wrangling tool in the python space, I believe the growing polars project offers the best experience for a transitioning developer (along with other nice-to-have benefits like being dependency free and blazingly fast). polars may feel more natural and less error-prone to R users for may reasons:\n\nit has more internal consistent (and similar to dplyr) syntax such as select, filter, etc. and has demonstrated that the project values a clean API (e.g. recently renaming groupby to group_by)\nit does not rely on the distinction between columns and indexes which can feel unintuitive and introduces a new set of concepts to learn\nit consistently returns copies of dataframes (while pandas sometimes alters in-place) so code is more idempotent and avoids a whole class of failure modes for new users\nit enables many of the same “advanced” wrangling workflows in dplyr with high-level, semantic code like making the transformation of multiple variables at once fast with column selectors, concisely expressing window functions, and working with nested data (or what dplyr calls “list columns”) with lists and structs\nsupporting users working with increasingly large data. Similar to dplyr’s many backends (e.g. dbplyr), polars can be used to write lazily-evaluated, optimized transformations and it’s syntax is reminiscent of pyspark should users ever need to switch between\n\nVisualization: Even some of R’s critics will acknowledge the strength of ggplot2 for visualization, both in terms of it’s intuitive and incremental API and the stunning graphics it can produce. seaborn’s object interface seems to strike a great balance between offering a similar workflow (which cites ggplot2 as an inspiration) while bringing all the benefits of using an industry-standard tool\n\n\n\nFor communication\nHistorically, one possible dividing line between R and python has been framed as “python is good at working with computers, R is good at working with people”. While that is partially inspired by reductive takes that R is not production-grade, it is not without truth that the R’s academic roots spurred it to overinvest in a rich “communication stack” and translating analytical outputs into human-readable, publishable outputs. Here, too, the gaps have begun to close.\n\nTables: R has no shortage of packages for creating nicely formatted tables, an area that has historically lacked a bit in python both in workflow and outcomes. Barring strong competition from the native python space, the one “port” I am bullish about is the recently announced Great Tables package. This is a pythonic clone of R’s gt package. I’m more comfortable recommending this since it’s maintained by the same developer as the R version (to support long-term feature parity), backed by an institution not just an individual (to ensure it’s not a short-lived hobby project), and the design feels like it does a good job balancing R inspiration with pythonic practices\nComputational notebooks: Jupyter Notebooks are widely used, widely critiqued parts of many python workflows. While the ability to mix markdown and code chunks. However, notebooks can introduce new types of bugs for the uninitiated; for example, they are hard to version control and easy to execute in the wrong environment. For those coming from the world of R Markdown, plaintext computational notebooks like Quarto may provide a more transparent development experience. While Quarto allows users to write in .qmd files which are more like their .rmd predecessors, its renderer can also handle Jupyter notebooks to enable collaboration across team members with different preferences\n\n\n\nMiscellaneous\nA few more tools may be helpful and familiar to some R users who tend towards the more “developer” versus “analyst” side of the spectrum. These, in my mind, have even more varied pros and cons, but I’ll leave them for consideration:\n\nEnvironment Management: There’s a truly overwhelming number of ways5 to manage project-level dependencies in python. As a consequence, there’s also a lot of outdated advice weighing pros and cons of feature sets that have since evolved. Here again, uv takes the cake as a swiss army knife tool. It features fast installation, auto-updating of the pyproject.toml and uv.lock files (so you don’t need to remember to pip freeze), separate trakcing of primary dependencies from the fully resolved environment (so you can cleanly and completely remove dependencies-of-dependencies you no longer need), and so much more. uv can operate as a drop in replacement for pip and generate a requirements.txt if needed for compatability; however, given it’s explosive popularity and ergonomic design, I doubt you’ll have trouble convincing collaborators to adopt the same.\nDeveloper Tools: ruff (another Astral project) provides a range of linting and styling options (think R’s lintr and styler) and provides a one-stop-shop over what can be an overwhelming number of atomic tools in this space (isort, black, flake8, etc.). ruff is super fast, has a nice VS Code extension, and, while this class of tools is generally considered more advanced, I think linters can be a fantastic “coach” for new users about best practices"
  },
  {
    "objectID": "post/py-rgo-2025/index.html#footnotes",
    "href": "post/py-rgo-2025/index.html#footnotes",
    "title": "Python Rgonomics - 2025 Update",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOf course, languages have their own subcultures too. The tidyverse and data.table parts of the R world tend to favor different semantics and ergonomics. This post caters more to the former.↩︎\nThere is no doubt a place for language ports, especially for earlier stage project where no native language-specific standard exists. For example, I like Karandeep Singh’s lab work on a tidyverse for Julia and maintain my own dbtplyr package to port dplyr’s select helpers to dbt↩︎\nHowever, to highlight some advances here, Posit’s newer rig project seems to be inspired by python install management tools and offers a convenient CLI for managing multiple version of R↩︎\n If anything, the one challenge of VS Code is the sheer number of set up options, but to start out, you can see these excellent tutorials from Rami Krispin on recommended python and R configurations ↩︎\npdm, virtualenv, conda, piptools, pipenv, poetry, and that doesn’t even scratch the surface↩︎"
  },
  {
    "objectID": "post/py-rgo-polars/index.html",
    "href": "post/py-rgo-polars/index.html",
    "title": "polars’ Rgonomic Patterns",
    "section": "",
    "text": "Photo credit to Hans-Jurgen Mager on Unsplash\nA few weeks ago, I shared some recommended modern python tools and libraries that I believe have the most similar ergonomics for R (specifically tidyverse) converts. This post expands on that one with a focus on the polars library.\nAt the surface level, all data wrangling libraries have roughly the same functionality. Operations like selecting existing columns and making new ones, subsetting and ordering rows, and summarzing results is tablestakes.\nHowever, no one falls in love with a specific library because it has the best select() or filter() function the world has ever seen. It’s the ability to easily do more complex transformations that differentiate a package expert versus novice, and the learning curve for everything that happens after the “Getting Started” guide ends is what can leave experts at one tool feeling so disempowered when working with another.\nThis deeper sense of intuition and fluency – when your technical brain knows intuitively how to translate in code what your analytical brain wants to see in the data – is what I aim to capture in the term “ergonomics”. In this post, I briefly discuss the surface-level comparison but spend most of the time exploring the deeper similarities in the functionality and workflows enabled by polars and dplyr."
  },
  {
    "objectID": "post/py-rgo-polars/index.html#what-are-dplyrs-ergonomics",
    "href": "post/py-rgo-polars/index.html#what-are-dplyrs-ergonomics",
    "title": "polars’ Rgonomic Patterns",
    "section": "What are dplyr’s ergonomics?",
    "text": "What are dplyr’s ergonomics?\nTo claim polars has a similar aesthetic and user experience as dplyr, we first have to consider what the heart of dplyr‘s ergonomics actually is. The explicit design philosophy is described in the developers’ writings on tidy design principles, but I’ll blend those official intended principles with my personal definitions based on the lived user experience.\n\nConsistent:\n\nFunction names are highly consistent (e.g. snake case verbs) with dependable inputs and outputs (mostly dataframe-in dataframe-out) to increase intuition, reduce mistakes, and eliminate surprises\nMetaphors extend throughout the codebase. For example group_by() + summarize() or group_by() + mutate() do what one might expect (aggregation versus a window function) instead of requiring users to remember arbitrary command-specific syntax\nAlways returns a new dataframe versus modifying in-place so code is more idempotent1 and less error prone\n\nComposable:\n\nFunctions exist at a “sweet spot” level of abstraction. We have the right primitive building blocks that users have full control to do anything they want to do with a dataframe but almost never have to write brute-force glue code. These building blocks can be layered however one choose to conduct\nConistency of return types leads to composability since dataframe-in dataframe-out allows for chaining\n\nHuman-Centered:\n\nPackages hit a comfortable level of abstraction somewhere between fully procedural (e.g. manually looping over array indexes without a dataframe abstraction) and fully declarative (e.g. SQL-style languages where you “request” the output but aspects like the order of operations may become unclear). Writing code is essentially articulating the steps of an analysis\nThis focus on code as recipe writing leads to the creation of useful optional functions and helpers (like my favorite – column selectors)\nUser’s rarely need to break the fourth wall of this abstraction-layer (versus thinking about things like indexes in pandas)\n\n\nTLDR? We’ll say dplyr’s ergonomics allow users to express complex transformation precisely, concisely, and expressively.\nSo, with that, we will import polars and get started!\n\nimport polars as pl\n\nThis document was made with polars version 0.20.4."
  },
  {
    "objectID": "post/py-rgo-polars/index.html#basic-functionality",
    "href": "post/py-rgo-polars/index.html#basic-functionality",
    "title": "polars’ Rgonomic Patterns",
    "section": "Basic Functionality",
    "text": "Basic Functionality\nThe similarities between polars and dplyr’s top-level API are already well-explored in many posts, including those by Tidy Intelligence and Robert Mitchell.\nWe will only do the briefest of recaps of the core data wrangling functions of each and how they can be composed in order to make the latter half of the piece make sense. We will meet these functions again in-context when discussing dplyr and polar’s more advanced workflows.\n\nMain Verbs\ndplyr and polars offer the same foundational functionality for manipulating dataframes. Their APIs for these operations are substantially similar.\nFor a single dataset:\n\nColumn selection: select() -&gt; select() + drop()\nCreating or altering columns: mutate() -&gt; with_columns()\nSubsetting rows: filter() -&gt; filter()\nOrdering rows: arrange() -&gt; sort()\nComputing group-level summary metrics: group_by() + summarize() -&gt; group_by() + agg()\n\nFor multiple datasets:\n\nMerging on a shared key: *_join() -&gt; join(strategy = '*')\nStacking datasets of the same structure: union() -&gt; concat()\nTransforming rows and columns: pivot_{longer/wider}()2 -&gt; pivot()\n\n\n\nMain Verb Design\nBeyond the similarity in naming, dplyr and polars top-level functions are substantially similar in their deeper design choices which impact the ergonomics of use:\n\nReferencing columns: Both make it easy to concisely references columns in a dataset without the repeated and redundant references to said dataset (as sometimes occurs in base R or python’s pandas). dplyr does this through nonstandard evaluation wherein a dataframe’s coumns can be reference directly within a data transformation function as if they were top-level variables; in polars, column names are wrapped in pl.col()\nOptional argument: Both tend to have a wide array of nice-to-have optional arguments. For example the joining capabilities in both libraries offer optional join validation3 and column renaming by appended suffix\nConsistent dataframe-in -&gt; dataframe-out design: dplyr functions take a dataframe as their first argument and return a dataframe. Similarly, polars methods are called on a dataframe and return a dataframe which enables the chaining workflow discussed next\n\n\n\nChaining (Piping)\nThese methods are applied to polars dataframes by chaining which should feel very familiar to R dplyr fans.\nIn dplyr and the broad tidyverse, most functions take a dataframe as their first argument and return a dataframe, enabling the piping of functions. This makes it easy to write more human-readable scripts where functions are written in the order of execution and whitespace can easily be added between lines. The following lines would all be equivalent.\n\ntransformation2(transformation1(df))\n\ndf |&gt; transformation1() |&gt; transformation2()\n\ndf |&gt;\n  transformation1() |&gt;\n  transformation2()\n\nSimilarly, polars’s main transfomration methods offer a consistent dataframe-in dataframe-out design which allows method chaining. Here, we similarly can write commands in order where the . beginning the next method call serves the same purpose as R’s pipe. And for python broadly, to achieve the same affordance for whitespace, we can wrap the entire command in parentheses.\n\n(\n  df\n  .transformation1()\n  .transformation2()\n)\n\nOne could even say that polars dedication to chaining goes even deeper than dplyr. In dplyr, while core dataframe-level functions are piped, functions on specific columns are still often written in a nested fashion4\n\ndf %&gt;% mutate(z = g(f(a)))\n\nIn contrast, most of polars column-level transformation methods also make it ergonomic to keep the same literate left-to-right chaining within column-level definitions with the same benefits to readability as for dataframe-level operations.\n\ndf.with_columns(z = pl.col('a').f().g())"
  },
  {
    "objectID": "post/py-rgo-polars/index.html#advanced-wrangling",
    "href": "post/py-rgo-polars/index.html#advanced-wrangling",
    "title": "polars’ Rgonomic Patterns",
    "section": "Advanced Wrangling",
    "text": "Advanced Wrangling\nBeyond the surface-level similarity, polars supports some of the more complex ergonomics that dplyr users may enjoy. This includes functionality like:\n\nexpressive and explicit syntax for transformations across multiple rows\nconcise helpers to identify subsets of columns and apply transformations\nconsistent syntax for window functions within data transformation operations\nthe ability to work with nested data structures\n\nBelow, we will examine some of this functionality with a trusty fake dataframe.5 As with pandas, you can make a quick dataframe in polars by passing a dictionary to pl.DataFrame().\n\nimport polars as pl \n\ndf = pl.DataFrame({'a':[1,1,2,2], \n                   'b':[3,4,5,6], \n                   'c':[7,8,9,0]})\ndf.head()\n\n\n\nshape: (4, 3)\n\n\n\na\nb\nc\n\n\ni64\ni64\ni64\n\n\n\n\n1\n3\n7\n\n\n1\n4\n8\n\n\n2\n5\n9\n\n\n2\n6\n0\n\n\n\n\n\n\n\n\nExplicit API for row-wise operations\nWhile row-wise operations are relatively easy to write ad-hoc, it can still be nice semantically to have readable and stylistically consistent code for such transformations.\ndplyr’s rowwise() eliminates ambiguity in whether subsequent functions should be applied element-wise or collectively. Similiarly, polars has explicit *_horizontal() functions.\n\ndf.with_columns(\n  b_plus_c = pl.sum_horizontal(pl.col('b'), pl.col('c')) \n)\n\n\n\nshape: (4, 4)\n\n\n\na\nb\nc\nb_plus_c\n\n\ni64\ni64\ni64\ni64\n\n\n\n\n1\n3\n7\n10\n\n\n1\n4\n8\n12\n\n\n2\n5\n9\n14\n\n\n2\n6\n0\n6\n\n\n\n\n\n\n\n\n\nColumn Selectors\ndplyr’s column selectors dynamically determine a set of columns based on pattern-matching their names (e.g. starts_with(), ends_with()), data types, or other features. I’ve previously written and spoken at length about how transformative this functionality can be when paired with\npolars has a similar set of column selectors. We’ll import them and see a few examples.\n\nimport polars.selectors as cs\n\nTo make things more interesting, we’ll also turn one of our columns into a different data type.\n\ndf = df.with_columns(pl.col('a').cast(pl.Utf8))\n\n\nIn select\nWe can select columns based on name or data type and use one or more conditions.\n\ndf.select(cs.starts_with('b') | cs.string())\n\n\n\nshape: (4, 2)\n\n\n\nb\na\n\n\ni64\nstr\n\n\n\n\n3\n\"1\"\n\n\n4\n\"1\"\n\n\n5\n\"2\"\n\n\n6\n\"2\"\n\n\n\n\n\n\n\nNegative conditions also work.\n\ndf.select(~cs.string())\n\n\n\nshape: (4, 2)\n\n\n\nb\nc\n\n\ni64\ni64\n\n\n\n\n3\n7\n\n\n4\n8\n\n\n5\n9\n\n\n6\n0\n\n\n\n\n\n\n\n\n\nIn with_columns\nColumn selectors can play multiple rows in the transformation context.\nThe same transformation can be applied to multiple columns. Below, we find all integer variables, call a method to add 1 to each, and use the name.suffix() method to dynamically generate descriptive column names.\n\ndf.with_columns(\n  cs.integer().add(1).name.suffix(\"_plus1\")\n)\n\n\n\nshape: (4, 5)\n\n\n\na\nb\nc\nb_plus1\nc_plus1\n\n\nstr\ni64\ni64\ni64\ni64\n\n\n\n\n\"1\"\n3\n7\n4\n8\n\n\n\"1\"\n4\n8\n5\n9\n\n\n\"2\"\n5\n9\n6\n10\n\n\n\"2\"\n6\n0\n7\n1\n\n\n\n\n\n\n\nWe can also use selected variables within transformations, like the rowwise sums that we just saw earlier.\n\ndf.with_columns(\n  row_total = pl.sum_horizontal(cs.integer())\n)\n\n\n\nshape: (4, 4)\n\n\n\na\nb\nc\nrow_total\n\n\nstr\ni64\ni64\ni64\n\n\n\n\n\"1\"\n3\n7\n10\n\n\n\"1\"\n4\n8\n12\n\n\n\"2\"\n5\n9\n14\n\n\n\"2\"\n6\n0\n6\n\n\n\n\n\n\n\n\n\nIn group_by and agg\nColumn selectors can also be passed as inputs anywhere else that one or more columns is accepted, as with data aggregation.\n\ndf.group_by(cs.string()).agg(cs.integer().sum())\n\n\n\nshape: (2, 3)\n\n\n\na\nb\nc\n\n\nstr\ni64\ni64\n\n\n\n\n\"1\"\n7\n15\n\n\n\"2\"\n11\n9\n\n\n\n\n\n\n\n\n\n\nConsistent API for Window Functions\nWindow functions are another incredibly important tool in any data wrangling language but seem criminally undertaught in introductory analysis classes. Window functions allows you to apply aggregation logic over subgroups of data while preserving the original grain of the data (e.g. in a table of all customers and orders and a column for the max purchase account by customer).\ndplyr make window functions trivially easy with the group_by() + mutate() pattern, invoking users’ pre-existing understanding of how to write aggregation logic and how to invoke transformations that preserve a table’s grain.\npolars takes a slightly different but elegant approach. Similarly, it reuses the core with_columns() method for window functions. However, it uses a more SQL-reminiscent specification of the “window” in the column definition versus a separate grouping statement. This has the added advantage of allowing one to use multiple window functions with different windows in the same with_columns() call if you should so choose.\nA simple window function tranformation can be done by calling with_columns(), chaining an aggregation method onto a column, and following with the over() method to define the window of interest.\n\ndf.with_columns(\n  min_b = pl.col('b').min().over('a')\n)\n\n\n\nshape: (4, 4)\n\n\n\na\nb\nc\nmin_b\n\n\nstr\ni64\ni64\ni64\n\n\n\n\n\"1\"\n3\n7\n3\n\n\n\"1\"\n4\n8\n3\n\n\n\"2\"\n5\n9\n5\n\n\n\"2\"\n6\n0\n5\n\n\n\n\n\n\n\nThe chaining over and aggregate and over() can follow any other arbitrarily complex logic. Here, it follows a basic “case when”-type statement that creates an indicator for whether column b is null.\n\ndf.with_columns(\n  n_b_odd = pl.when( (pl.col('b') % 2) == 0)\n              .then(1)\n              .otherwise(0)\n              .sum().over('a')\n)\n\n\n\nshape: (4, 4)\n\n\n\na\nb\nc\nn_b_odd\n\n\nstr\ni64\ni64\ni32\n\n\n\n\n\"1\"\n3\n7\n1\n\n\n\"1\"\n4\n8\n1\n\n\n\"2\"\n5\n9\n1\n\n\n\"2\"\n6\n0\n1\n\n\n\n\n\n\n\n\n\nList Columns and Nested Frames\nWhile the R tidyverse’s raison d’etre was originally around the design of heavily normalize tidy data, modern data and analysis sometimes benefits from more complex and hierarchical data structures. Sometimes data comes to us in nested forms, like from an API6, and other times nesting data can help us perform analysis more effectively7 Recognizing these use cases, tidyr provides many capability for the creation and manipulation of nested data in which a single cell contains values from multiple columns or sometimes even a whoel miniature dataframe.\npolars makes these operations similarly easy with its own version of structs (list columns) and arrays (nested dataframes).\n\nList Columns & Nested Frames\nList columns that contain multiple key-value pairs (e.g. column-value) in a single column can be created with pl.struct() similar to R’s list().\n\ndf.with_columns(list_col = pl.struct( cs.integer() ))\n\n\n\nshape: (4, 4)\n\n\n\na\nb\nc\nlist_col\n\n\nstr\ni64\ni64\nstruct[2]\n\n\n\n\n\"1\"\n3\n7\n{3,7}\n\n\n\"1\"\n4\n8\n{4,8}\n\n\n\"2\"\n5\n9\n{5,9}\n\n\n\"2\"\n6\n0\n{6,0}\n\n\n\n\n\n\n\nThese structs can be further be aggregated across rows into miniature datasets.\n\ndf.group_by('a').agg(list_col = pl.struct( cs.integer() ) )\n\n\n\nshape: (2, 2)\n\n\n\na\nlist_col\n\n\nstr\nlist[struct[2]]\n\n\n\n\n\"2\"\n[{5,9}, {6,0}]\n\n\n\"1\"\n[{3,7}, {4,8}]\n\n\n\n\n\n\n\nIn fact, this could be a good use case for our column selectors! If we have many columns we want to keep unnested and many we want to next, it could be efficient to list out only the grouping variables and create our nested dataset by examining matches.\n\ncols = ['a']\n(df\n  .group_by(cs.by_name(cols))\n  .agg(list_col = pl.struct(~cs.by_name(cols)))\n)\n\n\n\nshape: (2, 2)\n\n\n\na\nlist_col\n\n\nstr\nlist[struct[2]]\n\n\n\n\n\"2\"\n[{5,9}, {6,0}]\n\n\n\"1\"\n[{3,7}, {4,8}]\n\n\n\n\n\n\n\n\n\nUndoing\nJust as we constructed our nested data, we can denormalize it and return it to the original state in two steps. To see this, we can assign the nested structure above as df_nested.\n\ndf_nested = df.group_by('a').agg(list_col = pl.struct( cs.integer() ) )\n\nFirst explode() returns the table to the original grain, leaving use with a single struct in each row.\n\ndf_nested.explode('list_col')\n\n\n\nshape: (4, 2)\n\n\n\na\nlist_col\n\n\nstr\nstruct[2]\n\n\n\n\n\"1\"\n{3,7}\n\n\n\"1\"\n{4,8}\n\n\n\"2\"\n{5,9}\n\n\n\"2\"\n{6,0}\n\n\n\n\n\n\n\nThen, unnest() unpacks each struct and turns each element back into a column.\n\ndf_nested.explode('list_col').unnest('list_col')\n\n\n\nshape: (4, 3)\n\n\n\na\nb\nc\n\n\nstr\ni64\ni64\n\n\n\n\n\"1\"\n3\n7\n\n\n\"1\"\n4\n8\n\n\n\"2\"\n5\n9\n\n\n\"2\"\n6\n0"
  },
  {
    "objectID": "post/py-rgo-polars/index.html#footnotes",
    "href": "post/py-rgo-polars/index.html#footnotes",
    "title": "polars’ Rgonomic Patterns",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMeaning you can’t get the same result twice because if you rerun the same code the input has already been modified↩︎\nOf the tidyverse funtions mentioned so far, this is the only one found in tidyr not dplyr↩︎\nThat is, validating an assumption that joins should have been one-to-one, one-to-many, etc.↩︎\nHowever, this is more by convention. There’s not a strong reason why they would strictly need to be.↩︎\nI recently ran a Twitter poll on whether people prefer real, canonical, or fake datasets for learning and teaching. Fake data wasn’t the winner, but a strategy I find personally fun and useful as the unit-test analog for learning.↩︎\nFor example, an API payload for a LinkedIn user might have nested data structures representing professional experience and educational experience↩︎\nFor example, training a model on different data subsets.↩︎"
  },
  {
    "objectID": "post/quarto-comms/index.html",
    "href": "post/quarto-comms/index.html",
    "title": "How Quarto embed fixes data science storytelling",
    "section": "",
    "text": "Data science stakeholder communication is hard. The typical explanation of this is to parody data scientists as “too technical” to communicate with their audiences. But I’ve always found it unsatisfying to believe that “being technical” makes it too challenging to connect with the 0.1% of the population so similar to ourselves that we all happen to work in the same organization.\nInstead, I believe communication is rarely taught intentionally and, worse, is modeled poorly by educational communication which has different goals. This leads to an “explain all the things” mindset that is enabled by literate programming tools like notebooks. It’s said that “writing is thinking”, and literate programming excels at capturing our stream of conscience. However, our stream of conscience does not excel at succinct retrospective explanations of our work’s impact.\nData scientist do not have a communication problem. They have a problem in ordering their story for impact and engagement, driven by their background, context, and tools.\nFortunately, Quarto’s new embed feature bridges the gap between reproducible research and resonate story-telling. This simple feature allows us to cross-reference chunk output (tables, plots, text, or anything else) between documents. The ability to import reproducible results can completely change our writing workflow. It separates the tasks of analysis from summarization and changes our mindset to one of explaning “all the things” to curating the most persuasive evidence for our plaintxt arguments.\nIn this post, I discuss some of the reasons why I think data science communication goes wrong, why changing story orer helps, and how Quarto embeds can help us have reproducible results and a compelling story at the same time.\nThis topic has been on my mind for a while, and I was recently motivated to get this post over the finish line while talking to Dr. Lucy D’Agostino McGowan and Dr. Ellie Murray on their Casual Inference podcast. Thanks to them for the energy and inspiration!"
  },
  {
    "objectID": "post/quarto-comms/index.html#why-data-science-communication-is-hard",
    "href": "post/quarto-comms/index.html#why-data-science-communication-is-hard",
    "title": "How Quarto embed fixes data science storytelling",
    "section": "Why data science communication is hard",
    "text": "Why data science communication is hard\nThe deck is stacked against good communication of data science outcomes. Most of our experience with communication comes from education where it serves fundamentally different purposes. Educational communication tends to be linear and step-by-step, but professional communication should often lead with the key takeaway.\n\nCommunication in education\nThe majority of technical communication consumed and produced by early career professionals happened during their education. However, academic1 communication has a fundamentally different goal, so it does not provide an effective model.\nAcademic communcation leans towards exhaustive knowledge sharing of all the details – either because the target audience needs to know them or the audience needs to know that the communicator knows them.\nWhen students are communicating (completing problemsets or assignments), they have the goal of proving their mastery. Their audience (professors, TAs) can be assumed to have deeper knowledge of the topic than the presenter, and communication is intended to demonstrate comprehensiveness of knowledge – or at least to “show their work” for partial credit.\nWhen students are consuming communication (from an instructor or textbook), they experience communication with the goal of exhaustive knowledge transfer. Instructors or textbooks aim to make the audience know what they know and to be able to execute that information independently.\n\n\nCommunication in industry\nThese are decidedly not the objective of professional communication. We are given a job because we are judged to have the mastery of a topic specifically that no one else has the time, energy, or desire to think about in enough detail. The goal is not to show what you know (or, how much work you did along the way) or to get the audience to your intimacy of understanding.2\nInstead, the goal is to be an effective abstraction layer between the minute details and what is actually needed to act. Communication is an act of curating the minimal spanning set of relevant facts, context, and supporting evidence or caveats.3\n\n\nStory structuring\nRoughly speaking, this means we are used to talking about data science work in the procedural order:\n1. I wondered...\n2. I prepared my data...\n3. I ran this analysis...\n4. This gave me another question...\n5. That {did/not} work...\n6. So finally I ended up with this...\n7. I learned...\nHowever, for effective communication, it may be more useful to structure our work with progressive disclosure:\n1. I wondered...\n7. I ultimately found...\n6. This is based on trying this...\n(3-5). We also considered other options...\n2. And this is all based on this data, details, etc.\nIn short, we want to tell the story of why others should care about our results – not the story of how we got the result.4 Then, it helps turn a presentation or written document into a “conversation” where they can selectively partake of the details instead of waiting for the main point to be revealed as in a murder mystery."
  },
  {
    "objectID": "post/quarto-comms/index.html#communicating-and-the-data-science-workflow",
    "href": "post/quarto-comms/index.html#communicating-and-the-data-science-workflow",
    "title": "How Quarto embed fixes data science storytelling",
    "section": "Communicating and the data science workflow",
    "text": "Communicating and the data science workflow\nMoving between story structures isn’t just a matter of changing our mindset. Organizational pressures and tooling also bias us towards poor communication practices. I’ve always loved the phrase “writing is thinking”, but the corrolary is that your writing cannot be clearer than the amount of time you have take to think and synthesize what actually mattered from your own work.\nTimeline pressures push us towards more procedural stories. The story you yourself know about your work is the linear one that you just experienced – what you tried, why, and what happened next. If you need to communicate before you can synthesize and restructure, you will be caught flat-footed sharing anything but a procedural story. It’s likely better to begin drafting your final communication from a clean slate but tempting to reuse what exists.\nWhat’s more, even the best tools don’t set us up for success. I’ve long been a fan of literate programming tools like R Markdown and Quarto. I used to believe that these allowed me to effectively document while developing. This is true for documenting my raw stream of conscience for my own future reference or other colleagues. However, notebooks narratives are by definition in the procedural order.\nThis mindset is further embed as we think about working reproducibly and structuring our work into DAGs that can be rerun end-to-end. If I want to create a final manuscript that is fully reproducible with plots and tables generated dynamically (no copy pasting!), literate programming may feel like it is constraining me towards running things in order. (This isn’t entirely true, as I’ve written about before with R Markdown Driven Development.)"
  },
  {
    "objectID": "post/quarto-comms/index.html#using-quarto-embeds-to-improve-your-workflow",
    "href": "post/quarto-comms/index.html#using-quarto-embeds-to-improve-your-workflow",
    "title": "How Quarto embed fixes data science storytelling",
    "section": "Using Quarto embeds to improve your workflow",
    "text": "Using Quarto embeds to improve your workflow\nSo, we need to structure our stories differently for effective communication, but neither our timelines or tooling is conducive to it? That’s where the Quarto embed feature comes into the picture.\n\nQuarto embed overview\nThe embed shortcode lets us reference the output of another .qmd or ipynb Quarto document in a different Quarto file. This requires two steps:\nFirst, in the original notebook we label the top of the chunk whose output we wish to target, e.g. in our notebook analysis.ipynb:5\n#| label: my-calc\n\n1+1\nThen in our main document we can pull in the output (and optionally the code) of that calculation, e.g. in a final Quarto document final-writeup.qmd we could add:\n{{&lt; embed analysis.ipynb#my-calc &gt;}}\nThis works with any sort of cell output including raw print() statement output, plots, tables, etc.\n\n\nUsage patterns\nWhy are embeds a game-changer for data science communication? Because writing is thinking and storytelling is curation. Embeds can help us switch our mindset away from “showing our work” and towards providing persuasive evidence that supports our narrative.\nThe workflow I recommend is:\n\nStill use good practices to modularizes steps like data pulls, separate modules for reusable functions, etc.\nDo the analysis and last-mile transformation you would do in a Jupyter notebook, leaving the commentary that you would along the way\nAfter you’re done, think about what is important. What does your audience need to see and in what order?\nThen take a step back and write your actual story in a Quarto notebook\nSelectively embed compelling evidence at the right points in your narrative\n\nThis is illustrated in the figure below:\n\n\n\nContent embedding workflow illustration\n\n\nThis simple shortcode unblocks us from critical storytelling and workflow challenges:\n\nWe can generate content in rerunnable linear notebooks\nWe can start writing from a blank slate to ensure that we are focused on substance and not just sheer volume of content\nWe can then selectively curate output worthy of inclusion in a final document\nWe can insert these in the order that makes sense for the story versus for generation\nThis can exist without deleting or modifying our notebooks that capture the full thought process\nAs a bonus, our final document need not be a gnarly .ipynb but a plaintext .qmd to make version control, editing, and collaborating with noncoding contributors easier\n\nIt’s not just code output that can be imported either. Perhaps you already wrote up an introduction framed as an experimental design or a proposal? Other full markdown files can similarly be inclued with the includes shortcode. (includes adds the unrendered text of another .qmd file, including any code chunks to be executed, to your main .qmd; whereas, for embeds, we are referencing the output of another file without rerendering.)\n\n\n\n\n\n\nTip\n\n\n\nThis does not mean you should just cram all your analysis into your notebook and not worry about code quality, organization, or commentary!\nThe goal here is to have two good results for two different audiences without the overhead or reproducibility risks of maintaining them separately.\n\n\n\n\nDemo\nTo give a quick demo, I’ll made a separate notebook that just pulls some data from an API, cleans it up, and makes a few aggregations and plots. But suppose I doubt you’re interested in any of that. If you’ve read this long, you seem to trust me to do some amount of data stuff correctly.\nSo instead, I just put the following line in the .qmd file that is creating this post:\n{{&lt; embed raw-analysis.ipynb#tbl-pit-eo &gt;}}\nThat produces this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExecutive Orders Issued by Term\n\n\nNormalized for first 184 days in office (12.6% of term)\n\n\nTerm\nCount\n\n\nPoint in time\nFull term\n\n\n\n\n(2005) Bush\n13\n118\n\n\n(2009) Obama\n22\n148\n\n\n(2013) Obama\n14\n130\n\n\n(2017) Trump\n42\n220\n\n\n(2021) Biden\n52\n162\n\n\n(2025) Trump\n174\n-\n\n\n\n\n\n\n        \n\n\n            \n\n\n\nSource: Demo Notebook\nPerhaps I thought it was better to show you a simple table first. But, then I want to show you a more complex plot. You don’t have to see that in my original notebook I actually made the plot first. (Actually, I made two plots, but only one seemed important to show.) So, I write:\n{{&lt; embed raw-analysis.ipynb#fig-cum-eo &gt;}}\nThat produces this:\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nSource: Demo Notebook\nIf I had done a particularly good job of summarizing my thoughts immediately after seeing this plot, I might have already written them in a markdown cell over there. Embeds technically also work to embed markdown cells, so the following line you see is embedded from my other notebook also:\n\nI have some thoughts…\nSource: Demo Notebook\nHowever, I don’t advocate for embedding text. I think using a final qmd file as a single, self-contained spot to document your analysis has a lot of benefits.\nAnd then I could go on to add my relevant thoughts and analysis of specific to that plot. But, in this case, another part of professional communication is staying on topic."
  },
  {
    "objectID": "post/quarto-comms/index.html#footnotes",
    "href": "post/quarto-comms/index.html#footnotes",
    "title": "How Quarto embed fixes data science storytelling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCaveat, here I mostly am reflecting on US undergraduate education in STEM-related disciplines. And, yet, even narrowly scoped this is certainly a very sweeping generalization.↩︎\nThe aphorism “You don’t pay the plumber for banging on the pipes. You pay them for knowing where to bang.” really sums up what it means to be a professional. Similarly, you don’t hire them to tell you about why they are banging where.↩︎\nPersonally, my professional communication became a lot better only after I grew senior enough to be on the receiving end of a lot of communication. At the beginning of my career, I wondered: “Didn’t my more senior audiences get to those roles because they were smart? Didn’t they want all the details?” But we must considered the audience’s context – not just their background knowledge but also their environment. Considering your executive audience as people who have thought about 7 different topics at 20 minute intervals before talking to you today frames a whole different set of constraints. My overall philosophy for communication over time has shifted more towards “How to be kind to burned out brains” than “How to get other people excited by all the cool stuff I did”.↩︎\nI once coached an analyst who kept writing stories in this order. Responding to my feedback, they asked “But aren’t we supposed to tell a story?” This made me realize how overloaded and perhaps misleading the phrase “data storytelling” has become. Yes, we are telling the story from the data and not about analyzing the data. The analyst is not the main character!↩︎\nFWIW, I find Quarto can get confused if we don’t put the blank line after the label line.↩︎"
  },
  {
    "objectID": "post/recap-causal-2023/index.html",
    "href": "post/recap-causal-2023/index.html",
    "title": "Big ideas from the 2023 Causal Data Science Meeting",
    "section": "",
    "text": "Last week, I enjoyed attending parts of the annual virtual Causal Data Science Meeting organized by researchers from Maastricht University, Netherlands, and Copenhagen Business School, Denmark. This has been one of my favorite virtual events since the first iteration in 2020, and I find it consistently highlights the best of the causal research community: brining together industry and academia with concise talks that are at once thought-provoking, theoretically well-grounded, yet thoroughly pragmatic.\nWhile I could not join the entire event (running in CET time, some sessions fit snuggly between my first cup of coffee and first work meeting of the day in CST), this year’s conference did not disappoint! Below, I share a sampling with five “big ideas” from the sessions.\n\nWhat’s the current “gold standard” of causal ML methods in industry? Dima Goldenberg presented a great case study on heterogeneous uplift modeling at Booking.com. (While I couldn’t find the exact slides or paper, you can get a flavor of Booking’s work in experimentation and causal inference from their excellent tech blog )\nHow does causal evidence add value? Robert Kubinec conceptualized a measurable spectrum of descriptive to causal studies based on entropy. This framework broadens the aperture to think about how both quantitative and qualitative evidence can come together to form causal conclusions. (Preprint)\nBut how do we know the methods work? Causal methods are notoriously hard to validate since, by definition, we lack a ground truth against which to compare our estimate. To validate new methods, Lingjie Shen and coauthors presented one approach with their new [RCTrep R package] (https://github.com/duolajiang/RCTrep) which can be used to compare outcomes between real-world data (RWD) and randomized control trial data (RCT).\nAnd what do we do when they can’t get all the way there? Carlos Fernández-Loría and Jorge Loría talk on “Causal Scoring” explores how we can accept and make use of “causal ranking” or “causal classification” even when we do not believe we can generate fully credible, calibrated causal estimates. By defining which type of estimand is really necessary for a specific use case, they show how one can tailor their modeling approach and broaden the range of applications. (Preprint)\nFinally, do the best methods that correctly accrue causal evidence and validate matter? Ron Berman and Anya Shchetkina tackled this question in their paper about when correctly modeling uplift heterogeneity does and doesn’t matter. They decomposed potential causes using real-world marketing and public health examples and presented a methodology for identifying when uplift-based personalization makes a business impact (I couldn’t find pre-print, but they also presented at MIT’s CODE this week, so hopefully there will be a video soon!)\n\nOne of the joys of the causal DS community’s mindset is the inherent focus on impact and pragmatism, and this year’s conference continued to deliver in that vein. I’m marking my calendar (and setting my 4AM alarm!) for next year already."
  },
  {
    "objectID": "post/resource-roundup-r-industry/index.html",
    "href": "post/resource-roundup-r-industry/index.html",
    "title": "Resource Round-Up: R in Industry Edition",
    "section": "",
    "text": "Photo by Sharon McCutcheon on Unsplash\nOne of the ways that practices of reproducible research can be brought into industry is through the development of custom R packages and data tools for one’s company / organization. Not only can these tools deliver large efficiency gains and standardization, they ideally infuse corporate culture with the shared passion and mission found in open source communities.\nSimilar to [my favorite readings on reproducible research], the benefits of internal R packages are a common theme in my talks and on my Twitter feed1.\nThis post is a brief annotated bibliography of a few of my favorite “R use / packages in industry” case studies. There are far more great examples than I could ever cover, but the ones I tend to rely upon span a number of different types of organizations, all of whom use R in different and interesting ways and openly share both the benefits and challenges they’ve found. My summaries are intentionally kept short so that the authors’ words speak for themselves. I’ll just share a few key takeaways from each and commonalities between them. I encourage everyone to read the full articles:"
  },
  {
    "objectID": "post/resource-roundup-r-industry/index.html#footnotes",
    "href": "post/resource-roundup-r-industry/index.html#footnotes",
    "title": "Resource Round-Up: R in Industry Edition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee this thread for example↩︎\ncheck out AirBnb’s fantastic open source Knowledge Repository for a solution for hosting and browsing past enterprise research findings↩︎"
  },
  {
    "objectID": "post/rmarkdown-css-tips/index.html",
    "href": "post/rmarkdown-css-tips/index.html",
    "title": "RMarkdown CSS Selector Tips",
    "section": "",
    "text": "When working with R Markdown’s HTML output type, it’s possible to add a custom style to your output by passing in a CSS style sheet to the YAML header like this:\nTo use CSS effectively, it’s critical to understand how to specificy which selectors one wishes to style. This requires a mix of CSS knowledge and, in the case of R Markdown, an understanding of how HTML tags, IDs, and classes are used to wrap content from your R Markdown document.\nIn this post, I leave the first of these issues (CSS knowledge) out of scope. You may find a quick crash course in CSS in the CSS chapter of the blogdown book or study more in many free online tutorials such as W3 Schools, the Mozilla Foundation, and Free Code Camp.\nInstead, I focus on explaining tags, IDs, and classes for some frequently occurring R Markdown components and recommend tools for exploring and identifying more complex elements on your own."
  },
  {
    "objectID": "post/rmarkdown-css-tips/index.html#footnotes",
    "href": "post/rmarkdown-css-tips/index.html#footnotes",
    "title": "RMarkdown CSS Selector Tips",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHowever, when the chunk option results = 'asis' is set, output is wrapped in &lt;p&gt; tags instead.↩︎"
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html",
    "href": "post/rmddd-tech-appendix/index.html",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "",
    "text": "My initial post on RMarkdown Driven Development focuses on major concepts in the process of evolving a one-time, single-file analysis into a sustainable analytical tool. In the spirit of Etsy’s immutable documentation, I intentionally minimized references to specific tools or packages. After all, software is transient; principles are evergreen.\nHowever, RMarkdown Driven Development is more than just a conceptual framework. There are three main components: the main conceptual workflow (the why-to), the technical implementation (the how-to), and the comparison of merits between different destination states (the what-now). I touched on all three of these briefly in my rstudio::conf 2020 presentation1, but plan to explore each more deeply in their own post. This post focuses on the second: tools for technical implementation.\nTo implement RMarkdown Driven Development (RmdDD), one should take full advantage of the current state-of-the-art in R developer tools and the RStudio IDE. This article surveys helpful some of these helpful packages and tools. As the title implies, this piece is a appendix and not a complete narrative. It is neither a stand-along replacement for the original post, nor does it intend completely explain each referenced package and tool it mentions. This companion piece simply focuses on awareness raising for the plethora of tools that can make your job as a (file / project / package) engineer easier.\nAs this is a very lenthy post, please use the visual guide and the table of contents below to jump around as you please:\n\n\n\nDiagram of packages and tools on RmdDD timeline"
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#motivation",
    "href": "post/rmddd-tech-appendix/index.html#motivation",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "",
    "text": "My initial post on RMarkdown Driven Development focuses on major concepts in the process of evolving a one-time, single-file analysis into a sustainable analytical tool. In the spirit of Etsy’s immutable documentation, I intentionally minimized references to specific tools or packages. After all, software is transient; principles are evergreen.\nHowever, RMarkdown Driven Development is more than just a conceptual framework. There are three main components: the main conceptual workflow (the why-to), the technical implementation (the how-to), and the comparison of merits between different destination states (the what-now). I touched on all three of these briefly in my rstudio::conf 2020 presentation1, but plan to explore each more deeply in their own post. This post focuses on the second: tools for technical implementation.\nTo implement RMarkdown Driven Development (RmdDD), one should take full advantage of the current state-of-the-art in R developer tools and the RStudio IDE. This article surveys helpful some of these helpful packages and tools. As the title implies, this piece is a appendix and not a complete narrative. It is neither a stand-along replacement for the original post, nor does it intend completely explain each referenced package and tool it mentions. This companion piece simply focuses on awareness raising for the plethora of tools that can make your job as a (file / project / package) engineer easier.\nAs this is a very lenthy post, please use the visual guide and the table of contents below to jump around as you please:\n\n\n\nDiagram of packages and tools on RmdDD timeline"
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#review-of-rmarkdown-driven-development",
    "href": "post/rmddd-tech-appendix/index.html#review-of-rmarkdown-driven-development",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Review of RMarkdown Driven Development",
    "text": "Review of RMarkdown Driven Development\nBroadly speaking, RMarkdown Driven Development has five main steps.\n\nRemoving troublesome components\nRearranging chunks\nReducing duplication with functions\nModularizing Rmd chunks into separate files and folders\nMigrating modularized project assets into a package\n\nSteps 1-3 result in a well-engineered single-file RMarkdown, Step 4 in an R Project, and finally Step 5 in a Package. Critically, RmdDD introduces good software engineering practices as early as possible in each step of this workflow. This means that you can stop at steps 3, 4, or 5, and even up in a sustainable analytical tool in the form of either a single file .Rmd, an R project, or a package.2\nIn service of these ‘exit ramps’, this post also discusses steps 3.5 and 4.5, that is tools that are not part of the overall RmdDD ‘progression’ but add functionality to the single-file or project folder end state."
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#tools-covered",
    "href": "post/rmddd-tech-appendix/index.html#tools-covered",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Tools Covered",
    "text": "Tools Covered\nThe following R packages, RMarkdown features, and RStudio IDE utilities are mentioned in this post. If some are of particular interest to you, you can search this post to jump to where they are mentioned. Do not be overwhelmed by the sheer number of tools and functionality mentioned. Very few are actually necessary to conduct RmdDD or to build a good analysis tool. Simply pick whichever tools you like to add to your toolkit.\nR Packages\n\nrmarkdown\nkeyring\nhere\nroxygen2\nassertr\npointblank\nlintr\nstyler\nspelling\nDT\nxfun\ndiffr\nProjectTemplate\nstarters\nworkflowr\nrenv\ndrake\nusethis\ndevtools\ntestthat\npkgdown\nghactions\n\nRMarkdown Features\n\nParameters\nNamed chunks\nCode download\nEmbed a file\n\nRStudio IDE Utilities\n\nParameters UI\nNamed chunks\nIndexed comments\nFunction documentation skeleton shortcut\nLive spell-check\nR Project (.Rproj)\nVersion control\nPackage build pane"
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#step-1-removing-troublesome-components",
    "href": "post/rmddd-tech-appendix/index.html#step-1-removing-troublesome-components",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Step 1: Removing troublesome components",
    "text": "Step 1: Removing troublesome components\nThe goal of this step is to remove ‘clutter’ that your RMarkdown accumulated over the course of this analysis. Core examples are removing hard-coded variables, plain text credentials, local file paths, and unused code chunks.\n\nConvert hard-coded variables to parameters\nHard-coded values make code brittle. Future users may not know when and where to change them, or may change them inconsistently and violate the internal integrity of the analysis.\nFor example, imagine a quarterly report that requires filter multiple datasets to the same date range. This give an analyst updating the report to make multiple types of errors. First, they might forget to change the dates altogether, resulting in stale information. Even worse, they might remember to change some of the dates but accidentally miss one and end up blending two inconsistent data sources.\ndata1_lastyr &lt;- \n  data1 %&gt;%\n  filter(between(date, ‘2018-01-01’, ‘2018-03-31’))\n  \ndata2_lastyr &lt;- \n  data2 %&gt;%\n  filter(between(date, ‘2018-01-01’, ‘2018-03-31’))\nInstead, RMarkdown parameters allow us highlight key variables in the YAML header, and turn the entire RMarkdown document into a ‘mega-function’.\n---\ntitle: “My Analysis\"\noutput: html_document\nparams:\n  start: ‘2018-01-01’\n  end: ‘2018-03-31’\n---\nThese parameters are stored as list items in the params variable and can be referenced in the code like this:\ndata1_lastyr &lt;- \n  data1 %&gt;%\n  filter(between(date, params$start, params$end))\n\n\n\nPass secure parameters at knit-time\nParameters also provide a secure way to deal with passwords or other secret credentials. For example, if we need to connect to a database to pull data, we can create parameters in our YAML header with dummy values.\n---\ntitle: “My Analysis\"\noutput: html_document\nparams:\n  username: emily\n  password: x\n---\nThese values can then be referenced within the code like this.\ncon &lt;- \n  connect_to_database(\n    username = params$username,\n    password = params$password\n  )\nWhen it comes time to knit the RMarkdown, there are multiple ways to pass the parameters in at knit-time.\nIf we are knitting our RMarkdown from the RStudio IDE, we can use the parameters UI to prompt a pop-up box in which to enter parameters and replace the dummy values from the header. To do this, click the down arrow on the Knit button and chose Knit with Parameters....\nAlternatively, this can also be done programmatically with the rmarkdown::render function. The render function can kick-off the process of knitting any RMarkdown when provided its file path, e.g. rmarkdown::render(input = 'my-analysis.Rmd'). render has a params argument which, as stated in the documentation, accepts “a list of named parameters that override custom parameters specified in the YAML front-matter”. That is, we can write:\nrmarkdown::render(input = 'my-analysis.Rmd', params = list(password = {{CODE TO RETRIEVE PASSWORD}}))\nwhere {CODE TO RETRIEVE PASSWORD} is replaced with the relevant code. You could consider retrieving code from an environment variable, a keychain from the keyring package, or code you may use to interface with other password managers. Credential management is a large topic unto itself and beyond the scope of this post.\n\n\nCreate file paths with here\nWhen our RMarkdown includes external files such as data, images, scripts, etc. it can be tempting to reference the resource by its full, global file path. This can deceptively seem like the most robust option since we are telling our computer exactly where to find the resource. So we may have file paths that look like this:\ndata &lt;- readRDS(‘C:\\Users\\me\\Desktop\\my-project\\data\\my-data.rds’)\nHowever, this code is incredibly brittle. It effectively guarantees your code will not work on any machine but the one you are currently using. Even then, the path will break if you move the my-project directory.\nSlightly better is to use relative file paths based on the relationship to the working directory. By default, RMarkdown assumes the directory where it is stored is the working directory. So, if our RMarkdown lives in the my-project directory, the working directory will be ‘C:\\Users\\me\\Desktop\\my-project' and we can write:\ndata &lt;- readRDS('data\\my-data.rds’)\nThis version is resilient to moving around the my-project directory on your computer and will work on other computers so long as your RMarkdown and external file continue to have the same “steps” between them.\nHowever, this approach can still cause some unexpected behavior. Suppose we move our RMarkdown into the analysis sub-directory of my-project (as we will do in Step 4). Then, the effective working directory during the knitting process will be ‘C:\\Users\\me\\Desktop\\my-project\\analysis'. To reference the same data file, we would have to use a relative path that both gets us out of the analysis sub-directory (using .. to move up to the parent directory) and then go back down into the data sub-directory like this:\ndata &lt;- readRDS('..\\data\\my-data.rds')\nTo make our RMarkdown more resilient to where the RMarkdown lives within the project, we can construct file paths dynamically using the here package. Malcolm Barrett’s blog post provides a more complete description of here’s benefits, but in short, here helps guarantee consistent behavior both within a project and across operating systems. It’s user-friendly syntax looks like this:\ndata &lt;- readRDS(here::here(‘data’, ‘my-data.rds’))"
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#step-2-rearranging-chunks",
    "href": "post/rmddd-tech-appendix/index.html#step-2-rearranging-chunks",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Step 2: Rearranging chunks",
    "text": "Step 2: Rearranging chunks\nThe goal of this step is to group similar parts of code together, specifically by moving infrastructure (e.g. package loads, data ingestion) and heavy-duty computation chunks to the top and letting narratives, tables, and plots sink to the bottom. This makes our document more navigable and easier to edit by locating like elements (e.g. computation vs narration) more centrally, and it allows us to notice repeated code or narration for consolidation.\nWhile the process of rearranging chunks is mostly manually, we can also pursue the aim in enhancing developer navigability with a few useful features of the RStudio IDE.\n\nNamed Chunks\nSince we are grouping chunks together by intent in this step, We can use named chunks to communicate the purpose of each chunk. For example, most analyses probably include steps such as loading packages, loading data, cleaning data, etc. so we might want chunks named pkg-load, data-load, data-clean, etc. To do this, we would simply insert the name in the chunk header immediately after we specify the language engine, e.g. {r NAME-HERE}.\nNamed chunks have numerous benefits, but the one most relevant for improving navigability is that RStudio uses chunk names to create a dynamic table of contents at the bottom of the pane in which your RMarkdown lives, as shown below. This makes it easier to quickly find the piece of code we want to inspect and to rapidly move between sections.\n\n\n\nExample of RStudio table of contents\n\n\n\n\nIndexed Comments\nSimilarly to named chunks, sometimes we might want to annotate specific sections of code within a single chunk. To do this, we can use normal R code comments and simply put four dashes (----) at the end of the comment. RStudio uses this special type of comment in two ways. First, it adds this to the same table of contents we discussed before (as shown in the image above). Second, this also enables the ability to collapse code subsections by clicking on the small triangle that appears to the right of the line number of the commented line."
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#step-3-reducing-duplication-with-functions",
    "href": "post/rmddd-tech-appendix/index.html#step-3-reducing-duplication-with-functions",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Step 3: Reducing duplication with functions",
    "text": "Step 3: Reducing duplication with functions\nThe goal of this step is to reduce duplicated code by taking note of similar, repeated code patterns and converting them to functions. Doing this improves code readability and helps us apply changes more consistently. How to write R functions is out of scope for this post, but you can learn more here.\n\nDocument functions with roxygen2 template\nOne general benefit of R is the level of structure and consistency in the required documentation for R packages. Any time we bring up documentation with ? or the help() function, we benefit from precise descriptions of the functions intent, input, and output.\nroxygen2 provides a light-weight syntax for authors to write this documentation, which then is rendered to official documentation files in the package-building process. However, roxygen2 can be used even before we are building a package. This ensures we are writing documentation that is similar to what other R users are familiar with seeing and interpreting, and it also forces us to think more formally about the range of acceptable inputs and outputs for each function we write.\nSuppose we have defined a local scatterplot function that looks something like this:\nviz_scatter_x &lt;- function(data, vbl) {\n  ggplot(\n    data = data, \n    mapping = aes(x = x, y = {{vbl}}) +\n  geom_point()\n}\n\nWe can add roxygen2 documentation using special #' comments above our function followed by roxygen2 tags and values. To facilitate learning this system and rapidly generating documentation, we can use the RStudio IDE’s function documentation skeleton shortcut. With your cursor inside the function body, click Code &gt; Insert Roxygen Skeleton or use shortcut Ctrl+Alt+Shift+R. This adds into our code a basic documentation template.\n#' Title\n#'\n#' @param data \n#' @param vbl \n#'\n#' @return\n#' @export\n#'\n#' @examples\nviz_scatter_x &lt;- function(data, vbl) {\n  ggplot(\n    data = data, \n    mapping = aes(x = x, y = {{vbl}}) +\n  geom_point()\n}\n\nTo complete this example, we can fill out the template like this:\n#’ Scatterplot of variable versus x \n#'\n#' @param data Dataset to plot. Must contain variable named x\n#' @param vbl Name of variable to plot on y axis\n#'\n#' @return ggplot2 object\n#’ @import ggplot2\n#' @export\n\nviz_scatter_x &lt;- function(data, vbl) {\n  ggplot(\n    data = data, \n    mapping = aes(x = x, y = {{vbl}}) +\n  geom_point()\n}\nAn overview of roxygen2 syntax can be found here.\n\n\nValidate data inputs with assertr\nThe function above is not, in fact, an ideal R function. While the y variable in our plot is specified by a function argument, the x variable is hard-coded into the function internally. As such, we are assuming that the dataset a user passes into the function contains a variable specifically named x. This is not a best practice since it adds some brittleness to the code, but when you first begin writing functions, you might occasionally end up with some. Additionally, if you are confident your dataset should stay consistent overtime (e.g. pulling from a stable database schema),you may find this to be a practical option since it avoid repeatedly specifying arguments that you don’t expect to take different values.\nIf you are making any assumptions about the structure of your input data in your code, you might want to include a step to validate these assumptions and proactively flag to the user of the code if the data does not meet those expectations. R has multiple good options for data validation packages. If you are familiar with dplyr and are working with small datasets, the assertr package has a similar look and feel. If you want to validate large datasets on a remote system, the brand new pointblank package looks very promising, but I have not yet experimented with it personally.\n\n\nEnforce style guide with lintr or styler\nAt this point, we’ve eliminated as much unneeded or duplicative code as possible. Next, we should consider the quality and style of the code that remains. The lintr and the styler packages both accomplish this in slightly different ways.\nlintr works by analyzing your code and flagging issues for you to manually change. It can be run on one or more files and provides an itemized list of line numbers where a problem exists and a description of the issue. Example output is shown below:\n\n\n\nExample of lintr output\n\n\nstyler analyzes code similarly to lintr, but it automatically edits scripts to adhere to a style guide instead of providing suggestion.\nBoth packages allow us to customize what checks we impose on our script. The main differences is a personal preference how ‘human-in-the-loop’ you want to be. I personally prefer the lintr package to maintain full control over my code. Additionally, getting immediate feedback on ways to improve my code style helps me learn to avoid those specific mistakes in the future.\n\n\nCatch typos with spelling\nFinally, recall that one key reason we are working in RMarkdown to begin with is its capacity for literate programming and enabling communication of our results. We have thus far been mostly focused on the needs of future analysis tool users not the analysis results consumer. They will never see the style of our code but likely care a lot about the human-readability of our prose.\nFor this reason, as we finish polish our single-file RMarkdown, we should use the spelling package to check for typos in our analysis. Alternatively, if you are working in RStudio version 1.3 or later, you may also take advantage of the live spellcheck feature"
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#step-3.5-enhance-single-file-output",
    "href": "post/rmddd-tech-appendix/index.html#step-3.5-enhance-single-file-output",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Step 3.5: Enhance Single-File Output",
    "text": "Step 3.5: Enhance Single-File Output\nBefore we continue with the normal RmdDD ‘flow’, it’s worth taking a break to reflect upon what we’ve accomplished so far. At this point, we engineered a well-organized, well-document RMarkdown that is easier for developers to use.\nThere are a few reasons why this might be the right stopping point. First, if you are automating a report that is nearly identical from iteration to iteration, the value of single-file push-button execution may trump greater modularity or unneeded flexibility. Secondly, if you are in a resource constrained environment and lack a good system for sharing files with collaborators or conducting version control, a single-file documents is appealing for its portability.\nIf either of these describe your use case, this side-note is for you. Here, I comment on a few tips and tricks specific to getting the most out of single-file RMarkdowns. Otherwise, you may find it a bit tangential and prefer to jump to Step 4.\n\nSharing Resources\nIf portability is the goal, you can carry this to the logical extreme and ship additional resources in the output of your RMarkdown. In a sense, we are creating a ‘desert island’ RMarkdown that is completely self-sustaining. I don’t recommend doing all of the things listed below simultaneously;this will drastically increase file size, and if you need to do all of these things, it’s probably a sign you shouldn’t be aiming for the single-file endpoint. That said, I mention them all here in case any one might be useful for your scenario.\n\nAllow Code Download from RMarkdown Output\nYou can allow those with access to the HTML output of your RMarkdown download the source .Rmd by specifying code_download: true in the YAML header like this:\noutput:\n  html_document:\n    code_download: true\nThanks to Allison Hill for publicizing this great trick:\n\n\n{{% tweet \"1108925218850893832\" %}}\n\n\n\n\nAllow Data Preview/Download with DT\nFor relatively small datasets, you can both display you dataset and offer the option to download it using the DT package. This example demonstrates how you can add buttons to copy the data to clipboard or download it into multiple different formats.\n\n\nEmbed an arbitrary file\nMore broadly, you can embed a file, file(s), or folder from the HTML output of an RMarkdown with the functions embed_file(), embed_files(), and embed_dir() from the xfun package. The process of doing this is described in more detail in Yihui Xie’s blog post on these new functions.\n\n\n\nVersion Comparison with diffr\nAs mentioned above, single file outputs may be useful in scenarios in which you have no formal version control system. In this case, you can conduct a rudimentary comparison of the differences between separately saved versions of your script using the diffr package.\nThis tool wraps the JavaScript codediff library and provides an aesthetic comparison of differences between two text files, much like diff command line tools."
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#step-4-modularizing-rmd-chunks-into-a-project",
    "href": "post/rmddd-tech-appendix/index.html#step-4-modularizing-rmd-chunks-into-a-project",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Step 4: Modularizing Rmd chunks into a project",
    "text": "Step 4: Modularizing Rmd chunks into a project\nThe goal of this step is to strip as much as reasonable3 outside of our RMarkdown by organizing it into other sub-directories in a project directory. This forces us to make more modular and reusable project assets and to decrease our RMarkdown’s knit-time by not making it do unnecessary work.\n\nUse RStudio R Project (.Rproj)\nA R Project is a convenient structure that make a typically directory ‘smarter’. Projects have numerous benefits, but one particularly important one is that, when opened in RStudio, they immediately are treated as the working directory.\nIn the RStudio interface, they also improve navigability by enabling a Git tab for version control and easy access to a file explorer.\nYou can make a new project in RStudio using usethis::create_project() or by clicking File &gt; New Project....\n\n\nPick a consistent & opinionated folder structure\nAlternatively, other packages such as ProjectTemplate, starters, or workflowr will create your R project for you.\nEach ships with opinionated sub-directory structures to organize your work. These are similar in spirit to the structure I describe in the original RMarkdown Driven Development blog post but vary some in the exact semantics. Additionally, each provides a slightly different set of helper features to further tune and polish your project. I encourage you to explore all your options and pick what works best for you.\nThe most critical step here is consistency; the more you keep the same structure between projects (and, ideally, across collaborators or any organization of which you are part), the easier it is for others to navigate your directory and find the project assets in which they are interested."
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#step-4.5-manage-dependencies-versions-and-interactions",
    "href": "post/rmddd-tech-appendix/index.html#step-4.5-manage-dependencies-versions-and-interactions",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Step 4.5: Manage dependencies, versions, and interactions",
    "text": "Step 4.5: Manage dependencies, versions, and interactions\nOnce again, as in Section 3.5, we will briefly digress to take stock of where we are. Should we chose to take the ‘exit ramp’ of a well-formed R project, we will now find ourselves will a documented, modularized project with easy to find components. However, this structure has some additional benefits to reap and drawbacks to overcome if we plan to stop at the project stage, so this section focuses on overcoming them.\nOne advantage of moving from a single file to a project structure is that it enables you to create helper artifacts and sub-directories that contain various types of package metadata in order to better maintain your project over time. Three particularly important examples of this are package dependency management, version control, and project interdependency management.\n\nManage Package Dependencies with renv\nDependency management refers to the process of tracking the specific versions of the packages you used in your project. This helps ensure reproducibility of your code even if those packages change in future updates. For example, sometimes functions may be removed, renamed, or re-implemented in functions; these ‘breaking changes’ may be the best option for that package but may break your downstream code.\nPackage management is a challenging issue for many reasons. There are many critical decisions regarding when to capture the state of your dependencies, how to characterize the current version, and how to ensure you can recreate that environment in the future.\nCurrently, one promising tool in the dependency management space is renv. It provides a portable and light-weight solution to package management problems by logging metadata about dependencies without actually saving all packages that are needed locally. By preserving this information, it can help restore this information, renv can also help recreate these specific environments by re-installing specific versions of packages across a number of different repositories (CRAN, GitHub, etc.)\n\n\nConduct Version Control with RStudio git Integration\nThis is also a good time to start thinking about version control with git. This is yet another huge topic and out of scope for this post. A good place to get started with using git with R is Jenny Bryan’s Happy Git and GitHub for the UseR. More tactically, if you prefer to use Git through an IDE instead of the command line, RStudio has a built-in interface. You can add version control to an existing project by choosing Tools &gt; Project Options &gt; Version Control.\nI personally prefer (and, to some extent, recommend) executing git commands in a terminal, but I still make use of RStudio’s version control pane to easily preview my current branch and project status (e.g. equivalents of always-on access to the git status commands.)\nAs another alternative, usethis has a rapidly growing collection of functions to wrap conduct git and GitHub-related tasks in declarative R syntax.\n\n\nEnsure Project Syncing with drake\nHowever, moving from a single-file RMarkdown to a project is not purely upside; we’ve introduced to risks and brittleness of a different variety. Knitting a single-file RMarkdown causes all computation to rerun.4 This can be slow and tedious, but it helps guarantees that all of the parts of our analysis have run in order and all changes have complete ‘flowed through’. Projects, on the other hand, risk us forgetting steps (e.g. pulling fresh raw data but forgetting to refresh some intermediate data artifacts).\nThe drake package helps counteract this by providing an R wrapper for the command line utility make. These tools help us specify which documents in a project depend upon each other and allow us to selectively refresh elements that are ‘downstream’ from other critical elements."
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#step-5-migrating-a-project-to-a-package",
    "href": "post/rmddd-tech-appendix/index.html#step-5-migrating-a-project-to-a-package",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Step 5: Migrating a project to a package",
    "text": "Step 5: Migrating a project to a package\nThe goal of this step is to convert our R project to a package. This is fairly straightforward because an R package is, at the highest level, simple R files stored in the right places, and R has excellent developer tools to help us accomplish this. Additionally, there is a fairly clear one-to-one mapping between analysis project files and an R package structure, as is described in more detail in the main post.\n\nConfigure your package with usethis\nusethis is a phenomenal tool for build R packages. I said above that write an R package is basically about saving files in the right place. Well, usethis makes this blazingly fast and simple by auto-generating many skeleton files, folder structures, and configuration infrastructure for your with simple declarative commands (e.g. use_r_script(), use_vignette()) which give you detailed and helpful messages as they go to work.\nJenny Bryan is current rewriting the acclaimed R Packages book to highlight synergy with usethis. Check out this resource, even while the transformation is in progress, for a great overview of package components.\n\n\nAutogenerate documentation with devtools\ndevtools has many helpful functions for writing a package. Most critically for this discussion is the devtools::document() function which will translate the roxygen2 comments you wrote way back in Step 3 and translate them into official R documentation files (.Rd files which will populate the man/ folder).\n\n\nWrite unit tests with testthat\ntestthat offers a high-level interface for writing unit tests for your R functions. This is critical for a well-managed R package, but quite honestly, writing functions for your unit tests are something you should start to think about as soon as you write functions!5\nIn fact, as Davis Vaughn points out on Twitter, if you save your R functions in an R/ folder instead of the generic src/ folder at the project stage, you can actually go ahead and run testthat unit tests at that step.\n\n\n{{% tweet \"1124732877344784384\" %}}\n\n\n\n\nCreate a package website with pkgdown\npkgdown goes one step above-and-beyond the requirements for an R package. It cleverly re-uses all of the metadata contained in your resulting R package to create an aesthetic and highly accessible documentation website for your pages. Want to see an ‘in the wild example’? All of the links above to usethis, devtools, testthat, and pkgdown point to outputs of this magical tool! Advanced features include custom search bars and Google Analytics tracking.\n\n\nCoordinate tasks with RStudio’s Build Pane\nRStudio’s package build pane provides a great number of resources for helping with the package build process. This tab will help you execute and review the results of unit tests and R CMD check (don’t worry if you don’t know what that is yet!) and provides many helpful options for installing and building your package. The build pane accomplishes this by adding a UI layover over many of the functions in devtools and relates package development tools to further streamline your workflow.\n\n\nAutomate builds with ghactions\nAs mentioned above, a lot of packages like devtools and testthat help you easily set up a lot of critical package infrastructure. However, you may find that there is a lot of manual overhead in repeatedly rerunning commands to build documentation, execute unit tests, or rebuild your package website. Additionally, if you are working with collaborators, you may wish to ensure these process-steps are executed on their code contributions before you incorporate them into your repository. If any of this sounds like a problem you are having, you might need a solution for continuous integration.\nThe ghactions package is here to help! It provides an R interface to GitHub Actions, a workflow automation tool available for GitHub-based repos.\nThis is a relatively new package, and currently you will likely find more online support and documentation for other continuous integration tools like Travis CI and Appveyor (including usethis functions that will help you set up configuration files for these services, along with documentation in the R packages book). However, GitHub Actions appears to be growing in popularity due to it’s parity of features and superior integration with GitHub repositories. For more information, check out Jim Hester’s compelling rstudio::conf 2020 presentation on GitHub Actions."
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#closing-thoughts",
    "href": "post/rmddd-tech-appendix/index.html#closing-thoughts",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nThis post recounts the best practices and tools that I am aware of for RmdDD as of January 2020. As I stated in the introduction, I’m under no illusion that this content will not eventually grow stale. If you know over other good tools that should be included or find any of the information mentioned is out of date, please contact me on Twitter or consider submitting a PR on GitHub to add or update content."
  },
  {
    "objectID": "post/rmddd-tech-appendix/index.html#footnotes",
    "href": "post/rmddd-tech-appendix/index.html#footnotes",
    "title": "RMarkdown Driven Development: the Technical Appendix",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you wish, you may see the slides on SlideShare or watch the full conference video↩︎\nThe choice between these depends on the problem you are solving and the needs of your future users. While that choice is out-of-scope for this post, advice on making that decision is contained in my rstudio::conf slides and will be the subject of a future post.↩︎\n‘Reasonable’ in this case is highly dependent on what parts of the RMarkdown are static or dynamic.↩︎\nUnless one uses chunk caching. I personally find this to be a risky solution that is prone to human-error, so I leave it out of scope for this piece.↩︎\nArguably, proponents of test-driven development would say you think about your tests before writing any functions.↩︎"
  },
  {
    "objectID": "post/shiny-db/index.html",
    "href": "post/shiny-db/index.html",
    "title": "Using databases with Shiny",
    "section": "",
    "text": "Shiny apps are R’s answer to building interface-driven applications that help expose important data, metrics, algorithms, and more with end-users. However, the more interesting work that your Shiny app allows users to do, the more likely users are to want to save, return to, and alter some of the ways that they interacted with your work.\nThis creates a need for persistent storage in your Shiny application, as opposed to the ephemeral in-memory of basic Shiny applications that “forget” the data that they generated as soon as the application is stopped.\nRelational databases are a classic form of persistent storage for web applications. Many analysts may be familiar with querying relational databases to retrieve data, but managing a database for use with a web application is slightly more complex. You’ll find yourself needing to define tables, secure data, and manage connections. More importantly, you might worry about what things that you do not know you should be worrying about.\nThis post provides some tips, call-outs, and solutions for using a relational database for persistent storage with Shiny. In my case, I rely on a Shiny app built with the golem framework and served on the Digital Ocean App platform."
  },
  {
    "objectID": "post/shiny-db/index.html#databases-options-for-storage",
    "href": "post/shiny-db/index.html#databases-options-for-storage",
    "title": "Using databases with Shiny",
    "section": "Databases & Options for Storage",
    "text": "Databases & Options for Storage\nDean Attali’s blog post on persistent storage compares a range of options for persistent storage including databases, S3 buckets, Google Drive, and more.\nFor my application, I anticipated the need to store and retrieve sizable amounts of structured data, so using a relational database seemed like a good option. Since I was hosting my application on Digital Ocean App Platform, I could create a managed Postgres database with just a few button clicks. As I share in the “Key Issues” section, this solution offers some significant benefits in terms of security.\nFor more information on different options for hosting Shiny apps and some insight into why I chose Digital Ocean, check out Peter Solymos’ excellent blog on Hosting Data Apps."
  },
  {
    "objectID": "post/shiny-db/index.html#talking-to-your-database-through-shiny",
    "href": "post/shiny-db/index.html#talking-to-your-database-through-shiny",
    "title": "Using databases with Shiny",
    "section": "Talking to your database through Shiny",
    "text": "Talking to your database through Shiny\nGeneral information on working with databases with R is included on RStudio’s excellent website. Below, I focus on a few topics specific to databases with Shiny, Shiny apps built in the {golem} framework, and Shiny apps served on Digital Ocean in particular.\n\nCreating a database\nTo create a database for my application in DigitalOcean, I simply went to:\nSettings &gt; Add Component &gt; Database\nThis creates a fully-managed Postgres databases so you do not have to thing a ton about the underlying set-up or configuration.\nAt the time on writing, I was able to add a 1GB Dev Database for /$7 / month. For new users, DigitalOcean offers a generous number of free credits for use in the first 60 days. For a more mature product, one can add or switch to a production-ready Managed Database.\nAfter a few minutes, the database has launched and its Connection Parameters are provided, which look something like this:\nhost     : abc.b.db.ondigitalocean.com\nport     : 25060\nusername : db\npassword : abc123\ndatabase : db\nsslmode  : require\nBy default, the Dev Database registers your application as a Trusted Source, meaning that only traffic from the application can attempt to access the database. As the documentation explains, this type of firewall improves security by preventing against brute-force password or denial-of-service attacks from the outside.\nNote: If you just want to experiment with databases and Shiny but aren’t using an in-production, served application, you can mostly skip this step and use the “Dev” approach that is discuss in “Dev versus Prod” subsection of “Key Issues” below.\n\n\nConnecting to the database\nWe can use the connection parameters provided to connect to the database using R’s DBI package.\n\ncon &lt;- DBI::dbConnect(RPostgres::Postgres(),\n                      host   = \"aabc.b.db.ondigitalocean.com\",\n                      dbname = \"db\",\n                      user      = \"db\",\n                      password  = \"abc123\",\n                      port     = 25060)\n\nWe will talk about ways to not hardcode one’s password in the last section.\n\n\nCreating tables\nNext, you can set up tables in your database that your application will require.\nIf you know SQL DDL, you can write a CREATE TABLE statement which defines a tables names, fields, and data types. However, this can feel verbose or uncomfortable to analysts who mostly use DML (e.g. SELECT, FROM, WHERE).\nFortunately, you can also define a table using R’s DBI package. First, create a simple dataframe with a single record to help R infer the appropriate and expected data types. Then pass the first zero rows of the table (essentially, only the schema) to DBI::dbCreateTable().\n\ndf &lt;- data.frame(x = 1, y = \"a\", z = as.Date(\"2022-01-01\"))\nDBI::dbCreateTable(con, name = \"my_data\", fields = head(df, 0))\n\nTo prove that this works, I show a “round trip” of the data using an in-memory SQLite database. Note that this is not an option for persistent storage because in-memory databases are not persistent. This is only to “prove” that this approach can create database tables.\n\ncon_lite &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\ndf &lt;- data.frame(x = 1, y = \"a\", z = as.Date(\"2022-01-01\"))\nDBI::dbCreateTable(con_lite, name = \"my_data\", fields = head(df, 0))\nDBI::dbListTables(con_lite)\n\n[1] \"my_data\"\n\nDBI::dbReadTable(con_lite, \"my_data\")\n\n[1] x y z\n&lt;0 rows&gt; (or 0-length row.names)\n\n\nBut where should you run this script? You do not want to put this code in your app to run every time the app launches, but we just limited database traffic to the app so we cannot run it locally. Instead, you can run this code from the app’s console. (Alternatively, if you upgrade to a Managed Database, I believe you can also whitelist your local IP as another trusted source.)\n\n\nForming the connection within your app\nOnce your database is set-up and ready to go, you can begin to integrate it into your application.\nI was using the golem framework for my application, so I connected to the database and made the initial data pull by adding the following lines in my top-level app_server.R file.\n\ncon &lt;- db_con()\ntbl_init &lt;- DBI::dbReadTable(con, \"my_data\")\n\nThe custom db_con() function contains roughly the DBI::dbConnect() code we saw above, but I turned it into a function to incorporate some added complexity which I will describe shortly.\nMost of the rest of my application uses Shiny modules, and this connection object and initial data pull can be seamless passed into either.\nTo see similar code in a full app, check out Colin Fay’s golemqlite project on Github.\n\n\nCRUD operations\nCRUD operations (Create, Read, Update, Delete) are at the heart of any interactive application with persistent data storage.\nInteracting with your database within Shiny begins to look like more rote Shiny code. I do not describe this process in much detail since it is quite specific to what your app is trying to accomplish, but this blog post provides some nice examples.\nIn short:\n\nTo add records to the table, you can use DBI::dbAppendTable()\nTo remove records from the table, you can construct a DELETE FROM my_data WHERE &lt;conditions&gt; statement and run it with DBI::dbExecute()\n\nSome cautions on the second piece are included in the “Key Issues” section."
  },
  {
    "objectID": "post/shiny-db/index.html#key-issues",
    "href": "post/shiny-db/index.html#key-issues",
    "title": "Using databases with Shiny",
    "section": "Key Issues",
    "text": "Key Issues\nAdding a permanent data store to your application can open up a lot of exciting new functionality. However, it may create some challenges that your typical data analyst or Shiny developer has not faced before. In this last section, I highlight a few key issues that you should be aware of and provide some recommendations.\n\nSecuring data transfer\nAlready, we have one safeguard in place for data security since our application is the only Trusted Source able to interface with our database.\nBut, just like we secure our database credentials, it becomes important to think about securing the database itself. This is made easy with DigitalOcean because data is end-to-end encrypted, but depending on how or by whom your data is managed, this is something to bear in mind.\n\n\nSecuring database credentials\nNo matter how safe the data itself is, it still may be at risk if anyone can obtain our database credentials.\nPreviously, I demonstrated how to connect to a database from R like this:\n\ncon &lt;- DBI::dbConnect(RPostgres::Postgres(),\n                      host   = \"aabc.b.db.ondigitalocean.com\",\n                      dbname = \"db\",\n                      user      = \"db\",\n                      password  = \"abc123\",\n                      port     = 25060)\n\nHowever, you should never ever put your password in plaintext like this. Instead, you can use environment variables to store the value of sensitive credentials like a password or even a username like this:\n\ncon &lt;- DBI::dbConnect(RPostgres::Postgres(),\n                      host   = \"aabc.b.db.ondigitalocean.com\",\n                      dbname = \"db\",\n                      user      = \"db\",\n                      password  = Sys.getenv(\"DB_PASS\"),\n                      port     = 25060)\n\nThen, you can define that same environment variable more securely in within the App Platform.\n\n\nSecuring input integrity (SQL injection)\nFinally, it’s also important to be aware of SQL injection to ensure that your database does not get corrupted.\nSQL injection is usually discussed in the concept of malicious attacks. For example, W3 schools shows the following example where an application could be tricked into providing data on all users instead of a single user:\ntxtUserId = getRequestString(\"UserId\");\ntxtSQL = \"SELECT * FROM Users WHERE UserId = \" + txtUserId;\nIf the entered UserId is \"UserId = 105 OR 1=1\", then the full SQL string will be \"SELECT * FROM Users WHERE UserId = 105 OR 1=1;\".\nSQL injection is also at jokes you make have heard about “little Bobby Drop Tables” (xkcd).\n\nThat joke also, in some odd way, highlights that SQL injection need not be malicious. Rather, whenever we have software opened up to users beyond ourselves, they will likely use it in unexpected ways that push the system to its limit. For example, a user might try to enter or remove values from our database with double quotes, semicolons, or other features that mean something different to SQL than in human parlance and corrupt the code. Regardless of intent, we can protect against bad SQL that will break our application by using the DBI::sqlInterpolate() function.\nA demonstration of this function and how it can protect against bad query generation is shown in this post by RStudio.\n\n\nDev versus Prod\nHowever, you may have realized a flaw in this approach. Our entire app now depends on forming a connection that can only be made by the in-production app. This meams you cannot test your application locally. However, even if our local traffic was not categorically blocked, we wouldn’t want to test our app on the production database and recklessly add and remove entries.\nInstead, we would ideally have separate databases: one for development and one for production. Ideally, these would be the same type of database (e.g. both Postgres) to catch nuances of different SQL syntax and database operations. However, to keep things simpler (and cheaper), I decided to use an in-memory SQLite database locally.\nTo accomplish this, I wrapped my database connection in a custom db_con() function that checks if the app is running in development or production (using golem::app_prod() which in turn checks the R_CONFIG_ACTIVE environment variable) and connects to different databases in either case. In the development case, it creates an in-memory SQLite database and remakes the empty table.\n(Another alternative to creating the database on-the-fly is to pre-make a SQLite database saved to a .sqlite file and connect to that. But for this example, my sample table is so simple, creating it manually takes a negligible amount of time and keeps things quite readable, so I left it as-is.)\n\ndb_con &lt;- function(prod = golem::app_prod()) {\n  \n  if (prod) {\n    \n    con &lt;- DBI::dbConnect(RPostgres::Postgres(),\n                          host   = \"abc.b.db.ondigitalocean.com\",\n                          dbname = \"db\",\n                          user      = \"db\",\n                          password  = Sys.getenv(\"DB_PASS\"),\n                          port     = 25060)\n    \n  } else {\n    \n    stopifnot( require(\"RSQLite\", quietly = TRUE) )\n    con &lt;- DBI::dbConnect(SQLite(), \":memory:\")\n    df &lt;- data.frame(x = 1, y = \"a\", z = as.Date(\"2022-01-01\"))\n    DBI::dbWriteTable(con, \"my_data\", df)\n    \n  }\n  \n  return(con)\n  \n}\n\n\n\nManaging connections\nSo, you’ve built a robust app that can run against a database locally or on your production server. Great! It’s time to share your application with the world. But what if it is so popular that you have a lot of concurrent users and they are all trying to work with the database at once?\nTo maintain good application performance, you have to be careful about managing the database connection objects that you create (with DBI::dbConnect()) and to close them when you are doing using them.\nIf this sounds manual and tedious, you’re in luck! The {pool} package adds a layer of abstraction to manage a set of connections and execute new queries to an available idle collection. Full examples are given on the package’s website, but in short {pool} is quite easy to implement due to it’s DBI-like syntax. You can replace DBI::dbConenct() with pool::dbPool() and proceed as usual!"
  },
  {
    "objectID": "post/snow/index.html",
    "href": "post/snow/index.html",
    "title": "How to Make R Markdown Snow",
    "section": "",
    "text": "Last year, I tweeted about how to spread holiday cheer by letting your R Markdown documents snow. After all, what better to put people in the holiday spirit than to add a random 5% probability that whatever part of a document they are trying to read will be covered?\nI make no promises that this will amuse your recipients, but at least it seemed to strike a cord with other R Markdown creators. This year, I decided to write it up step-by-step. As silly as the example is, I think it demonstrates (through slight abuse) some useful features of R Markdown. Much like ice sculpting, we will apply the powertool that is R Markdown to achieve our rather fanciful end.\nIf you want to skip the discussed, you can check out the full project, the main R Markdown file, or the rendered output. The rendered output is also shown below:\nIn the rest of this post, I’ll touch on three R Markdown tricks and their fanciful uses:\nWe will see how to dress up this very important business R Markdown\nMuch more useful applications of these same features are discussed in the linked sections of the R Markdown Cookbook."
  },
  {
    "objectID": "post/snow/index.html#child-documents",
    "href": "post/snow/index.html#child-documents",
    "title": "How to Make R Markdown Snow",
    "section": "Child documents",
    "text": "Child documents\nChild documents allow R Markdown authors to combine multiple R Markdown files into a single final output rendered in a consistent environment. This helps create a more manageable, modular workflow if you are working on a long project or anaylsis with many distinct parts or if there are some pieces of boilerplate text or analysis that you wish to inject into many projects.\nTo add child documents, we create an empty R code chunk, and use the child chunk option to pass the path to the R Markdown file that we wish to include. In our case, we reference our snow.Rmd file.\n```{r child = \"snow.Rmd\"}`r ''`\n```\nOf course, since child documents are functionally the same as including files in the same document, we could have included this material in the same file. However, since snowflakes should clearly only be placed in very important documents, it is good to use best practices and take a modular approach. Tactically, this also makes it easier to “turn them on an off” at will or swap them our for New Years fireworks, Valentine’s Day hearts, and more."
  },
  {
    "objectID": "post/snow/index.html#including-html-and-css",
    "href": "post/snow/index.html#including-html-and-css",
    "title": "How to Make R Markdown Snow",
    "section": "Including HTML and CSS",
    "text": "Including HTML and CSS\nSo, what is in the snow.Rmd file?\nFirst, we have to bring in the snowflakes themselves.\n&lt;div class=\"snowflakes\" aria-hidden=\"true\"&gt;\n  &lt;div class=\"snowflake\"&gt;\n  ❅\n  &lt;/div&gt;\n  &lt;!-- many more snowflakes... --&gt;\n&lt;/div&gt;\nBecause this R Markdown will render to an HTML document, we are free to include raw HTML text the same way we include narrative, English-language text. Here, I wrap unicode snowflakes in &lt;divs&gt; so I can attach CSS classes to them.\nSimilarly, R Markdowns that will be rendered to HTML can use all the benefits of web technology like CSS and JavaScript. Custom CSS can be included either with the css language engine or a reference in the YAML header to an external .css file. For compactness, I go with the former.\nA css chunk adds CSS code used to animate the snowflake divs. This is taken nearly verbatim from this CodePen. Since this is rather lengthy, we can also use the echo = FALSE chunk option to not output all of the CSS in our final document.\n```{css echo = FALSE}`r ''`\n&lt;&lt;css goes here&gt;&gt;\n```\nFor more tips on writing CSS for R Markown, check out my post on finding the right selectors."
  },
  {
    "objectID": "post/snow/index.html#conditional-chunk-evaluation",
    "href": "post/snow/index.html#conditional-chunk-evaluation",
    "title": "How to Make R Markdown Snow",
    "section": "Conditional chunk evaluation",
    "text": "Conditional chunk evaluation\nThe above two tricks are as for as my demo goes since I only planned to render it once. However, if you are creating automated reports and fear your recipients have limited patience for animated snowflakes, we can also use R Markdown chunk options with variables as arguments to only allow these snowflakes to appear during a certain time period.\nSo, for example instead of:\n```{r child = \"snow.Rmd\"}`r ''`\n```\nWe might type:\n```{r child = \"snow.Rmd\", eval = (substr(Sys.Date(), 6, 7) == 12)}`r ''`\n```\nTo only allow the child document to be included in December.\nIf we had chosen not to use child documents, we could also use chunks to achieve conditional evaluation using the asis engine."
  },
  {
    "objectID": "post/sql-r-flow/index.html",
    "href": "post/sql-r-flow/index.html",
    "title": "Workflows for querying databases via R",
    "section": "",
    "text": "Simple, self-contained, reproducible examples are a common part of good software documentation. However, in the spirit of brevity, these examples often do not demonstrate the most sustainable or flexible workflows for integrating software tools into large projects. In this post, I document a few mundane but useful patterns for querying databases in R using the DBI package.\nA prototypical example of forming and using a database connection with DBI might look something like this:\nlibrary(DBI)\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\ndbWriteTable(con, \"diamonds\", ggplot2::diamonds)\ndat &lt;- dbGetQuery(con, \"select cut, count(*) as n from diamonds group by 1\")\ndat\n\n        cut     n\n1      Fair  1610\n2      Good  4906\n3     Ideal 21551\n4   Premium 13791\n5 Very Good 12082\nA connection is formed (in this case to a fake database that lives only in my computer’s RAM), the diamonds dataset from the ggplot2 package is written to the database (once again, this is for example purposes only; a real database would, of course, have data), and dbGetQuery() executes a query on the resulting table.\nHowever, as queries get longer and more complex, this succinct solution becomes less attractive. Writing the query directly inside dbGetQuery() blurs the line between “glue code” (rote connection and execution) and our more nuanced, problem-specific logic. This makes the latter harder to extract, share, and version.\nBelow, I demonstrate a few alternatives that I find helpful in different circumstances such as reading queries that are saved separately (in different files or at web URLs) and forming increasingly complex query templates. Specifically, we’ll see how to:"
  },
  {
    "objectID": "post/sql-r-flow/index.html#read-query-from-separate-file",
    "href": "post/sql-r-flow/index.html#read-query-from-separate-file",
    "title": "Workflows for querying databases via R",
    "section": "Read query from separate file",
    "text": "Read query from separate file\nA first enhancement is to isolate your SQL script in a separate file than the “glue code” that executes it. This improves readability and makes scripts more portable between projects. If a coworker who uses python or runs SQL through some other tool wishes to use your script, it’s more obvious which parts are relevant. Additionally, its easier to version control: we likely care far more about changes to the actual query than the boilerplate code that executes it so it feels more transparent to track them separately.\nTo do this, we can save our query in a separate file. We’ll call it query-cut.sql:\n\nselect\n  cut,\n  count(*) as n\nfrom diamonds\ngroup by 1\n\nThen, in our script that pulls the data, we can read that other file with readLines() and give the results of that to the dbGetQuery() function.\n\nlibrary(DBI)\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\nquery &lt;- paste(readLines(\"query-cut.sql\"), collapse = \"\\n\")\ndat &lt;- dbGetQuery(con, query)\ndat\n\n        cut     n\n1      Fair  1610\n2      Good  4906\n3     Ideal 21551\n4   Premium 13791\n5 Very Good 12082\n\n\nOf course, if you wish you could define a helper function for the bulky paste(readLines(...)) bit.\n\nread_source &lt;- function(path) {paste(readLines(path), collapse = \"\\n\")}"
  },
  {
    "objectID": "post/sql-r-flow/index.html#read-query-from-url-like-github",
    "href": "post/sql-r-flow/index.html#read-query-from-url-like-github",
    "title": "Workflows for querying databases via R",
    "section": "Read query from URL (like GitHub)",
    "text": "Read query from URL (like GitHub)\nSometimes, you might prefer that your query not live in your project at all. For example, if a query is used across multiple projects or if it changes frequently or is maintained by multiple people, it might live in a separate repository. In this case, the exact same workflow may be used if the path is replaced by a URL to a plain-text version of the query. (On GitHub, you may find such a link by clicking the “Raw” button when a file is pulled up.)\n\nlibrary(DBI)\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\nurl &lt;- \"https://raw.githubusercontent.com/emilyriederer/website/master/content/post/sql-r-flow/query-cut.sql\"\nquery &lt;- paste(readLines(url), collapse = \"\\n\")\ndat &lt;- dbGetQuery(con, query)\ndat\n\n        cut     n\n1      Fair  1610\n2      Good  4906\n3     Ideal 21551\n4   Premium 13791\n5 Very Good 12082\n\n\nThis works because the query variable simply contains our complete text file read from the internet:\n\ncat(query)\n\nselect\n  cut,\n  count(*) as n\nfrom diamonds\ngroup by 1\n\n\nAlternatively, in an institutional setting you may find that you need some sort of authentication or proxy to access GitHub from R. In that case, you may retrieve the same query with an HTTP request instead using the httr package.\n\nlibrary(httr)\n\nurl &lt;- \"https://raw.githubusercontent.com/emilyriederer/website/master/content/post/sql-r-flow/query-cut.sql\"\nquery &lt;- content(GET(url))\ncat(query)\n\nselect\n  cut,\n  count(*) as n\nfrom diamonds\ngroup by 1"
  },
  {
    "objectID": "post/sql-r-flow/index.html#use-query-template",
    "href": "post/sql-r-flow/index.html#use-query-template",
    "title": "Workflows for querying databases via R",
    "section": "Use query template",
    "text": "Use query template\nSeparating the query from its actual execution also allows us to do query pre-processing. For example, instead of a normal query, we could write a query template with a wildcard variable. Consider the file template-cut.sql:\n\nselect\n  cut,\n  count(*) as n\nfrom diamonds\nwhere price &lt; {max_price}\ngroup by 1\n\nThis query continues to count the number of diamonds in our dataset by their cut classification, but now is has parameterized the max_price variable. Then, we may use the glue package to populate this template with a value of interest before executing the script.\n\nlibrary(DBI)\nlibrary(glue)\n\ncon &lt;- dbConnect(RSQLite::SQLite(), \":memory:\")\ntemplate &lt;- paste(readLines(\"template-cut.sql\"), collapse = \"\\n\")\nquery &lt;- glue(template, max_price = 500)\ndat &lt;- dbGetQuery(con, query)\ndat\n\n        cut   n\n1      Fair   7\n2      Good 226\n3     Ideal 628\n4   Premium 215\n5 Very Good 653\n\n\nThis is a useful alternative to databases that do not allow for local variables."
  },
  {
    "objectID": "post/sql-r-flow/index.html#compose-more-complex-queries",
    "href": "post/sql-r-flow/index.html#compose-more-complex-queries",
    "title": "Workflows for querying databases via R",
    "section": "Compose more complex queries",
    "text": "Compose more complex queries\nThe idea of templating opens up far more interesting possibilities. For example, consider a case where you wish to frequently create the same data structure for a different population of observations (e.g. a standard set of KPIs for different A/B test experiments, reporting for different business units, etc.)\nA boilerplate part of the query could be defined as a template ready to accept a CTE or a subquery for a specific population of interest. For example, we could write a file template-multi.sql:\n\nwith\nsample as ({query_sample}),\nprices as (select id, cut, price from diamonds)\nselect prices.*\nfrom\n  prices\n  inner join\n  sample\n  on\n  prices.id = diamonds.id\n\nThen our “glue code” can combine the static and dynamic parts of the query at runtime before executing.\n\ntemplate &lt;- paste(readLines(\"template-multi.sql\"), collapse = \"\\n\")\nquery_sample &lt;- \"select * from diamonds where cut = 'Very Good' and carat &lt; 0.25\"\nquery &lt;- glue(template, query_sample = query_sample)\ncat(query)\n\nwith\nsample as (select * from diamonds where cut = 'Very Good' and carat &lt; 0.25),\nprices as (select id, cut, price from diamonds)\nselect prices.*\nfrom\n  prices\n  inner join\n  sample\n  on\n  prices.id = diamonds.id\n\n\n(Of course, this may seem like overkill and an unnecessarily inefficent query for the example above where a few more where conditions could have sufficed. But one can imagine more useful applications in a traditional setting where multiple tables are being joined.)"
  },
  {
    "objectID": "post/sql-r-flow/index.html#query-package",
    "href": "post/sql-r-flow/index.html#query-package",
    "title": "Workflows for querying databases via R",
    "section": "Query package",
    "text": "Query package\nFinally, these queries and query templates could even be shipped as part of an R package. Additional text files may be placed in the inst/ directory and their paths discovered by system.file(). So, if your package myPkg were to contain the template-multi.sql file we saw above, you could provide a function to access it like so:\n\nconstruct_query &lt;- function(stub, ...) {\n  \n  path &lt;- system.file(stub, package = 'myPkg')\n  template &lt;- paste(readLines(path), collapse = '\\n')\n  query &lt;- glue::glue(template, ...)\n  return(query)\n  \n}\n\nThen, that function could be called like so:\n\nsample &lt;- \"select * from diamonds where cut = 'Very Good' and carat &lt; 0.25\"\nquery &lt;- construct_query(\"multi\", query_sample = sample)\n\nThis approach has some benefits such as making it easier to share queries across users and benefit from package versioning and environment management standards. However, there are of course other risks; only dynamically generating queries could limit reproducibility or documentation about the actual query run to generate data. Thus, it might be a good idea to save the resulting query along with the resulting data:\n\ndat &lt;- dbGetQuery(con, query)\nsaveRDS(dat, \"data.rds\")\nwriteLines(query, \"query-data.sql\")"
  },
  {
    "objectID": "post/sql-r-flow/index.html#bonus-data-testing",
    "href": "post/sql-r-flow/index.html#bonus-data-testing",
    "title": "Workflows for querying databases via R",
    "section": "Bonus: Data Testing",
    "text": "Bonus: Data Testing\nAlthough unrelated to the previous workflow, another nice aspect of working with SQL through R is the ability to use R’s superior toolkit for creating small datasets on the fly for testing purposes.\nMocking data to easily test SQL can be a tedious exercise since you generally need to write out the dataset row-by-row:\n\nINSERT INTO test_tbl\n  ( x, y, z )\nVALUES\n  (1, 'A', NA), \n  (2, 'B', 0), \n  (3, 'C', 1);\n\nThis may be fine for a few rows, but it can easily get cumbersome when you require a mock dataset in the 10s or 20s of rows.\nOf course, R has many helpful functions for generating data including sequences, predefined vectors (e.g. letters), and random number generators. This can make it easy to quickly generate data and push it to a database for testing SQL scripts:\n\nn &lt;- 26\ntest_df &lt;- data.frame(\n  x = 1:n,\n  y = LETTERS,\n  z = sample(c(0,1,NA), n, replace = TRUE)\n)\ndbWriteTable(con, \"test_tbl\", test_df)"
  },
  {
    "objectID": "post/sticker-driven-maintenance/index.html",
    "href": "post/sticker-driven-maintenance/index.html",
    "title": "Sticker-driven maintenance",
    "section": "",
    "text": "One of the joys of the open-source community is not only learning from excellent developers but also from stellar community organizers. This time of the year, one initiative I particularly love is Hacktoberfest, a global month-long “hackathon” sponsored by GitHub and DigitalOcean. Hacktoberfest encourages first-time contributors (or anyone who really wants a T-shirt) to make pull requests throughout the month of October, provides on-ramps and Git training for new developers, and emphasizes the primacy of contributing to existing projects over starting new work.\nAs a hackathon organizer, this focus on maintenance is something I find particularly inspiring. Last spring, at our virtual Chicago R Collaborative, my favorite project was one led by James Lamb to help ensure a wide variety of CRAN packages that depend on {httr} were using best practices. This project demonstrated how a short hackathon sprint can create real, lasting value by focusing on maintenance instead of starting a brand new, exciting project with no clear path for ongoing development after the event has ended. This led me to think more critically about how to mobilize maintenance work in a gamified setting (“how many repos can we fix?”) and a incisive, narrow focus.\nBroadly speaking, maintenance is more important than innovation, and I love to see it in the spotlight. However, in open source and InnerSource, both human nature and incentive structures make it more challenging to motivate and invest in maintenance. There’s often more joy in dreaming up the first mile of a project than powering through the last, despite the fact that, without maintenance and sustenance, this surge of innovation is not valuable.\nInspired by Hacktoberfest, I recently started thinking about other whimsical, seasonally-themed events that a community or organization could host to intentionally create time, space, skills, and enthusiasm around maintaining quality code. My goal was to think of fun, engaging, and narrowly focused themes for events so that any necessary training could be supplied and real value could be produced within the span of a 1-2 day event.\nIn the R community, we joke about “sticker-driven development”1 so why not “sticker-driven maintenance”? I threw together a set of seasonally-themes logos or laptop stickers that could be used for event marketing and given to participants. I show previews below, and the full collection (as both png and svg) can be found on GitHub where anyone is welcome to use or modify them.\nThere are two main themes: those with a heavy grid/gradient design in vague homage to the notorious GitHub activity grid and “other random things I though of”. I’ll be the first to admit that these are quite kitschy – and intentionally so. When building communities and especially when trying to motivate some of the less glamorous work of development, I’ve often found that a touch of irrational exuberance on the part of the organizer can go a long way. Currently, this set is largely biased towards US holidays that I’m most familiar with. I’m most proficient at bad puns related to my own experiences, and it also seems most appropriate for me to riff off of those. I’d certainly welcome ideas or contributions for a broader set.\nAnd yes, is there a slight irony that this introduction amount to: “Maintenance is more important than innovation, so I made a bunch of random new stuff, and I leave you with the hard part of turning my silly graphic into a productive event”? Probably. I really don’t have a good answer for that. We’re all susceptible to the allure of novelty.2"
  },
  {
    "objectID": "post/sticker-driven-maintenance/index.html#designs",
    "href": "post/sticker-driven-maintenance/index.html#designs",
    "title": "Sticker-driven maintenance",
    "section": "Designs",
    "text": "Designs\n\nOctober\nAre there known bugs or issues in your org’s repos that could use a patch? A pumpkin patch, perhaps? Both of these designs are inspired by the GitHub activity board; the latter is more minimal but also (on beta testing) was not obvious to anyone but me that it was pumpkins.\n\n\n\n\nNovember\nIt’s time for the fall harvest! If your team’s work isn’t on GitHub or is on GitHub but not optimized for discoverability (in an org; consistently named; has a README, Description, tags, etc.), “harvest” them off your hard-drive and centralize them for reuse.\n\n\n\n\nDecember\nDeck the halls – with good documentation! We all have code or data that could use a bit better documentation, so take some time to work on this collaboratively. Think you’ve got all the boxes check on documentation? Think again. Consider if you have the best documentation for different audiences (users versus developers3), different formats (written or visual diagrams), and different time durations (I’m a big fan of setting up immutable documentation). There’s bound to be something to improve.\n\n\nAlternatively, your code “naughty or nice”? Santa makes his list and checks it twice; all you have to do is add unit tests and run them once.\nFor R users, this could be a great time to learn about testing packages such as {testthat} or {tinytest} or related packages like {assertr} or {pointblank} for a variety of static or live testing and checks.\n\n\n\nJanuary\nMuch like hackathons tend to be biased towards novelty, they are more broadly also focus on doing instead of thinking. In January, we have a new year, new resolutions, and an old and mature toolkit. Gather around a whiteboard as a team without building a thing to reassess priorities and simply make a plan. Think about what work needs to be done and how the tools and packages that you already have can be re-used instead of reinvented.\n\n\n\nFebruary\nBeing an open-source maintainer is a long-term relationship, and many open souce tools lack that “special someone” to support them when they break and help them to be their best. When codebases have few contributors or maintainers, they are more susceptible to going stale. Take the opportunity to do some “matchmaking” between orphaned packages and potential maintainers or, alternatively, help new maintainers get into relationships with existing projects.\nThis GitHub-esque grid asks, will you “Be Mine”? Or alternatively, a candy heart cuts right to the chase as asks to be maintained.\n\n\n\n\nMarch\nSee a tool that doesn’t quite meet your need? Work on an enhancement instead of starting a new project.\n\n\n\n\nApril\nAs Ben Franklin once said, “Nothing can be said to be certain, except death and taxes and technical debt” – or something like that. Tech debt is not an inherently bad thing. When it is taken on conscientiously, it can be an investment in the future – much like a car or house loan. However, just like those loans, it’s an investment that needs to get paid down. Take some time to identify and pay down the accumulated debt in your projects.\nThis could include a wide range of activities such as code refactoring or fixing code style. For R users, in particular, it could be a good time to break out the {lintr} or {styler} packages. Formatting isn’t generally high on the list of tech debt, but it can impede onboarding new people to your project and compounds over time since poorly formatted code doesn’t model good behavior for contributors. It’s also a relatively manageable issue to tackle during a short event.\n\n\n\nJuly\nDependencies are great for quick prototyping, but when you’re planning to launch or maintain a long-term project, there are a number of reasons you might want to ccut them back. Re-evaluate and potentially reduce dependencies in your project.\nFor R users, the {pkgdepends} R package is a great resource here."
  },
  {
    "objectID": "post/sticker-driven-maintenance/index.html#footnotes",
    "href": "post/sticker-driven-maintenance/index.html#footnotes",
    "title": "Sticker-driven maintenance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn which a developer might hypothetically create a new package simply to justify the existence of a stunning new hex sticker↩︎\nIn my defense, and in way of thanks, all logos were made with Inkscape, a great FOSS software with a very easy on-ramps. This was a weekend project mostly motivated by my needing to learn a touch of Inkscape basics, so I took this on as a use case. If you want to get hands-on to edit my designs or to make your own, I recommend the YouTube tutorials from Logos by Nick.↩︎\nI generally believe that most R packages have a significant underinvestment in developer-facing documentation: https://twitter.com/EmilyRiederer/status/1266431103948206080?s=20↩︎"
  },
  {
    "objectID": "post/team-of-packages/index.html",
    "href": "post/team-of-packages/index.html",
    "title": "Building a team of internal R packages",
    "section": "",
    "text": "Note: this post is a written version of my rstudio::global 2020 talk on the same topic. Please see the link for the slides and video version. I do elaborate on a few points here that I cut from the talk; if you’ve already watched the talk and just want the delta, please see the sections in blue\nMore and more organizations are beginning to write their own internal R packages. These internal tools have great potential to improve an organization’s code quality, promote reproducible analysis frameworks, and enhance knowledge management. Developers of such tools are often inspired by their favorite open-source tools and, consequently, rely on the design patterns and best practices they have observed. Although this is a good starting position, internal packages have unique challenges (such as a smaller developer community) and opportunities (such as an intimate understanding of the problem space and over-arching organizational goals). Internal packages can realize their full potential by engineering to embrace these unique features. In this post, I explore the jobs of internal packages and the types of different design decisions these jobs can inspire – from API design and error handling to documentation to training and testing. If you’d rather just read the main ideas instead of the full essay, skip to the tl;dr"
  },
  {
    "objectID": "post/team-of-packages/index.html#what-differentiates-internal-packages",
    "href": "post/team-of-packages/index.html#what-differentiates-internal-packages",
    "title": "Building a team of internal R packages",
    "section": "What differentiates internal packages",
    "text": "What differentiates internal packages\nTo begin, think about the last time that you joined a new organization. There was so much you had to learn before you could get started – accessing data, intuiting what problems are worth tackling, understanding team norms, and so much more. Thankfully, we only have to descend this learning curve once. However, the off-the-shelf tools we use can’t preserve this context and culture. Every day is like their first day at work.\n\n\n\n\n\n\n\n\n\nIn open-source, this high level of abstraction is a feature, not a bug. Stripping context from code enables reuse and collaboration across the globe. The survival package, for example, need not care whether one is modeling the time-to-death in a clinical trial or the time-to-attrition of a Netflix subscriber. In contrast, internal packages can drive additional value if they act more like a colleague and embrace institutional knowledge.\nContrasting internal and external packages on two dimensions illuminates this difference.\nFirst, internal packages can target more domain-specific and concrete problems than open-source packages. Inherently, within an institution, such tools can have a narrower problem definition without loss of useful generality. This means our functions can be more tailored to the contours and corner cases of our problems and our documentation can use more relevant examples.\nSecond, and perhaps less intuitively, internal packages can actually span a broader solution space, as measured by the number of steps in the workflow that they span. For example, an open source package might offer many different methods to model time-to-event but remain agnostic to where the input data came from or what the analyst does with the results. In contrast, an internal package might cater to answering a narrower question but apply its insight into our organization to cover more of the steps in the workflow of answering that questions – such as pulling data, engineering features, and communicating outcomes.\n\nBecause of these two factors, internal packages can make large contributions to our organization by addressing different needs such as:\n\nUtilities: Providing an abstraction layer over internal infrastructure, such as connecting to databases, APIs, servers or enabling proxies\nAnalysis: Guiding analysts through a curated set of methods and packages to answer common questions\nDeveloper Tools: Helping analysts produce better outputs faster with building blocks like ggplot themes, R Markdown templates, and Shiny modules\n\nBut it’s not just what internal packages do that distinguishes them from open-source tools. Even more critical is how these packages are designed to do these tasks. In the rest of this post, I’ll illustrate how internal packages can be conscientiously engineered to act more like a veteran than a new hire."
  },
  {
    "objectID": "post/team-of-packages/index.html#the-theory-of-jobs-to-be-done",
    "href": "post/team-of-packages/index.html#the-theory-of-jobs-to-be-done",
    "title": "Building a team of internal R packages",
    "section": "The theory of jobs-to-be-done",
    "text": "The theory of jobs-to-be-done\nTo motivate the design goals and decisions we’ll discuss, it’s useful to think about Harvard Business School Professor Clayton Christensen’s jobs-to-be-done theory of disruptive innovation. This asserts that\n\nwe hire a product to do a job that helps us make progress towards a goal\n\nand sometimes1 the theory further asserts that\n\nthese jobs can have functional, social, and emotional components\n\n\nIn the world of product development, that focus on the progress a customer is a critical distinction from the conventional understanding of competition and industry makeup. For example, if I ask you who Twitter’s competitors are, you might first think of Facebook, Instagram, or TikTok – other entrants in the category of “mobile apps for social networking”. However, if we think about the jobs that I “hire” Twitter to do, I might “hire” Twitter to help me pass the time while I wait in a long line or (back in “the olden days”) ride my long commute to work. Through that lens, it not longer matters what industry Twitter is nominally in; this is about me and my needs - not Twitter. So, Twitter is “competing” for the position of my travel companion against all sorts of other potential hires like Spotify, podcasts, books, or even an extra 20 minutes nap on the train.\nIn relation to R packages, open source packages know a task we want done (e.g. “fit a Cox proportional hazard model”) which is somewhat like a product category. But what really sets internal packages apart is that they can use their knowledge of our organization to cater to the kind of progress that we truly want to make.\nIt’s hard for passionate R users2 to imagine, but no one actually wants a package. I’d even go as far to say that no one even wants data (blasphemy!). Organizations need progress – they need strategies, fueled by decisions, fueled by answers to questions, and fueled, in turn and in part, by data sets and tools. Since internal tools have a better sense of what that key bit of progress is, they can be more focused on helping us get there.\n\nI claim that we can make internal tools more useful and more appealing by targeting them towards the specific jobs of an organization. To complete the analogy, I’ll restate the initial theory with a few alterations\n\nlet’s build a team of packages that can do the jobs that help our organization make answer impactful questions with efficient workflows\n\nSo how does jobs-to-be-done inform our package design? To explore this, we’ll consider what makes good teammates and how we can encode those traits in our tools."
  },
  {
    "objectID": "post/team-of-packages/index.html#the-it-guy---abstraction",
    "href": "post/team-of-packages/index.html#the-it-guy---abstraction",
    "title": "Building a team of internal R packages",
    "section": "The IT Guy - Abstraction",
    "text": "The IT Guy - Abstraction\n\n\n\n\n\n\n\n\n\nFirst, let’s meet the IT Guy. IT and DevOps colleagues complement the skills of data analysts by handling all of the things that analysts are generally not the most skilled or experienced at doing independently – production systems, servers, deployments, etc. In that abstraction process, they also take on additional responsibilities to promote good practices like data security and credential management. Ideally, they can save us time and frustration in navigating organization-specific roadblocks than no amount of searching on StackOverflow can help.\nPut another way, the IT Guy fills the jobs of\n\nFunctional: Abstracting away confusing quirks of infrastructure\nSocial: Promoting or enforcing good practices\nEmotional: Avoiding frustrating or stress of lost time or “silly” questions\n\nWe can emulate these characteristics in internal packages by including utility functions, taking an opinionated stance on design, and providing helpful and detailed error messages.3\nAs an example, let’s consider a simple function to connect to an organization’s internal database.4 First, we might start out with a rather boilerplate piece of code using the DBI package. We take in the username and password; hard-code the driver name, server location, and port; and return a connection object. (Note that the content of this example doesn’t matter. The jobs of the IT Guy prototype are abstracting away things we don’t need to think about and protecting us from anti-patterns – not just connecting to databases. The design patterns shown will apply to the more general problem.)\n\nget_database_conn &lt;- function(username, password) {\n\nconn &lt;-\n  DBI::dbConnect(\n    drv = odbc::odbc(),\n      driver = \"driver name here\",\n      server = \"server string here\",\n      UID = username,\n      PWD = password,\n      port = \"port number here\"\n  )\n\nreturn(conn)\n\n}\n\n\nOpinionated Design\nNow, let’s suppose our organization has strict rules against putting secure credentials in plain text. (Let’s actually hope they have such rules!) To enforce this, we can remove username and password from the function header, and use Sys.getenv() inside of the function to retrieve specifically named environment variables (DB_USER and DB_PASS) containing these quantities.5\nIn an open source package, I would not presume to force users’ hand to use one specific system set-up. However, in this case, we can make strong assumptions based on our knowledge of an organization’s rules and norms. And this sort of function can be great leverage to incentivize users to do it right (like storing their credentials in environment variables) because there’s only one way it can work.\n\nget_database_conn &lt;- function() {\n\nconn &lt;-\n  DBI::dbConnect(\n    drv = odbc::odbc(),\n      driver = \"driver name here\",\n      server = \"server string here\",\n      UID = Sys.getenv(\"DB_USER\"),\n      PWD = Sys.getenv(\"DB_PASS\"),\n      port = \"port number here\"\n  )\n\nreturn(conn)\n\n}\n\n\n\nHelpful Error Messages\nOf course, opinionated design is only helpful if we communicate those opinions to our users. Otherwise, they’ll get an error that DB_PASS is missing and find nothing useful online to help them troubleshoot since this is a specific internal choice that we made.\nSo, we can enhance the function with extremely custom and prescriptive error messages explaining what went wrong and either how to fix it (e.g. setting an environment variable, applying for an access) or where one can get more information (in a vignette, asking someone on a specific support team, checking out a specific document in a team wiki).\n\nSuch support messages can be made even more approachable with the use of the cli package which makes console messages more aesthetic, user-friendly, and clear in intent.\n\n\nget_database_conn &lt;- function() {\n\nif (any(Sys.getenv(c(\"DB_USER\", \"DB_PASS\")) == \"\")) {\n  stop(\n    \"DB_USER or DB_PASS environment variables are missing.\",\n    \"Please read set-up vignette to configure your system.\"\n  ) \n}\n\nconn &lt;-\n  DBI::dbConnect(\n    drv = odbc::odbc(),\n      driver = \"driver name here\",\n      server = \"server string here\",\n      UID = Sys.getenv(\"DB_USER\"),\n      PWD = Sys.getenv(\"DB_PASS\"),\n      port = \"port number here\"\n  )\n\nreturn(conn)\n\n}\n\n\n\nProactive Problem Solving\nOf course, even better than explaining errors is preventing them from occurring. We might also know at our specific organization, non alphanumeric characters are required in passwords and that DBI::dbConnect() does not natively encode these correctly when passing them to the database. Instead of troubling the users and telling them how to pick a password, we can instead handle this silently by running the password retrieved from the DB_PASS environment variable through the URLencode() function.\n\nget_database_conn &lt;- function() {\n\nif (any(Sys.getenv(c(\"DB_USER\", \"DB_PASS\")) == \"\")) {\n  stop(\n    \"DB_USER or DB_PASS environment variables are missing.\",\n    \"Please read set-up vignette to configure your system.\"\n  ) \n}\n\nconn &lt;-\n  DBI::dbConnect(\n    drv = odbc::odbc(),\n      driver = \"driver name here\",\n      server = \"server string here\",\n      UID = Sys.getenv(\"DB_USER\"),\n      PWD = URLencode(Sys.getenv(\"DB_PASS\"), reserved = TRUE),\n      port = \"port number here\"\n  )\n\nreturn(conn)\n\n}\n\n\nAt this point, you might be wondering: “Isn’t proactive troubleshooting and helpful error messages good standard practice?” And, yes, these are things that both external and internal packages should strive for. The differences with internal R packages are that you have a tighter feedback loop with your users and their systems. This has two advantages.\nFirst, you’ll be quicker to learn about the spectrum of issues your users are encountering (trust me, they’ll find you!), and proactively explaining or solving those issues in the package saves you both a lot of time.\nSecondly, in a constrained environment, there are more predictable ways things can go wrong, so you attempts at proactive problem solving and helpful error messages have a better chance at spanning the spectrum of commonly occurring problems."
  },
  {
    "objectID": "post/team-of-packages/index.html#the-junior-analyst---proactive-problem-solving",
    "href": "post/team-of-packages/index.html#the-junior-analyst---proactive-problem-solving",
    "title": "Building a team of internal R packages",
    "section": "The Junior Analyst - Proactive Problem-Solving",
    "text": "The Junior Analyst - Proactive Problem-Solving\n\n\n\n\n\n\n\n\n\nOf course, strong opinions and complete independence don’t always make a great teammate. Other times, we might want someone more like a junior analyst. They know a good bit about our organization, and we can trust them to execute calculations correctly and make reasonable assumptions. At the same time, we want them to be responsive to feedback and willing to try out things in more ways than one.\nMore concretely, a Junior Analyst fills the jobs:\n\nFunctional: Performs rote tasks effectively and makes reasonable assumptions where needed\nSocial: Communicates assumptions and eagerly adapts to feedback\nEmotional: Builds trust so you can focus on other things\n\nTo capture these jobs in our package, we can build proactive yet responsive functions by using default arguments, reserved keywords, and the ellipsis.\nTo illustrate, now let’s imagine a basic visualization function that wraps ggplot2 code but allows users to input their preferred x-axis, y-axis and grouping variables to draw cohort curves. (Again, note that the content of this example doesn’t matter. The jobs of the junior analyst prototype are performing rote tasks efficiently and flexibly – not visualizing cohort curves, per say. The design patterns shown will apply to the general problem of assisting in analysis without overbearing assumptions.)\n\nviz_cohort &lt;- function(data, time, metric, group) {\n\n  gg &lt;- \n    ggplot(data) +\n    aes(x = .data[[time]], \n        y = .data[[metric]],\n        group = .data[[group]]) +\n    geom_line() +\n    my_org_theme()\n\n  return(gg)\n\n}\n\nThis function is alright, but we can probably draw on institutional knowledge to make our junior analyst a bit more proactive.\n\nDefault Function Arguments\nIf we relied on the same opinionated design as the IT Guy, we might consider hard-coding some of the variables inside of the function. Here, though, that is not a great approach. The junior analyst’s job is not to give edicts. We might know what the desired x-axis will be 80% of the time, but hard-coding here is a too-strong assumption even for an internal package and decreases the usefulness of the function in the other 20% of applications.\n\nviz_cohort &lt;- function(data, metric, group) {\n\n  gg &lt;- \n    ggplot(data) +\n    aes(x = .data[[\"MONTHS_SUBSCRIBED\"]], \n        y = .data[[metric]],\n        group = .data[[group]]) +\n    geom_line() +\n    my_org_theme()\n\n  return(gg)\n\n}\n\nInstead, we can put our best-guess 80-percent-right names as the default arguments in the function header – ordered by decreasing likelihood to override. This means, when users do not provide their own value, a default value is used (that’s the junior analyst’s best-guess!), but users retain complete control to change it as they see fit.\n\nviz_cohort &lt;- function(data, \n                       metric = \"IND_ACTIVE\",\n                       time = \"MONTHS_SUBSCRIBED\", \n                       group = \"COHORT\") {\n\n  gg &lt;- \n    ggplot(data) +\n    aes(x = .data[[time]], \n        y = .data[[metric]],\n        group = .data[[group]]) +\n    geom_line() +\n    my_org_theme()\n\n  return(gg)\n\n}\n\n\n\nReserved Keywords\nThis approach becomes even more powerful if we can abstract out a small set of commonly occuring assumed variable names or other values. Then, we can define and document a set of “keywords” or special variable names that span all of our internal packages. Following the example above, we might define a handful of reserved keywords such as TIME_SUBSCRIBED, CUSTOMER_COHORT, and CUSTOMER_SEGMENT.\nIf these are well-known and well-documented, users will then get into the habit of shaping their data so it “plays nice” with the package ecosystem and save a lot of manual typing. This type of “incentive” to standardize field names can have other convenient consequences in making data extracts more sharable and code more readable.\n\nIn the spirit of the IT Guy’s proactive problem-solving, we can also help users catch potential errors caused by missing keywords. A few strategies here are to provide a function like validate_{package name}_data() to check that any required names exist or report out what is missing or to provide similar on-the-fly validation inside of functions that expect reserved keyword variable names to exist.\n\nvalidate_mypackage_data &lt;- function(vbl_names) {\n  \n  # validate variable names ----\n  required &lt;- c(\"TIME_SUBSCRIBED\", \"CUSTOMER_COHORT\", \"CUSTOMER_SEGMENT\")\n  not_present &lt;- setdiff(required, vbl_names)\n  \n  # report missing / required names ----\n  if (length(not_present) == 0) {return(TRUE)}\n  message(\"The following required variable(s) are missing: \", \n          paste(not_present, collapse = \", \"))\n  return(FALSE)\n  \n}\n\n(An alternative approach to reserved keywords is to let users specify important variables by using package options(). However, I do not prefer this approach because it does not have the benefits of driving consistency, and it requires more documentation and R knowledge. Most any user can grasp “)\n\n\n\nEllipsis\nFinally, one other nice trick in making our functions responsive to feedback is the ellipsis (..., also called “passing the dots”). This allows users to provide any number of additional, arbitrary arguments beyond what was specified by the developer and to plug them in at a designated place in the function body.\n\nviz_cohort &lt;- function(data, \n                       time = \"MONTHS_SUBSCRIBED\", \n                       metric = \"IND_ACTIVE\",\n                       group = \"COHORT\",\n                       ...) {\n\n  gg &lt;- \n    ggplot(data) +\n    aes(x = .data[[time]], \n        y = .data[[metric]],\n        group = .data[[group]]) +\n    geom_line(aes(...)) +\n    my_org_theme()\n\n  return(gg)\n\n}\n\nThis way, users can extend functions based on needs that the developer could not have anticipated, like customizing the color, size, and line type with commands such as:\n\nviz_cohort(my_org_data)\nviz_cohort(my_org_data, color = COHORT, linetype = COHORT)\n\n\nNote that the benefits of the ellipsis are very similar to the benefits of functional programming, in general, with small, modular, and composable functions that do not preserve state. This also provides users flexibility because they can continue to add on to our functions from the outside without having to modify the internals. For example, if users wanted to make our plot separate by the quarter that different cohorts began, they could simply create an additional column with this information before calling our function and add a facet_grid() call outside of viz_cohort().\n\nmy_org_data %&gt;%\n  mutate(COHORT_QUARTER = quarter(DATE_COHORT_START)) %&gt;%\n  viz_cohort(my_org_data) + \n  facet_grid(rows = vars(COHORT_QUARTER))"
  },
  {
    "objectID": "post/team-of-packages/index.html#the-tech-lead---knowledge-management",
    "href": "post/team-of-packages/index.html#the-tech-lead---knowledge-management",
    "title": "Building a team of internal R packages",
    "section": "The Tech Lead - Knowledge Management",
    "text": "The Tech Lead - Knowledge Management\nSo far, we have mostly focused on that first dimension of differentiation between internal and open-source tools – making our package teammates targeted to solving specific internal problems. But there’s just as much value in that second dimension: using internal packages as a way to ship not just calculations but workflows and share an understanding of how the broader organization operates. This allows our packages to play leadership and mentorship roles in our org. To illustrate this, consider our intrepid tech lead.\n\n\n\n\n\n\n\n\n\nWe value this type of teammate because they can draw from a breadth of past experience and institutional knowledge to help you weigh trade-offs, learn from collected wisdom, and inspire you to do your best work.\n\nFunctional: Coach you through weighing alternatives and debugging issues based on vast experience\nSocial: Shares collected “tribal” knowledge\nEmotional: Breaks down barriers to help you do your best work\n\nOf course, all of that is a pretty tall order for a humble R package! But conscientious use of vignettes, R Markdown templates, and project templates can do a lot to help us toward this goal.\n\nVignettes\nVignettes often help introduce the basic functionality of a package with a toy example, as found in the dplyr vignette, or less commonly may discuss a statistical method that’s implemented, as done in the survival package. Vignettes of internal R packages can do more diverse and numerous jobs by tackling comprehensive knowledge management. These vignettes can accumulate the hard-won experience and domain knowledge like an experienced tech leads’ photographic memory and hand-deliver them to anyone currently working on a related analysis.\nJust as a few examples, vignettes of an internal package might cover some of the following topics:\n\nConceptual Overview: Explain the problem(s) the package is trying to solve and how they help the organization make progress\nWorkflow: Describe the workflow for answering the question For example, does it require data collection or use of existing data? what sort of models will be employed? what are the steps in in what order?\nKey Questions: What sort of questions should an analyst be asking themselves throughout the analysis? What assumptions are the methods making that they should verify? What are common “gotchas”?\nProcess Documentation: What are organizational challenges to this problem (as opposed to quantiative ones)? What other systems might someone have to set up or teams to contact (e.g. if need to deploy a model or add features to a library)?\nTechnical Documentation: How does the conceptual workflow map to different functions in the package? What sort of inputs do these functions require and what sort of outputs do they produce? Where should these inputs and outputs be stored?\nMethod Comparison: When the package offers different options for solving specific problems, how should users evaluate these options and decide which is best? Are there quantitative measures? Examples from past experience?\nLessons Learned: What has gone wrong when solving this type of problem before? What can be done better?\nPast Examples: Previous case studies of successful use of the package and the organizational progress it drove\n\n\nThese examples of potential vignettes further illustrate how our internal packages can help us better achieve progress. No one in an organization wants a model for the sake of having a model; they want is to answer a question or suggest an action. Open-source packages are relatively “blind” to this, so their vignettes will focus on getting to the model; internal packages can partner with us in understanding that “moment before” and “moment after” with tips on everything from featuring engineering to assessing the ethics and bias in a certain output.\nOf course, any organization can document their processes, and this documentation doesn’t strictly have to live inside an R package. However, many organizations struggle with knowledge management. Including documentation in a related R package ensures that it’s easily discoverable to the right people (those doing a related project), at the right time (when they are working on that project). It also ensures that documentation won’t be lost when any one person changes teams, leaves the organization, or gets a new computer.\n\n\n\nPackage Websites\nIn fact, all of that context may be so helpful, even people who are not direct users of your package may wish to seek its mentorship.\nIn this case, you can use the pkgdown package to automatically (honestly, you just run pkgdown::build_site()) create a package website to share these vignettes with anyone who needs to learn about a specific problem space – whether or not they will ultimately be the data analyst using your package to solve the computational part of this quest for organizational progress. And, unlike their human counterparts, the packaged tech lead can always find time for another meeting.\n\n\n\n\n\n\n\n\n\nLike other R Markdown output types, pkgdown creates basic HTML output. This means that you can host it on an internal server or (if you organization uses it) GitHub Pages in GitHub enterprise. Alternatively, you may even simply store the HTML files in an internal shared drive and any users may open the site on their own computer in their favorite browser.\n\n\nR Markdown Templates\nSimilar to vignettes, embedded R Markdown templates take on a more important and distinctive role for internal packages.\nTemplates are a way to include prepopulated R Markdowns within your package, which users may access through File &gt; New File &gt; R Markdown &gt; From Template in the RStudio IDE or with the rmarkdown::draft() function. In open-source packages, R Markdown templates provide a pre-populated file instead of the default. This is most commonly used to demonstrate proper formatting syntax. For example, the flexdashboard package use a template to show users how to set up their YAML metadata and section headers.\nInstead, internal packages can use templates to coach users through workflows because they understand the problem users are facing and the progress they hope to achieve. Internal packages can mentor users and structure their work in two different ways:\nProcess walk-throughs can serve as interactive notebooks that “coach” users through common analyses. As an example, if a type of analysis requires manual data cleaning and curation, a template notebook could guide users to ask the right questions of their data and generate the common views they need to analyze.\nWe can also include full end-to-end analysis outlines which include placeholder text, commentary, and code if the type of analysis that a package supports usually results in a specific report.\n\n\nProject Templates\nSimilarly, our package can include RStudio project templates. These templates can predefine a standard file-structure and boilerplate set of files for a new project to give users a helping hand and drive the kind of consistency across projects that any tech lead dreams of when doing a code review. When any package user is beginning a new analysis, they may click File &gt; New Project in RStudio and select your custom project template among a number of options in the IDE.\nFor example, one flexible file structure might be something like:\n\nanalysis: RMarkdown files that constitute the final narrative output\nsrc: R scripts that contain useful helper functions or other set-up tasks (e.g. data pulls)\ndata: Raw data - this folder should be considered “read only”!\noutput: Intermediate data objects created in the analysis\ndoc: Any long form documentation or set-up instructions\next: Any miscellaneous external files or presentation material collected or created throughout the analysis\n\nAdditionally, for a given problem that our package attempts to solve, we could pre-populate this file structure with certain guaranteed files such as template scripts for querying and accessing data, configuration files, some of the template R Markdowns we mentions above, etc. The more that directories and files are standardized for a given problem type, the easier it is for anyone else in our organization that has previously used our tool to easily understand our project.\nThe idea of a standardized file structure is one of the 10 principles called out in the wonderful Good Enough Practices for Scientific Computing and potentially one of the single highest leverage practices I’ve found for driving consistency and preserving sanity when building collaboratively on large teams."
  },
  {
    "objectID": "post/team-of-packages/index.html#the-project-manager---coordination",
    "href": "post/team-of-packages/index.html#the-project-manager---coordination",
    "title": "Building a team of internal R packages",
    "section": "The Project Manager - Coordination",
    "text": "The Project Manager - Coordination\n\n\n\n\n\n\n\n\n\nNow, we discussed how a package filling the “tech lead” role can help structure our work with templates and how this can help collaboration between R users. But to think about collaboration more broadly, we must turn to our final teammate – the project manager.\nOne of the biggest differences between task-doing open-source packages versus problem-solving internal packages is understanding the whole workflow and helping coordinate projects across different components.\nIn short, a package can help project manage by addressing the jobs of\n\nFunctional: Integrate work across colleagues and components\nSocial: Help mediate conflict and find common ground\nEmotional: Meet individual teammates where they are at and remove barriers\n\nSpecifically, when writing open-source packages, we rightly tend to assume our target audience is R users, but on a true cross-functional team not everyone will be, so we can intentionally modularize the workflow, define specific tasks, and augment RStudio’s IDE to make sure our tools work well with all of our colleagues.\n\nModularization\nModularizing allows us to isolate parts of our workflow that do not really require R code to provide a lower barrier of entry for more of our colleagues to contribute.\nFor example, in the templates we just discussed, we could actually make separate template components for parts that require R code and for commentary. The commentary files could be plain vanilla markdown files that any collaborator could edit without even having R installed, and main R Markdown file can pull in this plaintext output using child documents. This approach is made even easier with advances in the RStudio IDE like the visual markdown editor provides great, graphical UI support for word processing in markdown.\n\n\nProject Planning\n\nPieces of code are not the only thing we can modularize for our users. Often, a good bit of work in the early stages of a project is figuring out what the right steps are. This is emphasized in the project management book Shape Up which illustrates the flow of a project with “hill diagrams”:\n\nOur “tech lead” documentation can already help document some of the learned project steps and help speed up the “hill climbing”. Additionally, a package could play the role of the project manager and make those steps more tangible by embedding a project plan. One option for this is the projmgr package6 which allows for specification of main steps as a project as in a YAML file which can then be bulk uploaded to GitHub issues and milestones.\nTechnically, this would mean writing a project plan, saving it in the inst/ folder of your package, and perhaps writing a custom function to wrap system.file() to help users access the plan. Your function might look something like:\n\nretrieve_{package name}_plan &lt;- function(proj_type = c(\"a\", \"b\", \"c\")) {\n  \n  match.arg(proj_type)\n  system.file(\"plan.yml\", package = \"{package name}\")\n  \n}\n\nAlternatively, you could automate similar project plans yourself using the public APIs for Jira, Trello, GitLab, or whatever project management solution your organization uses.\n\n\n\nIDE Support\nWe can also use RStudio Add-Ins to extend RStudio interface and ship interactive widgets (imagine writing a “baby Shiny app”) in our internal packages. For example, the esquisse package uses add-ins to ship a point-and-click plot coding assistant. Add-ins should be used judiciously because they require more investment upfront, but they are much easier to maintain than a full application and can help slowly convert more teammates to R users over time. Besides, a good project manager is willing to go the extra mile to support their team.\nOne other way to “go the extra mile” could also be to include an interactive learnr tutorial as another way to help onboard new users. These tutorials are now also viewable through the RStudio IDE’s Tutorial pane."
  },
  {
    "objectID": "post/team-of-packages/index.html#collaboration",
    "href": "post/team-of-packages/index.html#collaboration",
    "title": "Building a team of internal R packages",
    "section": "Collaboration",
    "text": "Collaboration\nNow, speaking of collaboration, we’ve talked about how packages can act like team members such as the IT Guy, Analyst, Tech Lead, or Project Manager. But being a good teammate is about more than doing individual jobs; organizational progress is made by working together. Another major opportunity when building a suite of internal tools is that we have the unique opportunity to think about how multiple packages on our team can interact most effectively.\nWe want teammates that are clear communicators, have defined responsibilities, and keep their promises. We can help our packages be good teammates with naming conventions, clearly defined scopes, and careful attention to dependencies and testing.\n\nClear Communication - Naming Conventions\nClear function naming conventions and consistent method signatures help packages effectively communicate with both package and human collaborators.\nInternally, we can give our suite of internal packages a richer language by define a consistent set of prefix name stubs that indicate how each function is used. One approach is that each function prefix can denote the type of object being returned (like “viz” for a ggplot object).7\n\nThis sort of convention can reduce the cognitive load when working with any packages; it’s not particularly unique to internal tools. However, while the practice is general, the benefits scale significantly for internal packages. Since we can drive this consistent standard across a collection of packages, past experience working with any one of our internal packages gives a user intuition when working with the next.\n\n\nDefined Roles - Curation\nAnother aspect of good team communication is having clearly defined roles and responsibilities. Again, since we own our whole internal stack, we have more freedom in how we chose to divide functionality across packages.\nOpen source packages inevitably have overlapping functionality which forces users to compare alternatives and decide which is best. The need to curate all of the potentially useful tools creates a meta-job for analysts of researching and weighing alternatives, seeking their colleague’s advice, and piecing disparate pieces of a pipeline together.\nBut internally, we can use some amount of central planning to ensure each package teammate has a clearly defined role – whether that be to provide a horizontal utility or to enable progress on a specific workstream. And just like one team ought to work well with another, that central planning can include curation and promotion of open-source tools that “play nicely” with our internal ones. After all, no one team can go it alone!\n\n\nDefinded Roles - Dependencies\nWhen assigning these roles and responsibilities to our team of packages, we should consider how to manage the dependencies between them when different functionality needs to be shared.\nPackages often have direct dependencies where a function in one package calls a function in another. This is not necessarily bad, but especially with internal packages which might sometimes have a shorter shelf-life and few developers, this can create a domino effect. If one package is deprecated (or decides to retire or take a vacation), we don’t want the rest of our ecosystem affected.\nAlternative, we can use the fact that both packages A and B are under our control and see if we can eliminate explicit dependencies by promoting a clean hand-off. We can see if a function in A can produce an output that B can consume instead of directly calling A’s function internally.\nAdditionally, because we own the full stack, we may also consider if there are shared needs in A and B that should be be extracted into a common “building block” package C. For instance, a set of common visualization function “primitives”. This was, we at least have a clear hierarchy of dependencies instead of a spiderweb and can identify a small number of truly essential ones.\n\n\n\nDelivering Reliably - Testing\nRegardless of the type of dependencies we end up with, we can use tests to make sure that our packages are reliable teammates who do what they promised.\nTypically, if I write a package B that depends on package A, I can only control package B’s tests, so I could write tests to see if A continues to perform as B is expecting.\n\n\n\n\n\n\n\n\n\nThis is a good safeguard, but it means that we will only detect problems after they have already been introduced in A. There’s nothing in place to actually stop package A from getting distracted in the first place.\n\n\n\n\n\n\n\n\n\nInstead, we’d prefer that both A and B be conscientious of the promises that they have made and stay committed to working towards their shared goals.\nWe can formalize that shared vision with integration tests. That is, we can add tests to both the upstream and downstream packages to ensure that they continue to check in with each other and inquire if any changes that they are planning could be disruptive. Now, just imagine having such rigorous and ongoing communication and prioritization with your actual teammates!\n\n\n\n\n\n\n\n\n\n\nThis type of two-way testing can also be deployed to more robustly test external and cross-platform dependencies. For example, if your internal package requires connects to external resources like APIs or databases, you could even write tests that connect to external resources and verify that certain API endpoints, database schemas, or even columns still exist.\nIn open source packages, we generally employ “mocking” to avoid testing external dependencies since these fail when run by unauthenticated users or programs (such as CRAN checks.) However, once again, internally we can get differential value and safety by making different decisions."
  },
  {
    "objectID": "post/team-of-packages/index.html#final-thoughts",
    "href": "post/team-of-packages/index.html#final-thoughts",
    "title": "Building a team of internal R packages",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIn summary, we all know the the joy of working with a great team, and, if you’ve made it this far, I suspect you know the pleasure of cracking open a new R package. By taking advantage of unique opportunities when designing internal packages, we can truly achieve the best of both worlds. We can share the fun of working with good tools with the teammates we care about, and elevate those same tools to full-fledged teammates by giving them the skills to succeed."
  },
  {
    "objectID": "post/team-of-packages/index.html#resources",
    "href": "post/team-of-packages/index.html#resources",
    "title": "Building a team of internal R packages",
    "section": "Resources",
    "text": "Resources\n\nReady to get started? Check out my posts on:\n\nR Markdown Driven Development to learn the technical side of converting existing analysis scripts into reusable tools and packages\nmy Rtistic project for a template to make your organization’s first package (for themes and color palettes)\nmy round-up of cool examples of R packages in industry.8\n\nFor general resources on building R packages, check out:\n\nthe R Packages book\nMalcolm Barrett and Rich Iannone’s My Organization’s First Package training from rstudio::conf 2020\nHadley Wickham’s Building Tidy Tools training from rstudio::conf 2020\n\nFor opinionated guides to package design, check out:\n\nrOpenSci Packages: Development, Maintenance, and Peer Review\nHow to develop good R packages for open science by Maëlle Salmon"
  },
  {
    "objectID": "post/team-of-packages/index.html#too-long-didnt-read",
    "href": "post/team-of-packages/index.html#too-long-didnt-read",
    "title": "Building a team of internal R packages",
    "section": "Too Long Didn’t Read",
    "text": "Too Long Didn’t Read\nThe following summarizes the key principles and practices discussed in this article.\nAbstraction & Enforcement: In an organization, there are more things that we absolutely know to be true (e.g. the location of a server) or required (e.g. the right way to authenticate.) Thus, for internal tools, sometimes less flexible functions are useful; these don’t require users to input things that will not change or that they may not completely understand (e.g. encryption). So, internal packages could include functions that:\n\nProvide utilities such as database connections objects\nTake an opinionated stance to be more efficient and enforce best practices (by what they chose not to parameterize)\nProvide extremely helpful and prescriptive error messages not only explaining failure but explaining what organizational resources are needed to fix (e.g. perhaps someone needs to access a DBA for access to a schema). Bonus points for using the cli package to make these errors easy to read\nContinually learn from user challenges to silently handle more potential errors and corner cases internally\n\nInformed & Flexible: Because we know our organization’s problem domain, there are some things we probably know to be true 80% of the time. When designing functions, we can make our functions informed enough to save user’s time and typing by handling this 80% of cases more automatically but still providing complete flexibility to override with:\n\nDefault arguments which allow us to provide a likely value for a function argument that is used unless overridden by the user\nReserved keywords that provide at the package or package-ecosystem level a small set of variable names that will be repeatedly used for these defaults\nEllipsis to allow users to insert additional arguments and customize functions in ways we did not foresee\nFunctional programming which, like the ellipsis, makes our functions seamless to extend\n\nProviding Context & Coaching: Internal packages can shared knowledge learned from past experience and nudge users towards better coding standards by include:\n\nVignettes that package the accumulated knowledge about solving a certain type of problem within the bounds of an organization\npkgdown sites to share all of the qualitative information contained in vignettes as part of institutional knowledge management\nR Markdown templates to encode an interactive workflow or provide an analysis template\nR Project templates to drive consistency in how a project using that package is structured\n\nImproving accessibility and coordination: Internal packages can become more integral to how our organization works if they help all of our coworkers engage – even non-R users. Some strategies for this are:\n\nModularizing parts of the workflow that do and do not require programming. One way to do this is with R Markdown child documents\nDefining project plans with projmgr (or other APIs) and automatically publishing them to one’s prefered project management platform\nPromoting use of the RStudio Visual Markdown Editor\nProviding RStudio Add-ins to help new users get some amount of graphical support in performing simple tasks with out package\n\nCollaboration: When we write multiple internal R package, we also have the unique ability to think about how our set of packages can interact well with each other. To fully capitalize on this, we can:\n\nDefine a naming convention to use across all packages and help make “performance guarantees” so knowledge transfers between packages\nCarefully rationalizing the functions in each package to avoid confusing redunancy and promote a curated set of (internal and open) tools\nTackle refactoring across all our packages instead of one-by-one to attempt to unneeded, potentially brittle dependencies\nSet up two-way unit tests (or “integration tests”) where we have dependencies to ensure that the intent of dependent packages doesn’t drift apart"
  },
  {
    "objectID": "post/team-of-packages/index.html#footnotes",
    "href": "post/team-of-packages/index.html#footnotes",
    "title": "Building a team of internal R packages",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMany practitioners have built off of Christensen’s initial theory with sometimes passionately conflicting opinions on the nuances↩︎\nParticularly users interested enough to read this post =)↩︎\nThe usethis package also has many great examples of “IT Guy”-like qualities. One that I don’t discuss in this post is explicitly informing users what it intends to do and then seeking permission↩︎\nNote that any functions I show in the post are psuedo-code and simplified to illustrate my key points.↩︎\nThis is not a recommendation for the best way to store credentials! There are many other options such as keychains and password managers that your organization may prefer. I use a relatively simple example here that I could implement without adding any bulky code or additional packages to illustrate the point of opinionated design - not to give advice on security.↩︎\nDisclaimer, this is a shameless promotion of my own package.↩︎\nrOpenSci has a nice discussion on function naming in their package style guide↩︎\nThis round-up also illustrates some great examples of when you should not use an R package for your organization and when other types of end-products could be more beneficial.↩︎"
  },
  {
    "objectID": "project/dbtplyr/index.html",
    "href": "project/dbtplyr/index.html",
    "title": "dbtplyr",
    "section": "",
    "text": "GitHub Repo\nGitHub Docs\nRelated Talk"
  },
  {
    "objectID": "project/dbtplyr/index.html#quick-links",
    "href": "project/dbtplyr/index.html#quick-links",
    "title": "dbtplyr",
    "section": "",
    "text": "GitHub Repo\nGitHub Docs\nRelated Talk"
  },
  {
    "objectID": "project/dbtplyr/index.html#description",
    "href": "project/dbtplyr/index.html#description",
    "title": "dbtplyr",
    "section": "Description",
    "text": "Description\ndbtdplyr is an dbt add-on package bringing dplyr semantics to SQL. In particular, it mimics dplyr’s across() function and the select helpers to easily apply operations to a set of columns programmatically determined by their names.\ndplyr (&gt;= 1.0.0) has helpful semantics for selecting and applying transformations to variables based on their names. For example, if one wishes to take the sum of all variables with name prefixes of N and the mean of all variables with name prefixes of IND in the dataset mydata, they may write:\nsummarize(\n  mydata, \n  across( starts_with('N'), sum),\n  across( starts_with('IND', mean)\n)\nThis package enables us to similarly write dbt data models with commands like:\n{% cols_n = dbtplyr.starts_with( ref('mydata'), 'N') %}\n{% cols_ind = dbtplyr.starts_with( ref('mydata'), 'IND') %}\n\nselect\n\n  {{ dbtplyr.across(cols_n, \"sum({{var}}) as {{var}}_tot\") }},\n  {{ dbtplyr.across(cols_ind, \"mean({{var}}) as {{var}}_avg\") }}\n\nfrom {{ ref('mydata') }}\nwhich dbt then compiles to standard SQL."
  },
  {
    "objectID": "project/projmgr/index.html",
    "href": "project/projmgr/index.html",
    "title": "projmgr",
    "section": "",
    "text": "GitHub Repo\nGitHub Docs\nCRAN\nRelated Talk\nRelated Post"
  },
  {
    "objectID": "project/projmgr/index.html#quick-links",
    "href": "project/projmgr/index.html#quick-links",
    "title": "projmgr",
    "section": "",
    "text": "GitHub Repo\nGitHub Docs\nCRAN\nRelated Talk\nRelated Post"
  },
  {
    "objectID": "project/projmgr/index.html#description",
    "href": "project/projmgr/index.html#description",
    "title": "projmgr",
    "section": "Description",
    "text": "Description\nprojmgr is an R package which provides an opinionated interface to the GitHub v3 REST API with features focused on improving project management workflows. Many data scientists and software engineers commonly use GitHub for version control, storage, and distribution, and priorities are often motivated by GitHub features and bugs. For these reasons, using GitHub issues and milestones for project management offers numerous benefits over external project management tools like JIRA or Trello.\nFeatures include:\n\nGET-ting and POST-ing issues and milestones to GitHub repositories\nBulk-uploading GitHub issues and milestones from custom YAML templates\nCommunicating progress through custom visualizations and HTML/CSS reports\n\nPlease see package website for more details, including detailed walk-throughs of functionality and persona/use-case driven cookbooks."
  },
  {
    "objectID": "publication/causal-language/index.html",
    "href": "publication/causal-language/index.html",
    "title": "Causal and Associational Language in Observational Health Research: A Systematic Evaluation",
    "section": "",
    "text": "We estimated the degree to which language used in the high-profile medical/public health/epidemiology literature implied causality using language linking exposures to outcomes and action recommendations; examined disconnects between language and recommendations; identified the most common linking phrases; and estimated how strongly linking phrases imply causality. We searched for and screened 1,170 articles from 18 high-profile journals (65 per journal) published from 2010-2019. Based on written framing and systematic guidance, 3 reviewers rated the degree of causality implied in abstracts and full text for exposure/outcome linking language and action recommendations. Reviewers rated the causal implication of exposure/outcome linking language as none (no causal implication) in 13.8%, weak in 34.2%, moderate in 33.2%, and strong in 18.7% of abstracts. The implied causality of action recommendations was higher than the implied causality of linking sentences for 44.5% or commensurate for 40.3% of articles. The most common linking word in abstracts was “associate” (45.7%). Reviewers’ ratings of linking word roots were highly heterogeneous; over half of reviewers rated “association” as having at least some causal implication. This research undercuts the assumption that avoiding “causal” words leads to clarity of interpretation in medical research."
  },
  {
    "objectID": "publication/index.html",
    "href": "publication/index.html",
    "title": "Publications",
    "section": "",
    "text": "Causal and Associational Language in Observational Health Research: A Systematic Evaluation\n\n\n\n\n\n\ncausal\n\n\n\nWe screened over 1,000 articles from 18 high-profile journals published from 2010-2019 to review the use of causal language to describe results in experimental and observational data. \n\n\n\n\n\nNov 19, 2022\n\n\nAmerican Journal of Epidemiology\n\n\n\n\n\n\n\n\n\n\n\n\n97 Things Every Data Engineer Should Know: Collective Wisdom from the Experts\n\n\n\n\n\n\ndata\n\n\n\nWith this in-depth book, data engineers will learn powerful, real-world best practices for managing data—both big and small. Contributors from companies including Google, Microsoft, IBM, Facebook, Databricks, and GitHub share their experiences and lessons learned on cleaning, prepping, wrangling, and storing data. I contributed 6 of the 97 essays. \n\n\n\n\n\nAug 1, 2021\n\n\nO’Reilly Media\n\n\n\n\n\n\n\n\n\n\n\n\nR Markdown Cookbook\n\n\n\n\n\n\nrmarkdown\n\n\nrstats\n\n\n\nThis book is designed to provide a range of examples of how to extend the functionality of your R Markdown documents. As a cookbook, this guide is recommended to new or intermediate R Markdown users who desire to enhance the efficiency of using R Markdown and also explore the power of R Markdown. Read online at https://bookdown.org/yihui/rmarkdown-cookbook/ \n\n\n\n\n\nSep 1, 2020\n\n\nCRC Press\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talk/casual-pod/index.html",
    "href": "talk/casual-pod/index.html",
    "title": "Casual Inference Pod - Optimizing Data Workflows with Emily Riederer (Season 6, Episode 8)",
    "section": "",
    "text": "Podcast Episode \nCasual Inference is a podcast on all things epidemiology, statistics, data science, causal inference, and public health. Sponsored by the American Journal of Epidemiology. As a guest on this episode, I discuss data science communication, the different challenges of causal analysis in industry versus academia, and much more."
  },
  {
    "objectID": "talk/casual-pod/index.html#quick-links",
    "href": "talk/casual-pod/index.html#quick-links",
    "title": "Casual Inference Pod - Optimizing Data Workflows with Emily Riederer (Season 6, Episode 8)",
    "section": "",
    "text": "Podcast Episode \nCasual Inference is a podcast on all things epidemiology, statistics, data science, causal inference, and public health. Sponsored by the American Journal of Epidemiology. As a guest on this episode, I discuss data science communication, the different challenges of causal analysis in industry versus academia, and much more."
  },
  {
    "objectID": "talk/col-names-contract/index.html",
    "href": "talk/col-names-contract/index.html",
    "title": "Column Names as Contracts",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n Post - Column Names as Contracts \n\n\nComplex software systems make performance guarantees through documentation and unit tests, and they communicate these to users with conscientious interface design. However, published data tables exist in a gray area; they are static enough not to be considered a “service” or “software”, yet too raw to earn attentive user interface design. This ambiguity creates a disconnect between data producers and consumers and poses a risk for analytical correctness and reproducibility.\nIn this talk, I will explain how controlled vocabularies can be used to form contracts between data producers and data consumers. Explicitly embedding meaning in each component of variable names is a low-tech and low-friction approach which builds a shared understanding of how each field in the dataset is intended to work.\nDoing so can offload the burden of data producers by facilitating automated data validation and metadata management. At the same time, data consumers benefit by a reduction in the cognitive load to remember names, a deeper understanding of variable encoding, and opportunities to more efficiently analyze the resulting dataset.\nAfter discussing the theory of controlled vocabulary column-naming and related workflows, I illustrate how to implement these ideas at various stages in the data management lifecycle, either with the R package convo or with the SQL-based tool dbt."
  },
  {
    "objectID": "talk/data-error-gen/index.html",
    "href": "talk/data-error-gen/index.html",
    "title": "The Data (error) Generating Process",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n Post - Why Group Data Tests? \n Post - Grouped Data Tests in dbt-utils \n\n\nStatisticians often approach probabilistic modeling by first understanding the conceptual data generating process. However, when validating messy real-world data, the technical aspects of the data generating process is largely ignored.\nIn this talk, I will argue the case for developing more semantically meaningful and well-curated data tests by incorporating both conceptual and technical aspects of “how the data gets made”.\nTo illustrate these concepts, we will explore the NYC subway rides open dataset to see how the simple act of reasoning about real-world events their collection through ETL processes can help craft far more sensitive and expressive data quality checks. I will also illustrate how to implement such checks based on new features which I recently contributed to the open-source dbt-utils package.\nAudience members should leave this talk with a clear framework in mind for ideating better tests for their own pipelines."
  },
  {
    "objectID": "talk/ent-pkg-design/index.html",
    "href": "talk/ent-pkg-design/index.html",
    "title": "Assorted talks on designing analytical tools and communities for enterprise",
    "section": "",
    "text": "What’s in Your Workflow?: Reproducible Business Analysis at Capital One\nDomino Data Pop-Up, Chicago 2017\n\n\n\n \n\n \nInnerSource for reproducible and extensible business analysis\nO’Reilly Strata Data Conference, NYC 2018\nDesigning Empathetic, Empowering, and Engaging Internal Tools\nIDEASS, Chicago 2018\nBuilding an Innersource Culture for Data Analysis\ndata.world Afternoon of Data, Chicago 2019\nWhile every data analysis produces unique insights, the analysis process consists of many standard processes and building blocks. This talk explores what data analysts in an organization can learn from open source to collaboratively create a reproducible analytical infrastructure. Based on lessons learned at Capital One, we will discuss the motivations for cultivating an innersource culture, practical advice on designing and developing internal analytical tools, and strategies for helping data analysts evolve from passive users to active contributors."
  },
  {
    "objectID": "talk/featured.html",
    "href": "talk/featured.html",
    "title": "Top Talks",
    "section": "",
    "text": "Operationalizing Column-Name Contracts with dbtplyr\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nAn exploration of how data producers and consumers can use column names as interfaces, configuations, and code to improve data quality and discoverability. The second half of the talk demonstrates how to implement these ideas with my dbtplyr dbt package.\n\n\n\n\n\nSep 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Design Patterns\n\n\n\n\n\n\ncausal\n\n\n\nAn overview of basic research design patterns in causal inference, modern extensions, and data management strategies to set up a causal inference initiative for success\n\n\n\n\n\nJun 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\noRganization: Design patterns for internal packages\n\n\n\n\n\n\nworkflow\n\n\npkgdev\n\n\nrstats\n\n\n\nAn overview of the unique design challenges and opportunities when building R packages for use inside of a single organization versus open-source. By using the jobs-to-be-done framework, this talk explores how internal packages can be better teammates by following specific design patterns for API design, testing, documentaiton, and more.\n\n\n\n\n\nJan 21, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown Driven Development\n\n\n\n\n\n\nworkflow\n\n\nrmarkdown\n\n\nrstats\n\n\n\nHow and why to refactor one time analyses in RMarkdown into sustainable data products\n\n\n\n\n\nJan 30, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talk/meetup-datafold/index.html",
    "href": "talk/meetup-datafold/index.html",
    "title": "DataFold Data Quality Meet Up",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n\n\nThis is the full recording from Datafold’s 9th Data Quality Meetup on Thursday, May 11th, 2023, which was focused on ‘Running dbt at scale’.\nFollowing our usual structure, each of our speakers present a lightning talk and then we transition into a panel discussion moderated by Gleb Mezhanskiy - who pulls in the audiences’ questions.\nWe had 6 guest speakers & panelists: 1. Emily Riederer @ Capital One - “Operationalizing Column Name Contracts” 2. Felix Kreitschmann and Jorrit Posor @ FINN Auto - “Supercharging Analytics Engineers: How to save time and prevent technical debt by automating CI checks” 3. Alexandra Gronemeyer @ Airbyte - “adopting and running dbt within a small data team at Airbyte” 4. Jason Jones @ Virgin Media O2 - “Zero to 200: scaling analytics engineering within an enterprise” 5. Sung Won Chung @ dbt Labs - “Experiences implementing dbt at scale”"
  },
  {
    "objectID": "talk/meetup-stat447/index.html",
    "href": "talk/meetup-stat447/index.html",
    "title": "UIUC STAT447 (Data Science Programming) Guest Lecture",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n\n\nIn this talk, I pulled from themes in my work on R Markdown Driven Development, innersource analytical tooling, and data science career pathing to help students understand the connections between their classroom learnings and “real world” data science work"
  },
  {
    "objectID": "talk/midterm-iptw/index.html",
    "href": "talk/midterm-iptw/index.html",
    "title": "Evaluation without Experimentation",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video   Post - Causal Design Patterns \n\n\nIn this four-minute lightning talk, I explain the motivation for using observational versus experimental methods when measuring the impact of GOTV (voter turnout) efforts. Next, I provide a high-level explanation of a specific method (inverse propensity of treatment weighting) and demonstrate the method’s ability to balance baseline traits and historical performance of a synthetic control population for credible effect estimation.\nThis talk is based on my related blog post and summarizes my work as a Bluebonnet Data Fellow with the Two Million Texans relational organizing campaign during the 2022 midterms."
  },
  {
    "objectID": "talk/pod-tdep-colnames/index.html",
    "href": "talk/pod-tdep-colnames/index.html",
    "title": "The Data Engineering Podcast: Column Names as Contracts",
    "section": "",
    "text": "Quick LinksAbstractListen\n\n\n Pod \nDownload MP3\n Post - Column Names as Contracts \n\n\nCommunication and shared context are the hardest part of any data system. In recent years the focus has been on data catalogs as the means for documenting data assets, but those introduce a secondary system of record in order to find the necessary information. In this episode Emily Riederer shares her work to create a controlled vocabulary for managing the semantic elements of the data managed by her team and encoding it in the schema definitions in her data warehouse. She also explains how she created the dbtplyr package to simplify the work of creating and enforcing your own controlled vocabularies."
  },
  {
    "objectID": "talk/python-rgonomics/index.html",
    "href": "talk/python-rgonomics/index.html",
    "title": "Python Rgonomics",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n Post - Python Rgonomics \n Post - Advanced polars versus dplyr \n\n\n\n\n\n\n\n\nWarning\n\n\n\nTooling changes quickly. Since this talk occured, Astral’s uv project has come out as a very strong contender to replace pyenv, pdm, and more of the devtools part of a python stack.\n\n\nData science languages are increasingly interoperable with advances like Arrow, Quarto, and Posit Connect. But data scientists are not. Learning the basic syntax of a new language is easy, but relearning the ergonomics that help us be hyperproductive is hard. In this talk, I will explore the influential ergonomics of R’s tidyverse. Next, I will recommend a curated stack that mirrors these ergonomics while also being genuinely truly pythonic. In particular, we will explore packages (polars, seaborn objects, greattables), frameworks (Shiny, Quarto), dev tools (pyenv, ruff, and pdm), and IDEs (VS Code extensions). The audience should leave feeling inspired to try python while benefiting from their current knowledge and expertise."
  },
  {
    "objectID": "talk/shiny-modules/index.html",
    "href": "talk/shiny-modules/index.html",
    "title": "Taking Flight with Shiny: a Modules-First Approach",
    "section": "",
    "text": "Quick LinksAbstractSlidesVideo\n\n\n Slides \n Video \n Post - A beginner’s guide to Shiny modules \n\n\nR users are increasingly trained to develop with good principles such as writing modular functions, testing their code, and decomposing long individual scripts into R projects. In recent years, such approaches have crept earlier into introductory R education with the realization that applying best practices is not an advanced skill but rather empowers beginners with a more robust structure.\nHowever, new Shiny developers are confronted with two challenges: they must simultaneously learn new packages and concepts (like reactivity) which introductory tutorials demonstrate how to write their apps as single scripts. This means small changes are harder to test and debug for the groups that need it the most. Although Shiny modules offer a solution to this exact problem, they are regarded as an advanced topic and often not encountered until much later in a developer’s journey.\nIn this talk, I will demonstrate a workflow to encourage the use of modules for newer Shiny users. I argue that a ‘module-first’ approach helps to decompose design into more tangible, bite-sized, and testable components and prevent the code sprawl that makes Shiny feel intimidating. Further, this approach can be even more powerful when developing Shiny applications in the enterprise setting and onboarding new team members to existing applications."
  },
  {
    "objectID": "docs/post/quarto-comms/raw-analysis.html",
    "href": "docs/post/quarto-comms/raw-analysis.html",
    "title": "Demo Notebook",
    "section": "",
    "text": "import polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nimport requests"
  },
  {
    "objectID": "docs/post/quarto-comms/raw-analysis.html#data-retrieval",
    "href": "docs/post/quarto-comms/raw-analysis.html#data-retrieval",
    "title": "Demo Notebook",
    "section": "Data Retrieval",
    "text": "Data Retrieval\n\nres = []\nurl = 'https://www.federalregister.gov/api/v1/documents.json?fields[]=document_number&fields[]=excerpts&fields[]=page_length&fields[]=president&fields[]=publication_date&fields[]=raw_text_url&fields[]=signing_date&fields[]=title&fields[]=toc_subject&fields[]=topics&per_page=20&conditions[publication_date][gte]=2005-01-20&conditions[presidential_document_type][]=executive_order&conditions[president][]=george-w-bush&conditions[president][]=barack-obama&conditions[president][]=donald-trump&conditions[president][]=joe-biden&conditions[president][]=donald-trump'\n\nwhile url != '':\n\n    # retrieve results\n    response = requests.get(url)\n    response.raise_for_status()\n    res_temp = response.json()\n    res += [res_temp]\n\n    # get next url\n    url = res_temp.get('next_page_url','')"
  },
  {
    "objectID": "docs/post/quarto-comms/raw-analysis.html#data-cleaning",
    "href": "docs/post/quarto-comms/raw-analysis.html#data-cleaning",
    "title": "Demo Notebook",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\ndf_res = (\n    pl.DataFrame(res)\n    .select('results')\n    .explode('results')\n    .unnest('results')\n    .unnest('president')\n)\ndf_res.glimpse()\n\n\n# lookup table\n\nterm = [1,2,1,2,1,1,2]\nstart_year = [2001, 2005, 2009, 2013, 2017, 2021, 2025]\nstart_date = [f'{year}-01-20' for year in start_year]\nend_date = start_date[1:] + ['2029-01-20']\npresident_lname = ['Bush']*2 + ['Obama']*2 + ['Trump'] + ['Biden'] + ['Trump']\ndf_lkup = pl.DataFrame([term, start_year, start_date, end_date, president_lname], \n                       schema = ['term','start_year','start_date', 'end_date', 'president_lname'],\n                       orient = 'col')\ndf_lkup\n\n\n\nshape: (7, 5)\n\n\n\nterm\nstart_year\nstart_date\nend_date\npresident_lname\n\n\ni64\ni64\nstr\nstr\nstr\n\n\n\n\n1\n2001\n\"2001-01-20\"\n\"2005-01-20\"\n\"Bush\"\n\n\n2\n2005\n\"2005-01-20\"\n\"2009-01-20\"\n\"Bush\"\n\n\n1\n2009\n\"2009-01-20\"\n\"2013-01-20\"\n\"Obama\"\n\n\n2\n2013\n\"2013-01-20\"\n\"2017-01-20\"\n\"Obama\"\n\n\n1\n2017\n\"2017-01-20\"\n\"2021-01-20\"\n\"Trump\"\n\n\n1\n2021\n\"2021-01-20\"\n\"2025-01-20\"\n\"Biden\"\n\n\n2\n2025\n\"2025-01-20\"\n\"2029-01-20\"\n\"Trump\"\n\n\n\n\n\n\n\n\ndf_clean = (\n    df_res\n    .with_columns(\n        president_lname = pl.col('identifier')\n                            .str.extract('[A-Za-z ]+-([A-Za-z]+$)', group_index = 1)\n                            .str.to_titlecase(),\n    )\n    # there's a bug in polars join_asof() at the time of writing...\n    .join(df_lkup, on = 'president_lname')\n    .filter( pl.col('signing_date') &gt;= pl.col('start_date'))\n    .filter( pl.col('signing_date') &lt;  pl.col('end_date'))\n    .with_columns( pres_term = pl.concat_str( pl.col('president_lname'), pl.lit(' - '), pl.col('term')) )\n)\ndf_clean.glimpse()\n\nRows: 952\nColumns: 17\n$ document_number         &lt;str&gt; '2025-14212', '2025-14217', '2025-14218', '2025-13925', '2025-12961', '2025-12962', '2025-12774', '2025-12775', '2025-12505', '2025-12506'\n$ excerpts               &lt;null&gt; None, None, None, None, None, None, None, None, None, None\n$ page_length             &lt;i64&gt; 4, 3, 3, 3, 2, 2, 3, 2, 1, 5\n$ identifier              &lt;str&gt; 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump'\n$ name                    &lt;str&gt; 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump'\n$ publication_date        &lt;str&gt; '2025-07-28', '2025-07-28', '2025-07-28', '2025-07-23', '2025-07-10', '2025-07-10', '2025-07-09', '2025-07-09', '2025-07-03', '2025-07-03'\n$ raw_text_url            &lt;str&gt; 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14212.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14217.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14218.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/23/2025-13925.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12961.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12962.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12774.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12775.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12505.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12506.txt'\n$ signing_date            &lt;str&gt; '2025-07-23', '2025-07-23', '2025-07-23', '2025-07-17', '2025-07-07', '2025-07-07', '2025-07-03', '2025-07-03', '2025-06-30', '2025-06-30'\n$ title                   &lt;str&gt; 'Accelerating Federal Permitting of Data Center Infrastructure', 'Preventing Woke AI in the Federal Government', 'Promoting the Export of the American AI Technology Stack', 'Creating Schedule G in the Excepted Service', 'Ending Market Distorting Subsidies for Unreliable, Foreign-Controlled Energy Sources', 'Extending the Modification of the Reciprocal Tariff Rates', \"Establishing the President's Make America Beautiful Again Commission\", 'Making America Beautiful Again by Improving Our National Parks', 'Establishing a White House Office for Special Peace Missions', 'Providing for the Revocation of Syria Sanctions'\n$ toc_subject             &lt;str&gt; None, 'Government Agencies and Employees', None, 'Government Agencies and Employees', None, None, None, None, None, None\n$ topics           &lt;list[null]&gt; [], [], [], [], [], [], [], [], [], []\n$ president_lname         &lt;str&gt; 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump'\n$ term                    &lt;i64&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n$ start_year              &lt;i64&gt; 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025\n$ start_date              &lt;str&gt; '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20'\n$ end_date                &lt;str&gt; '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20'\n$ pres_term               &lt;str&gt; 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2'\n\nRows: 952\nColumns: 17\n$ document_number         &lt;str&gt; '2025-14212', '2025-14217', '2025-14218', '2025-13925', '2025-12961', '2025-12962', '2025-12774', '2025-12775', '2025-12505', '2025-12506'\n$ excerpts               &lt;null&gt; None, None, None, None, None, None, None, None, None, None\n$ page_length             &lt;i64&gt; 4, 3, 3, 3, 2, 2, 3, 2, 1, 5\n$ identifier              &lt;str&gt; 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump'\n$ name                    &lt;str&gt; 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump'\n$ publication_date        &lt;str&gt; '2025-07-28', '2025-07-28', '2025-07-28', '2025-07-23', '2025-07-10', '2025-07-10', '2025-07-09', '2025-07-09', '2025-07-03', '2025-07-03'\n$ raw_text_url            &lt;str&gt; 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14212.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14217.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14218.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/23/2025-13925.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12961.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12962.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12774.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12775.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12505.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12506.txt'\n$ signing_date            &lt;str&gt; '2025-07-23', '2025-07-23', '2025-07-23', '2025-07-17', '2025-07-07', '2025-07-07', '2025-07-03', '2025-07-03', '2025-06-30', '2025-06-30'\n$ title                   &lt;str&gt; 'Accelerating Federal Permitting of Data Center Infrastructure', 'Preventing Woke AI in the Federal Government', 'Promoting the Export of the American AI Technology Stack', 'Creating Schedule G in the Excepted Service', 'Ending Market Distorting Subsidies for Unreliable, Foreign-Controlled Energy Sources', 'Extending the Modification of the Reciprocal Tariff Rates', \"Establishing the President's Make America Beautiful Again Commission\", 'Making America Beautiful Again by Improving Our National Parks', 'Establishing a White House Office for Special Peace Missions', 'Providing for the Revocation of Syria Sanctions'\n$ toc_subject             &lt;str&gt; None, 'Government Agencies and Employees', None, 'Government Agencies and Employees', None, None, None, None, None, None\n$ topics           &lt;list[null]&gt; [], [], [], [], [], [], [], [], [], []\n$ president_lname         &lt;str&gt; 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump', 'Trump'\n$ term                    &lt;i64&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2\n$ start_year              &lt;i64&gt; 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025, 2025\n$ start_date              &lt;str&gt; '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20', '2025-01-20'\n$ end_date                &lt;str&gt; '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20', '2029-01-20'\n$ pres_term               &lt;str&gt; 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2', 'Trump - 2'\n\n\n\n\ndf_agg = (\n    df_clean\n    .group_by('pres_term')\n    .agg(\n        n_eo_issued = pl.col('document_number').count(),\n        start_date = pl.col('publication_date').min()\n    )\n)\n\ndf_agg.sort('start_date')\n\n\n\nshape: (6, 3)\n\n\n\npres_term\nn_eo_issued\nstart_date\n\n\nstr\nu32\nstr\n\n\n\n\n\"Bush - 2\"\n118\n\"2005-01-31\"\n\n\n\"Obama - 1\"\n148\n\"2009-01-26\"\n\n\n\"Obama - 2\"\n130\n\"2013-02-19\"\n\n\n\"Trump - 1\"\n220\n\"2017-01-24\"\n\n\n\"Biden - 1\"\n162\n\"2021-01-25\"\n\n\n\"Trump - 2\"\n174\n\"2025-01-28\"\n\n\n\n\n\n\n\n\ndf_cumul = (\n    df_clean\n    .with_columns( cs.ends_with('date').cast(pl.Date) )\n    .with_columns(\n        day_since_start = ( pl.col('signing_date') - pl.col('start_date') ).dt.total_days(),\n        term_label = pl.concat_str( pl.lit('('), pl.col('start_date').dt.year(), pl.lit(')'),\n                                    pl.lit(' '),\n                                    pl.col('president_lname') )\n    )\n    .group_by('pres_term', 'term_label', 'day_since_start')\n    .len()\n    .with_columns( n_cumul = pl.col('len').cum_sum().over('pres_term', order_by = 'day_since_start') )\n)\ndf_cumul.sort('pres_term', 'day_since_start').glimpse()\n\nRows: 711\nColumns: 5\n$ pres_term       &lt;str&gt; 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1'\n$ term_label      &lt;str&gt; '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden'\n$ day_since_start &lt;i64&gt; 0, 1, 2, 5, 6, 7, 8, 13, 15, 21\n$ len             &lt;u32&gt; 9, 8, 2, 2, 1, 2, 1, 3, 1, 1\n$ n_cumul         &lt;u32&gt; 9, 17, 19, 21, 22, 24, 25, 28, 29, 30\n\nRows: 711\nColumns: 5\n$ pres_term       &lt;str&gt; 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1', 'Biden - 1'\n$ term_label      &lt;str&gt; '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden', '(2021) Biden'\n$ day_since_start &lt;i64&gt; 0, 1, 2, 5, 6, 7, 8, 13, 15, 21\n$ len             &lt;u32&gt; 9, 8, 2, 2, 1, 2, 1, 3, 1, 1\n$ n_cumul         &lt;u32&gt; 9, 17, 19, 21, 22, 24, 25, 28, 29, 30\n\n\n\n\n(\nggplot(df_cumul) +\naes(x = 'day_since_start', y = 'n_cumul', color = 'term_label') +\ngeom_line() +\nlabs(\n    title = 'Executive Orders Signed by Days in Office',\n    x = 'Days since Term Start', \n    y = 'EOs Issued', \n    color = 'Term') +\ntheme_538() +\nscale_color_brewer('qual', palette = 'Set1') +\ntheme(legend_position = 'bottom')\n)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nI have some thoughts…"
  },
  {
    "objectID": "docs/post/orbital-xgb/orbital-xgb.out.html",
    "href": "docs/post/orbital-xgb/orbital-xgb.out.html",
    "title": "orbital for xgboost",
    "section": "",
    "text": "Set Up\n\nimport orbital\nimport duckdb\nimport sqlglot\nimport skl2onnx\nimport pandas as pd \nimport numpy as np\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_classification\nfrom skl2onnx import update_registered_converter\nfrom skl2onnx.common.shape_calculator import calculate_linear_classifier_output_shapes\nfrom onnxmltools.convert.xgboost.operator_converters.XGBoost import convert_xgboost\n\n\n# make data dataset\nX_train, y_train = make_classification(10000, random_state = 102)\nX_train = X_train.round(3)\n\n# get column names for use in pipeline\n## onnxmltools forces these to be formatted as \"f&lt;number&gt;\"\nn_cols = len(X_train[0])\nnm_cols = [f\"f{i}\" for i in range(n_cols)]\nfeat_dict = {c:orbital.types.DoubleColumnType() for c in nm_cols}\n\n\n\nConvert an xgboost model to a SciKitLearn pipeline with XGBClassifier\n\n# train with xgb learning api\n## keeping parameters super simple so it trains fast and easy to compare\n## important: this only works for now with base_score=0.5 \n## this is the default assumed by orbital's logic, and I haven't figured out how to convince it otherwise\ndtrain = xgb.DMatrix(X_train, y_train, feature_names = nm_cols)\nparams = {'max_depth':2, \n          'objective':'binary:logistic', \n          'base_score':0.5, \n          'seed':504}\nmodel = xgb.train(params, num_boost_round = 1, dtrain = dtrain)\npreds_xgb = model.predict(xgb.DMatrix(X_train, feature_names = nm_cols))\n\n# convert back to skl interface & rebuild needed metadata\nclf = xgb.XGBClassifier()\nclf._Booster = model\nclf.n_classes_ = 2\nclf.base_score = 0.5\npreds_skl = clf.predict_proba(X_train)[:,-1]\n\n# validate that the results are the same\nprint(f\"xgb and skl match: {np.all(np.isclose(preds_xgb, preds_skl))}\")\n\n# add to skl pipeline\nppl = Pipeline([(\"gbm\", clf)])\npreds_ppl = ppl.predict_proba(X_train)[:,-1]\n\n# validate that the results are the same\nprint(f\"xgb and ppl match: {np.all(np.isclose(preds_xgb, preds_ppl))}\")\n\nxgb and skl match: True\nxgb and ppl match: True\nxgb and skl match: True\nxgb and ppl match: True\n\n\n\n# now we actually make a slightly more complicated pipeline\n# orbital seems unhappy if there isn't at least one preprocessing step,\n# so we make one that processes no variables and passes through the rest\npipeline = Pipeline([\n    (\"pre\", ColumnTransformer([], remainder=\"passthrough\")),\n])\npipeline.fit(X_train)\npipeline.steps.append((\"gbm\", clf))\npreds_ppl2 = pipeline.predict_proba(X_train)[:,1]\nprint(f\"xgb and ppl2 matches: {np.all(np.isclose(preds_xgb, preds_ppl2))}\")\n\nxgb and ppl2 matches: True\nxgb and ppl2 matches: True\n\n\n\n\nRegister translator for XGBClassifier\nXGBClassifier is implemented in onnxmltools. This converter can then be registered to skl2onnx.\n\n# `options` copied straight from `onnxmltools` docs\nupdate_registered_converter(\n    XGBClassifier,\n    \"XGBoostXGBClassifier\",\n    calculate_linear_classifier_output_shapes,\n    convert_xgboost,\n    options={\"nocl\": [True, False], \n             \"zipmap\": [True, False, \"columns\"], \n            },\n)\n\n\n\nOverride parse_pipeline function from orbital\nThis solves a bit of a niche (?) problem. The ONNNX model spec is versioned, and using onnxmltools in the previous step seems to encourage a target_opset which is not comparible with something in either skl2onnx or orbital. The error message advised explicitly specifying the desired target_opset in skl2onnx.to_onnx(), but orbital doesn’t let us pass this into parse_pipeline().\nSo, we override parse_pipeline(). This is a top-level function and not a class method, so we don’t need to monkey patch; we can just steal the code directly and define our own function.\n\ndef parse_pipeline_local(\n    pipeline: Pipeline, features: orbital.types.FeaturesTypes\n) -&gt; orbital.ast.ParsedPipeline:\n\n    onnx_model = skl2onnx.to_onnx(\n        pipeline,\n        initial_types=[\n            (fname, ftype._to_onnxtype())\n            for fname, ftype in features.items()\n            if not ftype.is_passthrough\n        ],\n        target_opset={\"\": 15,'ai.onnx.ml':3}\n    )\n    return orbital.ast.ParsedPipeline._from_onnx_model(onnx_model, features)\n\n\n\nRun orbital\nStill with me? Now, we run orbital which is what we were trying to do all along.\n\n# translate into an Orbital Pipeline\norbital_pipeline = parse_pipeline_local(pipeline, features=feat_dict)\nsql_raw = orbital.export_sql(\"DATA_TABLE\", orbital_pipeline, dialect=\"duckdb\")\n\nc:\\Users\\emily\\Desktop\\website\\post\\orbital-xgb\\.venv\\Lib\\site-packages\\orbital\\translation\\steps\\trees\\classifier.py:135: FutureWarning: `case` is deprecated as of v10.0.0, removed in v11.0; use ibis.cases()\n  ibis.case().when(condition, t_val).else_(f_val).end()\nc:\\Users\\emily\\Desktop\\website\\post\\orbital-xgb\\.venv\\Lib\\site-packages\\orbital\\translation\\steps\\trees\\classifier.py:157: FutureWarning: `case` is deprecated as of v10.0.0, removed in v11.0; use ibis.cases()\n  ibis.case()\n\n\n\n\nClean up query (optional)\nAs we all know, ugly queries still run. But we can use sqlglot (used under the hood already anyway) to clean up the query.\n\n# parse AST from SQL script\nast = sqlglot.parse_one(sql_raw)\n\n# clean up SQL\n## drop the class prediction and negative-event predictions\nast.expressions[0] = None\nast.expressions[1] = None \n## add back a variable for reference (typically like an ID for joining to other tables)\nast = ast.select('f1')\n## rename positive-event predictions for something simpler to interpret\nsql_mod = ast.sql()\nsql_mod = sql_mod.replace(\"output_probability.1\", \"pred\")\n## pretty print -- not important for usage; but we'll take a peak at the output at the end here\nsql_fmt = sqlglot.transpile(sql_mod, write=\"duckdb\", identify=True, pretty=True)[0]\n\n\n\nValidate Inputs\nAnother fun form of validation – we can visually inspect the beginning of our xgboost tree and compare it to the compile SQL!\n\nfig, ax = plt.subplots(figsize=(30, 30))\nxgb.plot_tree(model, tree_idx=0, rankdir='LR', ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(sql_fmt)\n\nSELECT\n  1 / (\n    EXP(\n      -CASE\n        WHEN \"t0\".\"f4\" &lt; -0.04800000041723251\n        THEN CASE\n          WHEN \"t0\".\"f4\" &lt; -0.8119999766349792\n          THEN -0.5087512135505676\n          ELSE -0.21405750513076782\n        END\n        ELSE CASE\n          WHEN \"t0\".\"f18\" &lt; -0.4269999861717224\n          THEN -0.3149999976158142\n          ELSE 0.5008015036582947\n        END\n      END\n    ) + 1\n  ) AS \"pred\",\n  \"f1\"\nFROM \"DATA_TABLE\" AS \"t0\"\nSELECT\n  1 / (\n    EXP(\n      -CASE\n        WHEN \"t0\".\"f4\" &lt; -0.04800000041723251\n        THEN CASE\n          WHEN \"t0\".\"f4\" &lt; -0.8119999766349792\n          THEN -0.5087512135505676\n          ELSE -0.21405750513076782\n        END\n        ELSE CASE\n          WHEN \"t0\".\"f18\" &lt; -0.4269999861717224\n          THEN -0.3149999976158142\n          ELSE 0.5008015036582947\n        END\n      END\n    ) + 1\n  ) AS \"pred\",\n  \"f1\"\nFROM \"DATA_TABLE\" AS \"t0\"\n\n\n\n\nValidate Output\nAlthough we can confirm visually this does what we want, we don’t get perfectly matching predictions for observations at the corner cases. Due to floating point math, it seems observations may sometimes end up on dirrect sides of the cutpoints.\n\nDATA_TABLE = pd.DataFrame(X_train, columns = nm_cols)\ndb_preds = duckdb.sql(sql_mod).df()\npreds_orb = db_preds['pred']\nprint(f\"xgb and orb match: {np.all(np.isclose(preds_xgb, preds_orb))}\")\n\nxgb and orb match: False\nxgb and orb match: False\n\n\n\n# isolate and size misses\nmisses = np.where(~np.isclose(preds_xgb, preds_orb))\nprint(f'Different predictions (N): {len(misses[0])}')\nprint(f'Different predictions (P): {len(misses[0]) / len(X_train):.4f}')\n\n# pull out f4 and f18; notice that all discrepancies lie exactly at the splitting points\nX_train[misses][:,[4,18]]\n\nDifferent predictions (N): 5\nDifferent predictions (P): 0.0005\nDifferent predictions (N): 5\nDifferent predictions (P): 0.0005\n\n\narray([[-0.812, -0.515],\n       [-0.812, -0.739],\n       [ 1.715, -0.427],\n       [ 0.025, -0.427],\n       [ 2.119, -0.427]])"
  },
  {
    "objectID": "docs/post/quarto-comms/raw-analysis.out.html",
    "href": "docs/post/quarto-comms/raw-analysis.out.html",
    "title": "Demo Notebook",
    "section": "",
    "text": "import requests\nimport polars as pl\nimport polars.selectors as cs\nfrom plotnine import *\nfrom great_tables import *"
  },
  {
    "objectID": "docs/post/quarto-comms/raw-analysis.out.html#data-retrieval",
    "href": "docs/post/quarto-comms/raw-analysis.out.html#data-retrieval",
    "title": "Demo Notebook",
    "section": "Data Retrieval",
    "text": "Data Retrieval\n\nres = []\nurl = 'https://www.federalregister.gov/api/v1/documents.json?fields[]=document_number&fields[]=excerpts&fields[]=page_length&fields[]=president&fields[]=publication_date&fields[]=raw_text_url&fields[]=signing_date&fields[]=title&fields[]=toc_subject&fields[]=topics&per_page=20&conditions[publication_date][gte]=2005-01-20&conditions[presidential_document_type][]=executive_order&conditions[president][]=george-w-bush&conditions[president][]=barack-obama&conditions[president][]=donald-trump&conditions[president][]=joe-biden&conditions[president][]=donald-trump'\n\nwhile url != '':\n\n    # retrieve results\n    response = requests.get(url)\n    response.raise_for_status()\n    res_temp = response.json()\n    res += [res_temp]\n\n    # get next url\n    url = res_temp.get('next_page_url','')"
  },
  {
    "objectID": "docs/post/quarto-comms/raw-analysis.out.html#data-cleaning",
    "href": "docs/post/quarto-comms/raw-analysis.out.html#data-cleaning",
    "title": "Demo Notebook",
    "section": "Data Cleaning",
    "text": "Data Cleaning\n\ndf_res = (\n    pl.DataFrame(res)\n    .select('results')\n    .explode('results')\n    .unnest('results')\n    .unnest('president')\n)\ndf_res.glimpse()\n\nRows: 969\nColumns: 11\n$ document_number         &lt;str&gt; '2025-14212', '2025-14217', '2025-14218', '2025-13925', '2025-12961', '2025-12962', '2025-12774', '2025-12775', '2025-12505', '2025-12506'\n$ excerpts               &lt;null&gt; None, None, None, None, None, None, None, None, None, None\n$ page_length             &lt;i64&gt; 4, 3, 3, 3, 2, 2, 3, 2, 1, 5\n$ identifier              &lt;str&gt; 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump'\n$ name                    &lt;str&gt; 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump'\n$ publication_date        &lt;str&gt; '2025-07-28', '2025-07-28', '2025-07-28', '2025-07-23', '2025-07-10', '2025-07-10', '2025-07-09', '2025-07-09', '2025-07-03', '2025-07-03'\n$ raw_text_url            &lt;str&gt; 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14212.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14217.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14218.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/23/2025-13925.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12961.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12962.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12774.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12775.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12505.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12506.txt'\n$ signing_date            &lt;str&gt; '2025-07-23', '2025-07-23', '2025-07-23', '2025-07-17', '2025-07-07', '2025-07-07', '2025-07-03', '2025-07-03', '2025-06-30', '2025-06-30'\n$ title                   &lt;str&gt; 'Accelerating Federal Permitting of Data Center Infrastructure', 'Preventing Woke AI in the Federal Government', 'Promoting the Export of the American AI Technology Stack', 'Creating Schedule G in the Excepted Service', 'Ending Market Distorting Subsidies for Unreliable, Foreign-Controlled Energy Sources', 'Extending the Modification of the Reciprocal Tariff Rates', \"Establishing the President's Make America Beautiful Again Commission\", 'Making America Beautiful Again by Improving Our National Parks', 'Establishing a White House Office for Special Peace Missions', 'Providing for the Revocation of Syria Sanctions'\n$ toc_subject             &lt;str&gt; None, 'Government Agencies and Employees', None, 'Government Agencies and Employees', None, None, None, None, None, None\n$ topics           &lt;list[null]&gt; [], [], [], [], [], [], [], [], [], []\n\nRows: 969\nColumns: 11\n$ document_number         &lt;str&gt; '2025-14212', '2025-14217', '2025-14218', '2025-13925', '2025-12961', '2025-12962', '2025-12774', '2025-12775', '2025-12505', '2025-12506'\n$ excerpts               &lt;null&gt; None, None, None, None, None, None, None, None, None, None\n$ page_length             &lt;i64&gt; 4, 3, 3, 3, 2, 2, 3, 2, 1, 5\n$ identifier              &lt;str&gt; 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump', 'donald-trump'\n$ name                    &lt;str&gt; 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump', 'Donald Trump'\n$ publication_date        &lt;str&gt; '2025-07-28', '2025-07-28', '2025-07-28', '2025-07-23', '2025-07-10', '2025-07-10', '2025-07-09', '2025-07-09', '2025-07-03', '2025-07-03'\n$ raw_text_url            &lt;str&gt; 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14212.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14217.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/28/2025-14218.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/23/2025-13925.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12961.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/10/2025-12962.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12774.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/09/2025-12775.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12505.txt', 'https://www.federalregister.gov/documents/full_text/text/2025/07/03/2025-12506.txt'\n$ signing_date            &lt;str&gt; '2025-07-23', '2025-07-23', '2025-07-23', '2025-07-17', '2025-07-07', '2025-07-07', '2025-07-03', '2025-07-03', '2025-06-30', '2025-06-30'\n$ title                   &lt;str&gt; 'Accelerating Federal Permitting of Data Center Infrastructure', 'Preventing Woke AI in the Federal Government', 'Promoting the Export of the American AI Technology Stack', 'Creating Schedule G in the Excepted Service', 'Ending Market Distorting Subsidies for Unreliable, Foreign-Controlled Energy Sources', 'Extending the Modification of the Reciprocal Tariff Rates', \"Establishing the President's Make America Beautiful Again Commission\", 'Making America Beautiful Again by Improving Our National Parks', 'Establishing a White House Office for Special Peace Missions', 'Providing for the Revocation of Syria Sanctions'\n$ toc_subject             &lt;str&gt; None, 'Government Agencies and Employees', None, 'Government Agencies and Employees', None, None, None, None, None, None\n$ topics           &lt;list[null]&gt; [], [], [], [], [], [], [], [], [], []\n\n\n\n# lookup table\n\nterm = [1,2,1,2,1,1,2]\nstart_year = [2001, 2005, 2009, 2013, 2017, 2021, 2025]\nstart_date = [f'{year}-01-20' for year in start_year]\nend_date = start_date[1:] + ['2029-01-20']\npresident_lname = ['Bush']*2 + ['Obama']*2 + ['Trump'] + ['Biden'] + ['Trump']\ndf_lkup = pl.DataFrame([term, start_year, start_date, end_date, president_lname], \n                       schema = ['term','start_year','start_date', 'end_date', 'president_lname'],\n                       orient = 'col')\ndf_lkup"
  },
  {
    "objectID": "post/orbital-mlops/index.html",
    "href": "post/orbital-mlops/index.html",
    "title": "MLOrbs?: MLOps in the database with orbital and dbt",
    "section": "",
    "text": "So, you build a great predictive model. Now what?\nMLOps is hard. Deploying a model involves different tools, skills, and risks than model development. This dooms some data science projects to die on their creator’s hard drive.\nTools like dbt and SQLMesh entered the scene to solve a similar problem for data analysts. These tools offer an opinionatee frameowrk for organizing multiple related SQL scripts into fully tested, orchestrated, and version conotrolled projects. Data analysts can deliver end-to-end pipelines by applying their existing business context, SQL experience, and database modeling1 acumen into existing infrastructure, resulting in the rise of “analytics engineering”.\nSo what is the dbt for data scientists doing MLOps? It turns out, it might just be… dbt! (Enter caveats galore).\nPosit’s recently announced orbital project2 translates feature enginering and model scoring code into raw SQL code for supported model types (e.g. linear, tree-based) trained in scikit-learn pipelines (python) and tidymodels workflows. Similar to dbt, this has the potential to help data scientist’s deploy their own models batch-scoring models using existing tools (R, python, SQL) an infrastructure (analytical database) by creating a new table or view in their data warehouse (or pointing duckdb against their data lake!) Coupled with dbt, could orbital unlock “good enough”, zero-infra MLOps practices in a resource-contrained environment?\nIn this post, I explore a workflow for using orbital and dbt for zero-infrastructure deployment of batch models inside of a dbt pipeline. We’ll discuss:\nAlong the way, we’ll walk through this demo implementation of a churn prediction model (wow, what a cliche). The demo is fully self-contained with open data and a duckdb backend if you want to pull it down and play along!"
  },
  {
    "objectID": "post/orbital-mlops/index.html#orbital-dbt-overview",
    "href": "post/orbital-mlops/index.html#orbital-dbt-overview",
    "title": "MLOrbs?: MLOps in the database with orbital and dbt",
    "section": "orbital + dbt Overview",
    "text": "orbital + dbt Overview"
  },
  {
    "objectID": "post/orbital-mlops/index.html#mlops-challenges",
    "href": "post/orbital-mlops/index.html#mlops-challenges",
    "title": "MLOrbs?: MLOps in the database with orbital and dbt",
    "section": "MLOps Challenges",
    "text": "MLOps Challenges\nPredictive modeling requires nuanced, detailed thinking; MLOps requires systems thinking. Success requires an unlikely combination of skills including a deep understanding of the business problem, the modeling workflow, and engineering principles. Some key challenges include:4\n\nData management & testing\n\nPre-modeling (not exactly MLOps) – test source data upstream of the initial query for better visibility into quality errors or concept drift\nScoring time – preventing scoring on features ranges not seen in training when this poses inordinate risk\n\nRecreating the development / evaluation environment\n\nFeature transformations – ensuring feature availability in prod (no leakage!) and same transformation logic as dev\nEnvironment management – controling package versions and dependencies for scoring code\nVersioning – tracking changes to the model over time\n\nServing relevance\n\nAccess – controlling access to intended consumers and delivering to platform they can use\nReliability – ensuring predictions are retrievable on-demand\n\nReproducibility / Observability\n\nSnapshotting – ability to store past predictions for auditability and performance monitoring\nTesting – inserting tests at relevant points in the pipeline, providing observability and automated error handling\nLogging – ensuring general system observability of performance, errors, retries, and latency\n\n\nThese technical challenges are exacerbated by cultural factors. In small companies, data teams may be small (or even a data team-of-one) and lack bandwidth for model deployment, engineering-focused skillsets, access to enterprise-grade tools, or stakeholders who would know how to consume model predictions published in bespoke environments. In large companies, modelers may not be allowed access to production systems required for deployment, so handoffs often require prioritization and context sharing across multiple teams."
  },
  {
    "objectID": "post/orbital-mlops/index.html#deploying-to-the-database",
    "href": "post/orbital-mlops/index.html#deploying-to-the-database",
    "title": "MLOrbs?: MLOps in the database with orbital and dbt",
    "section": "Deploying to the database",
    "text": "Deploying to the database\nFor the right use cases, publishing predictions back into an analytical warehouse can be an attractive proposition. This approach is best suited for offline batch scoring, such as models that:\n\ndrive bulk actions in downstrean CRMs, e.g. marketing segments to drive targeted emails5\ninform human decision-making, e.g. individual predictions that rollup into quarterly sales forecast dashboard\nare reincorporated in downstream analysis similar to raw data, e.g. model validation and backtesting, publish a set of propensity scores back into a clinical database\n\nIn such cases, there are many advantages of having model predictions in the database:\n\nFast & accurate deployment: SQL-based deployment means you can deploy your data against exactly the same data is was trained on, reducing the risk of feature drift between dev and prod. Similarly, it reduced ongoing headaches of dependency management since SQL is generally a stable language6 and does not depend on external packages.\nData management tools: Features, scores – at the end of the day its all just data flow. Having predictions in the database unlocks the ability to leverage on other features integrated in your database like access controls, data quality checks, scheduled updates, incremental loads, and snapshots.\nIntegration: Many modern data stacks have their analytical data warehouse connected to many other business-critical systems like dashboards and CRMs (e.g. MailChimp) or are easy to integrate via numerous reverse ETL solutions. Serving predictions to a warehouse is a great first step to syncing them beyond the warehouse in the platforms where they can drive actions like customer contacts.\nDevelopment language agnosticism: For R users tired of the “we don’t put R in production” conversations, SQL provides a generic abstraction layer to “hand off” a model object regardless of how it was developed\n\nConversely, this solution is poorly suited for real-time models where models are scored for single observations on the fly. I share a few more thoughts on when this solution is not well-suited in the final section of this post, Section 4."
  },
  {
    "objectID": "post/orbital-mlops/index.html#orbital-dbt-pattern",
    "href": "post/orbital-mlops/index.html#orbital-dbt-pattern",
    "title": "MLOrbs?: MLOps in the database with orbital and dbt",
    "section": "orbital + dbt Pattern",
    "text": "orbital + dbt Pattern\nTo demonstrate how an orbital + dbt pattern could work, I’ll walk through this example project, using IBM’s telcom churn dataset. The project mostly is mostly structured like a standard dbt project with most of the model training and orbital code in this additional notebook.\nChurn prediction might be a good candidate for batch scoring. Each month, accounts reaching their resubscription date could be scored and published to a database. Scores might then be used analytical use cases like monitoring or revenue forecasting and operational use cases like ingesting segments into a CRM to email targeted retention offers.\nWe’ll work up to this final pipeline:\n\nThe point of this exercise is to think about the orbital and dbt architecture, so the model we deploy will be quite uninventive. Pull down some features, one-hot encode, fit a random forest, and do it all in a (gasp) Jupyter notebook. (Please dont’ do this.)\n\nKey Features and Design Choices\nIf you want the TLDR, I’ll briefly explain the key design choices for this pipeline:\n\nInitial Data Preparation\n\nSet up dbt tests to test sources before joining your feature table. This can better catch dropout from failed joins, newly emerging encoded categories, etc. Consider what additional filters you want to put in downstream tables (Better to “alert and allow” and “block until fixed”?)\nPrepare feature separately (for normalization) but join different features in database to take advantage of database processing\nConsider adding random number in training table for reproducible test/train split (this has to be linked to hash or something about the entities your randomizing to ensure reproducibility without regard to ordering of data samples)\n\nFeature Engineering\n\nCreate separate scikit-learn pipelines and/or tidymodels workflows for the feature engineering and training steps so you can render these as separate queries. This can enable better data testing and make queries more efficient so orbital does not repeat the feature transformation logic\nUse test-driven development to update dbt data tests as you develop. For example, encoding a categorical? Immediately add an upstream test to check for previously unseen values.\n\nPreparing orbital SQL (supported by sqlglot)\n\nAdd back your identifier column to the query so predictions are joinable\nAdd a model version field into the query for better context to users\nChange placeholder table to a dbt ref()\nRename columns to remove .s so you do not have to always quote in queries\nOutput nicely formatted version for readability\n\nDeploying as a model\n\nConsider carefully whether to make a table, view, or macro depending on your specific database, query latency, and desire to score bespoke populations\n\nObservability, logging, and error handling\n\nUse dbt snapshots to save timestamped past predictions and feature values if these can change over time. This improves auditability and future analysis\nExecute tests to --store-failures to detect changes in your data that might require model retraining or additional error handling\nCheck out dbt packages like elementary to log more aspects of the model run process\n\n\n\n\nSet-Up\nThe sample IBM data is provided as “one big table”, so I break things up to look a bit more like normalized database data representing subscription information, billing information, demographics, and churn targets. I also add a few columns to simulate different months, censor data I want to pretend is in the future, and add a few data errors for fun.\nHere’s a preview of the resulting tables, connected by a customer_id primary key:\n\nimport duckdb\nimport polars as pl\n\nwith duckdb.connect('dev.duckdb') as con:\n\n    df_serv = con.table('serv').pl()\n    df_bill = con.table('bill').pl()\n    df_demo = con.table('demo').pl()\n    df_chrn = con.table('chrn').pl()\n\n\nServices EnrolledBilling InformationDemographicsChurn\n\n\n\ndf_serv.glimpse()\n\nRows: 7043\nColumns: 12\n$ customer_id        &lt;str&gt; '7590-VHVEG', '5575-GNVDE', '3668-QPYBK', '7795-CFOCW', '9237-HQITU', '9305-CDSKC', '1452-KIOVK', '6713-OKOMC', '7892-POOKP', '6388-TABGU'\n$ tenure             &lt;i32&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62\n$ phone_service      &lt;str&gt; 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes'\n$ multiple_lines     &lt;str&gt; 'No phone service', 'No', 'No', 'No phone service', 'No', 'Yes', 'Yes', 'No phone service', 'Yes', 'No'\n$ internet_service   &lt;str&gt; 'DSL', 'DSL', 'DSL', 'DSL', 'Fiber optic', 'Fiber optic', 'Fiber optic', 'DSL', 'Fiber optic', 'DSL'\n$ online_security    &lt;str&gt; 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'Yes'\n$ online_backup      &lt;str&gt; 'Yes', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes'\n$ device_protection  &lt;str&gt; 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'No'\n$ tech_support       &lt;str&gt; 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No'\n$ streaming_tv       &lt;str&gt; 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No'\n$ streaming_movies   &lt;str&gt; 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No'\n$ dt_renewal        &lt;date&gt; 2025-07-01, 2025-07-01, 2025-07-01, 2025-07-01, 2025-07-01, 2025-07-01, 2025-08-01, 2025-07-01, 2025-07-01, 2025-07-01\n\n\n\n\n\n\ndf_bill.glimpse()\n\nRows: 7043\nColumns: 6\n$ customer_id       &lt;str&gt; '7590-VHVEG', '5575-GNVDE', '3668-QPYBK', '7795-CFOCW', '9237-HQITU', '9305-CDSKC', '1452-KIOVK', '6713-OKOMC', '7892-POOKP', '6388-TABGU'\n$ contract          &lt;str&gt; 'Month-to-month', 'One year', 'Month-to-month', 'One year', 'Month-to-month', 'Month-to-month', 'Month-to-month', 'Month-to-month', 'Month-to-month', 'One year'\n$ paperless_billing &lt;str&gt; 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No'\n$ payment_method    &lt;str&gt; 'Electronic check', 'Mailed check', 'Mailed check', 'Bank transfer (automatic)', 'Electronic check', 'Electronic check', 'Credit card (automatic)', 'Mailed check', 'Electronic check', 'Bank transfer (automatic)'\n$ monthly_charges   &lt;f64&gt; 29.85, 56.95, 53.85, 42.3, 70.7, 99.65, 89.1, 29.75, 104.8, 56.15\n$ total_charges     &lt;f64&gt; 29.85, 1889.5, 108.15, 1840.75, 151.65, 820.5, 1949.4, 301.9, 3046.05, 3487.95\n\n\n\n\n\n\ndf_demo.glimpse()\n\nRows: 7043\nColumns: 5\n$ customer_id    &lt;str&gt; '7590-VHVEG', '5575-GNVDE', '3668-QPYBK', '7795-CFOCW', '9237-HQITU', '9305-CDSKC', '1452-KIOVK', '6713-OKOMC', '7892-POOKP', '6388-TABGU'\n$ gender         &lt;str&gt; 'Female', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male'\n$ senior_citizen &lt;i32&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ partner        &lt;str&gt; 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No'\n$ dependents     &lt;str&gt; 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes'\n\n\n\n\n\n\ndf_chrn.glimpse()\n\nRows: 7043\nColumns: 2\n$ customer_id &lt;str&gt; '7590-VHVEG', '5575-GNVDE', '3668-QPYBK', '7795-CFOCW', '9237-HQITU', '9305-CDSKC', '1452-KIOVK', '6713-OKOMC', '7892-POOKP', '6388-TABGU'\n$ churn       &lt;str&gt; 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'No'\n\n\n\n\n\n\nUltimately, these are saved as seeds in the dbt project as a lightweight way to ingest small CSVs; in reality, they would be my sources flowing into my data warehouse from other production sources.\n\n\nFeatures & Training\nFeature preparation and training are the heart of where orbital fits into our pipelines. I recommend doing these steps one-at-a-time and explain them similarly. However, since the code is closely coupled, I’ll provide it at once for reference. The combination of feature engineering and model training steps look like this:\n\n\nPipeline to orbital\n# build pipeline(s)\n\n## feature pipeline does OneHotEncoding on all string columns (all are low/known cardinality)\n## orbital can create some very verbose variable names (for uniqueness) so we clean those up some\ncols_str = X.select( cs.string() ).columns\nonho_enc = ('oh', OneHotEncoder(sparse_output = False), cols_str)\nppl_feat = Pipeline([\n  (\"encoder\", ColumnTransformer([onho_enc], remainder='passthrough'))\n]).set_output(transform=\"polars\")\nX_tran = ppl_feat.fit_transform(X, y)\nX_tran.columns = [c.replace(' ','_').replace('-','_').replace('(','').replace(')','') for c in X_tran.columns]\n\n## training pipeline fits actual random forest model\nppl_pred = Pipeline([\n  (\"prep\", ColumnTransformer([], remainder='passthrough')),\n  (\"pred\", RandomForestClassifier(max_depth=3, n_estimators=100, random_state=123))\n])\nppl_pred.fit(X_tran, y)\n\n# convert to orbital\n\ntbl = \"TBL_REF\" # placeholder replaced in cleaning\n\n## creating mapping of source data types to orbital types \ntype_map = {\n    pl.String:orbital.types.StringColumnType(),\n    pl.Int32:orbital.types.Int32ColumnType(),\n    pl.Float64:orbital.types.DoubleColumnType()\n}\ndict_feat = {e: type_map.get(t) for e, t in zip(X.columns, X.dtypes)}\ndict_pred = {e: type_map.get(t) for e, t in zip(X_tran.columns, X_tran.dtypes)}\n\n## features\norb_ppl_feat = orbital.parse_pipeline(ppl_feat, features=dict_feat)\nsql_raw_feat = orbital.export_sql(tbl, orb_ppl_feat, dialect=\"duckdb\")\n\n## scoring\norb_ppl_pred = orbital.parse_pipeline(ppl_pred, features=dict_pred)\nsql_raw_pred = orbital.export_sql(tbl, orb_ppl_pred, dialect=\"duckdb\")\n\n\n\nFeatures\nFeature prep is the first use case for integrating orbital code in our dbt pipeline. Ultimately, we want to be sure our production features are identical to our development features. To do this, we make three design choices:\n\nPrepare raw features in the database (pre-joining) to take advantage of database-grade computational power and have preprocessing “published” to fuel different model experimentation\n\nAdding a model raw_feat to my dbt project that simply pre-joins relevant sources\n\nMake separate scikit-learn pipelines and orbital SQL output for feature and training steps for separate testing and faster scoring (Otherwise, orbital-generated SQL sometimes reproduces feature transformation logic at every use of the feature versus doing it once upfront. Depending one your database’s optimizer, it may or may not be smart enough to reorder this at runtime.)\n\nIn python, fit the ppl_feat pipeline (cell 4) which only fits the feature transformation steps\nRetrieve the resulting SQL code from orbital and clean it up (discussed below)\nDeploy it by writing the SQL back to the models/ folder as a model prep_feat\n\nNoting the assumptions we are making about our data while engineering features and pushing those tests upstream to the source in the database\n\nFor example, one-hot encoding assumes the categories won’t change. So, since we are one-hot encoding the internet_service field from source, we can update our schema.yml file to add an accepted_values data test for that field to warn us if our model is beginning to see data is was not prepared to handle7. Subsequent data models could, in theory, route these cases away from our scoring table and into a separate logging table for separate handling.\n\n\nThis way, we can deploy our exact features to the database separately from our final model for additional data validation. We can also run our dbt tests before consuming the results to ensure the assumptions that went into feature creation still hold.\nAgain, because we are using dbt, we can take advtange of related tools. Using the VS Code extension, we can examine our database’s DAG so far and see that our data test was correctly placed on the source:\n\n\n\nTraining\nModel training follows similarly. We create another sci-kit-learn pipline ppl_pred and train it (cell 4). This time, we just use the preprocessed data that was fit_transformed in the prior step. Alternatively, we could re-retrieve our newly prepared features from the database.\nIn theory, this is where we’d also do a lot of model evaluation and iteration where being outside of the database in a joy. I don’t do this since getting a good model is not my goal.\n\n\n\nSQL Cleanup\nWhile orbital does a lot of heavy lifting, the SQL it produces is not perfect:\n\nIt does not SELECT any metadata or identifier columns, rendering your predictions impossible to join to other data sources. Inserting this column requires care because sometimes the upstream data is being queried within the main query and other times it is queried in a CTE\nIts hard to get orbital to query from a ref() that plays nice with dbt’s Jinja because orbital is rigorous about quoting table and column names. So, it’s easier to put a placeholder table name and edit it in post-processing.\nIt uses somewhat long and bulky variable names that reflect scikit-learn internals, including .s in column names which can reduce readability and requires quoting since . usually means something different in SQL\nIt includes positive predictions, negative predictions, and labels which may be excessive. I’ve never wanted anything more than the positive predictions\nIt’s not formatted which shouldn’t matter but will wrankle anyone who has ever worked with SQL\n\nTo mitigate these multiple issues, sqlglot makes it easy to further parse the query. sqlglot is a package that allows you to turn any SQL script into an AST for ease of programatic modification. I defined a helper function with sqlglot to fix all of the above.\n\n\nCleaning function definition\nimport sqlglot\nfrom sqlglot import parse_one, exp \n\ndef clean_sql(sql_raw: str, \n              tbl_ref: str, \n              model_version: str = None,\n              col_id: str = 'id', \n              cols_renm: dict[str, str] = {'output_probability.1':'pred', \n                                           'output_probability.0': '0', \n                                           'output_label': '0'},\n              ) -&gt; str:\n\n    \"\"\"Opinionated clean-up of SQL returned by orbital\n\n    This function executes the following transformations:\n    - Rename desired columns such as the prediction column (per result of cols_renm)\n    - Remove unwanted variables (those being \"renamed\" to \"0\")\n    - Add back ID variable for joining predictions to other datasets \n    - Fix table reference from default TBL_REF to a specific dbt model reference\n    - Reformats SQL for improved readability\n\n    Parameters\n    ----------\n    sql_raw: SQL string provided by `orbital`\n    tbl_ref: Name of dbt model to be referenced in query's FROM clause\n    model_version: Version number of model to be added as own column. Defaults to None to add no column\n    col_id: Name of the column representing the unique identifier of entities to be predicted\n    cols_renm: Dictionary of {default_name: desired_name} to rename fields\n\n    Returns\n    -------\n    str\n        A formatted and updated SQL query\n    \"\"\"\n\n\n    ast = parse_one(sql_raw)\n    \n    for e in ast.expressions:\n        # rename prediction column\n        if cols_renm.get(e.alias) == '0':\n            e.set(arg_key='this',value=None)\n            e.set(arg_key='alias',value=None)\n        if e.alias in cols_renm.keys():\n            e.set(arg_key='alias',value=cols_renm.get(e.alias))\n    \n    # add back a variable for reference (typically like an ID for joining to other tables)\n    # this is tricky because sometimes orbital uses CTEs and other times it doesn't;\n    # generally, we need to get the identifier inside the CTE if it exists\n    col = exp.Column(this=exp.to_identifier(col_id))\n    if ast.find(exp.CTE) is not None:\n        cte_select = ast.find(exp.CTE).this\n        cte_select.expressions.append(col)\n    ast = ast.select(col_id)\n\n    # add model version to outer query if desired\n    if model_version is not None:\n\n        col_version = exp.Alias(\n            this=exp.Literal.string(model_version), \n            alias=\"model_version\")\n        ast.find(exp.Select).expressions.append(col_version)\n    \n    # pretty print\n    sql_fmt = sqlglot.transpile(ast.sql(), \n                                write=\"duckdb\", \n                                identify=True, \n                                pretty=True)[0]\n    \n    # change out table to dbt reference\n    ref_str = f\"{{{{ ref('{tbl_ref}')}}}}\"\n    sql_fnl = sql_fmt.replace('\"TBL_REF\"', ref_str) \n  \n    return sql_fnl\n\n\nI run the SQL generated from both ppl_feat and ppl_rafo through this function before writing them to models/churn_model/prep_feat.sql and models/churn_model/pred_churn.sql in my dbt models/ directory.\nThis establishes our core model deployment pipeline:\n\n\n\nScoring, Preserving, and Activating Predictions\nWe now have a table in our database that has our churn model predictions! Here is where we can begin to utilize the full benefit of the data management tools that dbt has built in.\nBefore scoring, we can run our dbt test to ensure that our features are stable and valid.\nFor scoring, depending on our use case we can set the table materialization to be a table (rebuilt on a schedule) or a view (generated on the fly for a specific population).\nFor archiving past scores, we can update our dbt-project.yml to include snapshotting our predictions table. This means even if we publish our tables as a view, we could schedule a call to dbt snapshot on a regular basis to record a timestamped record of what our scores were at any given point in time. This could be useful for model monitoring or auditiability. For example, if we are using our churn model to segment a marketing campaign, we might need these scores later to determine who got what treatment in the campaign.\nFor staging analysis, we can use dbt analyses to render the scripts that might be needed to conduct model monitoring (e.g. merging past scores with observed targets.)\nWe can see examples of these different artifacts branching off of our DAG:\n\n\n\nDatamart Preview\nBelow, we can tour the resulting datasets:\n\nimport duckdb\nimport polars as pl\n\nwith duckdb.connect('dev.duckdb') as con:\n\n    df_feat = con.table('raw_feat').pl()\n    df_prep = con.table('prep_feat').pl()\n    df_pred = con.table('pred_churn').pl()\n    df_snap = con.table('main_snapshots.pred_churn_snapshot').pl()\n    df_fail = con.table('main_audit.accepted_values_serv_internet_service__DSL__Fiber_optic__No').pl()\n\n\nRaw Training DataPrepared FeaturesPredictionsSnapshotsFailures\n\n\n\ndf_feat.glimpse()\n\nRows: 6944\nColumns: 21\n$ customer_id       &lt;str&gt; '7590-VHVEG', '5575-GNVDE', '3668-QPYBK', '7795-CFOCW', '9237-HQITU', '9305-CDSKC', '1452-KIOVK', '6713-OKOMC', '7892-POOKP', '6388-TABGU'\n$ cat_train_test    &lt;str&gt; 'Train', 'Train', 'Train', 'Train', 'Train', 'Train', 'Train', 'Train', 'Train', 'Train'\n$ tenure            &lt;i32&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62\n$ phone_service     &lt;str&gt; 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes'\n$ multiple_lines    &lt;str&gt; 'No phone service', 'No', 'No', 'No phone service', 'No', 'Yes', 'Yes', 'No phone service', 'Yes', 'No'\n$ internet_service  &lt;str&gt; 'DSL', 'DSL', 'DSL', 'DSL', 'Fiber optic', 'Fiber optic', 'Fiber optic', 'DSL', 'Fiber optic', 'DSL'\n$ online_security   &lt;str&gt; 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'Yes'\n$ online_backup     &lt;str&gt; 'Yes', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes'\n$ device_protection &lt;str&gt; 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'Yes', 'No'\n$ tech_support      &lt;str&gt; 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No'\n$ streaming_tv      &lt;str&gt; 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No'\n$ streaming_movies  &lt;str&gt; 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No'\n$ gender            &lt;str&gt; 'Female', 'Male', 'Male', 'Male', 'Female', 'Female', 'Male', 'Female', 'Female', 'Male'\n$ senior_citizen    &lt;i32&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n$ partner           &lt;str&gt; 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No'\n$ dependents        &lt;str&gt; 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes'\n$ contract          &lt;str&gt; 'Month-to-month', 'One year', 'Month-to-month', 'One year', 'Month-to-month', 'Month-to-month', 'Month-to-month', 'Month-to-month', 'Month-to-month', 'One year'\n$ paperless_billing &lt;str&gt; 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No'\n$ payment_method    &lt;str&gt; 'Electronic check', 'Mailed check', 'Mailed check', 'Bank transfer (automatic)', 'Electronic check', 'Electronic check', 'Credit card (automatic)', 'Mailed check', 'Electronic check', 'Bank transfer (automatic)'\n$ monthly_charges   &lt;f64&gt; 29.85, 56.95, 53.85, 42.3, 70.7, 99.65, 89.1, 29.75, 104.8, 56.15\n$ total_charges     &lt;f64&gt; 29.85, 1889.5, 108.15, 1840.75, 151.65, 820.5, 1949.4, 301.9, 3046.05, 3487.95\n\n\n\n\n\n\ndf_prep.glimpse()\n\nRows: 6944\nColumns: 47\n$ oh__phone_service_No                       &lt;f64&gt; 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n$ oh__phone_service_Yes                      &lt;f64&gt; 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0\n$ oh__multiple_lines_No                      &lt;f64&gt; 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0\n$ oh__multiple_lines_No_phone_service        &lt;f64&gt; 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n$ oh__multiple_lines_Yes                     &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0\n$ oh__internet_service_DSL                   &lt;f64&gt; 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0\n$ oh__internet_service_Fiber_optic           &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0\n$ oh__internet_service_No                    &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ oh__online_security_No                     &lt;f64&gt; 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0\n$ oh__online_security_No_internet_service    &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ oh__online_security_Yes                    &lt;f64&gt; 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0\n$ oh__online_backup_No                       &lt;f64&gt; 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0\n$ oh__online_backup_No_internet_service      &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ oh__online_backup_Yes                      &lt;f64&gt; 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0\n$ oh__device_protection_No                   &lt;f64&gt; 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0\n$ oh__device_protection_No_internet_service  &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ oh__device_protection_Yes                  &lt;f64&gt; 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0\n$ oh__tech_support_No                        &lt;f64&gt; 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0\n$ oh__tech_support_No_internet_service       &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ oh__tech_support_Yes                       &lt;f64&gt; 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n$ oh__streaming_tv_No                        &lt;f64&gt; 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0\n$ oh__streaming_tv_No_internet_service       &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ oh__streaming_tv_Yes                       &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0\n$ oh__streaming_movies_No                    &lt;f64&gt; 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0\n$ oh__streaming_movies_No_internet_service   &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ oh__streaming_movies_Yes                   &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0\n$ oh__gender_Female                          &lt;f64&gt; 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0\n$ oh__gender_Male                            &lt;f64&gt; 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0\n$ oh__partner_No                             &lt;f64&gt; 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0\n$ oh__partner_Yes                            &lt;f64&gt; 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0\n$ oh__dependents_No                          &lt;f64&gt; 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0\n$ oh__dependents_Yes                         &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0\n$ oh__contract_Month_to_month                &lt;f64&gt; 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0\n$ oh__contract_One_year                      &lt;f64&gt; 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n$ oh__contract_Two_year                      &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ oh__paperless_billing_No                   &lt;f64&gt; 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0\n$ oh__paperless_billing_Yes                  &lt;f64&gt; 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0\n$ oh__payment_method_Bank_transfer_automatic &lt;f64&gt; 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0\n$ oh__payment_method_Credit_card_automatic   &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0\n$ oh__payment_method_Electronic_check        &lt;f64&gt; 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0\n$ oh__payment_method_Mailed_check            &lt;f64&gt; 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0\n$ remainder__tenure                          &lt;f64&gt; 1.0, 34.0, 2.0, 45.0, 2.0, 8.0, 22.0, 10.0, 28.0, 62.0\n$ remainder__senior_citizen                  &lt;f64&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n$ remainder__monthly_charges                 &lt;f64&gt; 29.85, 56.95, 53.85, 42.3, 70.7, 99.65, 89.1, 29.75, 104.8, 56.15\n$ remainder__total_charges                   &lt;f64&gt; 29.85, 1889.5, 108.15, 1840.75, 151.65, 820.5, 1949.4, 301.9, 3046.05, 3487.95\n$ customer_id                                &lt;str&gt; '7590-VHVEG', '5575-GNVDE', '3668-QPYBK', '7795-CFOCW', '9237-HQITU', '9305-CDSKC', '1452-KIOVK', '6713-OKOMC', '7892-POOKP', '6388-TABGU'\n$ model_version                              &lt;str&gt; '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0'\n\n\n\n\n\n\ndf_pred.glimpse()\n\nRows: 6944\nColumns: 3\n$ pred          &lt;f64&gt; 0.4350639304611832, 0.14068829294410534, 0.34994459204608575, 0.10898763570003211, 0.5811184463091195, 0.5483232741244137, 0.4043196897255257, 0.311830934981117, 0.3962726652389392, 0.1372128768125549\n$ customer_id   &lt;str&gt; '7590-VHVEG', '5575-GNVDE', '3668-QPYBK', '7795-CFOCW', '9237-HQITU', '9305-CDSKC', '1452-KIOVK', '6713-OKOMC', '7892-POOKP', '6388-TABGU'\n$ model_version &lt;str&gt; '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0'\n\n\n\n\n\nScore versioned and timestamped predictions from snapshots for auditability.\n\ndf_snap.glimpse()\n\nRows: 6944\nColumns: 7\n$ pred                    &lt;f64&gt; 0.4350639304611832, 0.14068829294410534, 0.34994459204608575, 0.10898763570003211, 0.5811184463091195, 0.5483232741244137, 0.4043196897255257, 0.311830934981117, 0.3962726652389392, 0.1372128768125549\n$ customer_id             &lt;str&gt; '7590-VHVEG', '5575-GNVDE', '3668-QPYBK', '7795-CFOCW', '9237-HQITU', '9305-CDSKC', '1452-KIOVK', '6713-OKOMC', '7892-POOKP', '6388-TABGU'\n$ model_version           &lt;str&gt; '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0', '1.0'\n$ dbt_scd_id              &lt;str&gt; 'c4671964ba707c90a41d74f6f2ef75b7', '7dc40efa71bcee4795c7f54b3b5bc783', 'b05d4425f5d07106f1f2f2e782461f44', '3b919e27eb23ba54e200462af172e7da', 'eb6117ba3156a771b0e02e5e7bc644ab', 'ddae31e6abdabdd771ea4bbd1072fe55', 'aa7fe49fcbb5a937b44f7ac589b3ff34', 'da7eb2655934862105e8782e40ca5eb5', '882f945d0e265290e5976d4c8d04679e', '72f44f68e12a53baaf1d9ddd2469a616'\n$ dbt_updated_at &lt;datetime[μs]&gt; 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000\n$ dbt_valid_from &lt;datetime[μs]&gt; 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000, 2025-08-15 19:22:45.830000\n$ dbt_valid_to   &lt;datetime[μs]&gt; 9999-12-31 00:00:00, 9999-12-31 00:00:00, 9999-12-31 00:00:00, 9999-12-31 00:00:00, 9999-12-31 00:00:00, 9999-12-31 00:00:00, 9999-12-31 00:00:00, 9999-12-31 00:00:00, 9999-12-31 00:00:00, 9999-12-31 00:00:00\n\n\n\n\n\nWhat happens when the internet_service field is recoded in production data from “Fiber optic” to “Fiber” after training? If we are checking for accepted_values, we capture that change in our failures table before scoring on bad data!\n\ndf_fail.glimpse()\n\nRows: 1\nColumns: 2\n$ value_field &lt;str&gt; 'Fiber'\n$ n_records   &lt;i64&gt; 48\n\n\n\n\n\n\n\n\nDreaming bigger\nThis demo shows just orbital + dbt, but that’s just the beginning. Treating the whole MLOps process just like data processing means you can benefit from a wide range of integrated tools and capabilities, e.g.:\n\ndata ingestion\n\nretrieve training data for APIs with dlt\ningest features from flatfiles on blob sources via the dbt external-tables package\n\nbetter testing with dbt packages such as dbt-expectatons (from Great Expectations)\nlogging and observability\n\nsnapshot features table as well as predictions table\nuse dbt packages like elementary to write more run metadata to your warehouse\n\norchestration with Dagster\n\nunfurl your local dbt DAG into a broader pipeline\ntrigger more model-adjacent tasks from refitting, monitoring, etc.\n\ndocumentions with dbt docs (which can be enhanced with Quarto)\nreverse ETL with tools like HighTouch or Census to easily sync analytical data models into production systems like CRMs"
  },
  {
    "objectID": "post/orbital-mlops/index.html#limitations",
    "href": "post/orbital-mlops/index.html#limitations",
    "title": "MLOrbs?: MLOps in the database with orbital and dbt",
    "section": "Limitations",
    "text": "Limitations\nWhile I see a lot of promise in model deployment to the database, it’s currently not without it’s limitations. Tobias Macey of the excellent Data Engineering Podcast always ends his show by asking his guests (mostly tool developers): “When is  not the right solution?” I’ll conclude by answering the same.\nThere are many things I would consider if using orbital today for business use cases versus hobby projects:\n\nUse Case: ML in Database only makes sense for batch predictions. orbital is not the right solution if there is a chance you’ll want realtime predictions\nScale: SQL is generally good at optimizing large-scale data processing jobs. However, extremely large ensemble models may experience slower runtimes. If such a model was to be run at extreme scale, one would need to consider the relative latency7 and cost8 of this versus other solutions\nAlgorithms: Right now orbital is mostly limited to scikit-learn models and select feature engineering steps (or tidymodels in R). This can be a challenge if you want to use other common algorithms. I’ve figured out some workarounds for xgboost but at some point, the amount of hacking around the periphery reduces the “same code in dev and prod” benefits\nPrecision: orbital uses sklearn-onnx which can create some issues when floating point precision. It is easily tested how critical this is for your use case, but you may find corner cases where it is difficult to precisely recreate your local predictions – particularly for tree-based models where tiny perturbations send an observation down a different path.\nBugs: orbital still has some bugs it’s working out and seems to still be building out its testing infrastructure. For example, at the time of writing this demo, I started out trying to use the TargetEncoder() which failed unexpectedly so I switched to the OneHotEncoder(). That’s fine for a demo, but I wouldn’t be so cavelier about letting tool limitations shape my modeling choices in real life.\nGovernance: Similar to the downsides of dbt, the risk of lowering the barriers to entry to deploying a new data model or machine learning model is that it will be done carelessly or prolificly. As the demo above shows, a rigorous approach can add many data artifacts to your datamart and could risk causing bloat if done casually. Having the right controls to determine who should be allowed to deploy models of what materiality is key.\n\nThe good news is, most of these downsides are fully testable. You can quickly and pretty robustly dual-validated orbital’s logic and cross-check prediction speed and accuracy from python and SQL environments. So, if the idea sounds intriguing, take it for a spin! There aren’t too many “unknown unknowns”. These packages are under active development and improving by the day. I am excited to continue following the progress and experimenting with this project."
  },
  {
    "objectID": "post/orbital-mlops/index.html#footnotes",
    "href": "post/orbital-mlops/index.html#footnotes",
    "title": "MLOrbs?: MLOps in the database with orbital and dbt",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis post is cursed because “data modeling” and “predictive modeling” are completely different things, one involving data pipelines and the other involve machine learning. Both happen to be relevant here.↩︎\nI say project versus package because orbital is really a “concept” with parallel but programmatically unrelated R and python implementations; the R project has been around for a but, but the python version is recently released .↩︎\nJust want a few concrete ideas for stitching these tools together without the wind-up? Jump to Section 3.1.↩︎\nThis list is, of course, non-comprehensive and coincidentally cherry-picked towards the problems which I’ll claim orbital might address. For a thoughtful and comprehensive take on MLOps, check out this excellent survey by Shreya Shankar who, coincidentally enough, made MLOps the focus on her Stanford PhD in… Databases!↩︎\nIn my dual life volunteering on downballot campaigns, I also thing this pattern would be very effective to publish partisanship and turnout scores back to BigQuery, the beating heart of campaign data infrastructure.↩︎\nWithin a given database. SQL is a loosely enforced spec leading to an absurd amount of arbitrary uniqueness on top of ANSI. But, happily, so long as you aren’t switching databases, this does not matter.↩︎\nIf you run dbt test or dbt test --store-failures, you can find two such failure cases.↩︎\nOr mitigate it through off-hours scheduling and materializing as a table versus a view↩︎\nComparing cost of database compute versus egress/ingress of pulling data from database to execute somewhere else↩︎"
  },
  {
    "objectID": "post/orbital-mlops/index.html#sec-limitations",
    "href": "post/orbital-mlops/index.html#sec-limitations",
    "title": "MLOrbs?: MLOps in the database with orbital and dbt",
    "section": "Limitations",
    "text": "Limitations\nWhile I see a lot of promise in model deployment to the database, it’s currently not without it’s limitations. Tobias Macey of the excellent Data Engineering Podcast always ends his show by asking his guests (mostly tool developers): “When is  not the right solution?” I’ll conclude by answering the same.\nThere are many things I would consider if using orbital today for business use cases versus hobby projects:\n\nUse Case: ML in Database only makes sense for batch predictions. orbital is not the right solution if there is a chance you’ll want realtime predictions\nAlgorithms: Right now orbital is mostly limited to scikit-learn models and select feature engineering steps (or tidymodels in R). This can be a challenge if you want to use other common algorithms. I’ve figured out some workarounds for xgboost but at some point, the amount of hacking around the periphery reduces the “same code in dev and prod” benefits\nScale/Complexity: SQL is generally good at optimizing large-scale data processing jobs. However, extremely large ensemble models may experience slower runtimes. If such a model was to be run at extreme scale, one would need to consider the relative latency8 and cost9 of this versus other solutions. Depending on your engine, there are also some query optimizations to consider\nPlatform: Surprisingly, as I explored this mashup, I came to learn both BigQuery and Azure impose maximum query length limits which could pose challenges for large models (e.g. deep trees in random forests or GBMs). One could work around this with a lot of views, but it’s generally better to not pick a fight with your infrastructure.\nPrecision: orbital uses sklearn-onnx which can create some issues when floating point precision. It is easily tested how critical this is for your use case, but you may find corner cases where it is difficult to precisely recreate your local predictions – particularly for tree-based models where tiny perturbations send an observation down a different path.\nBugs: orbital still has some bugs it’s working out and seems to still be building out its testing infrastructure. For example, at the time of writing this demo, I started out trying to use the TargetEncoder() which failed unexpectedly so I switched to the OneHotEncoder(). That’s fine for a demo, but I wouldn’t be so cavelier about letting tool limitations shape my modeling choices in real life.\nGovernance: Similar to the downsides of dbt, the risk of lowering the barriers to entry to deploying a new data model or machine learning model is that it will be done carelessly or prolificly. As the demo above shows, a rigorous approach can add many data artifacts to your datamart and could risk causing bloat if done casually. Having the right controls to determine who should be allowed to deploy models of what materiality is key.\n\nThe good news is, most of these downsides are fully testable. You can quickly and pretty robustly dual-validated orbital’s logic and cross-check prediction speed and accuracy from python and SQL environments. So, if the idea sounds intriguing, take it for a spin! There aren’t too many “unknown unknowns”. These packages are under active development and improving by the day. I am excited to continue following the progress and experimenting with this project."
  }
]